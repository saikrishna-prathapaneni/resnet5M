{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]           4,704\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              ReLU-3         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 32, 56, 56]               0\n",
      "            Conv2d-5           [-1, 32, 56, 56]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 56, 56]              64\n",
      "              ReLU-7           [-1, 32, 56, 56]               0\n",
      "            Conv2d-8           [-1, 32, 56, 56]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 56, 56]              64\n",
      "           ResNet-10           [-1, 32, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]          18,432\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "           Conv2d-16           [-1, 64, 56, 56]           2,048\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "           ResNet-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "           ResNet-24           [-1, 64, 56, 56]               0\n",
      "           Conv2d-25           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
      "             ReLU-27           [-1, 64, 56, 56]               0\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "           ResNet-30           [-1, 64, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "             ReLU-33           [-1, 64, 56, 56]               0\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "           ResNet-36           [-1, 64, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
      "             ReLU-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "           Conv2d-42          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "           ResNet-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "           ResNet-50          [-1, 128, 28, 28]               0\n",
      "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
      "             ReLU-53          [-1, 128, 28, 28]               0\n",
      "           Conv2d-54          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "           ResNet-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "           ResNet-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-70          [-1, 256, 14, 14]             512\n",
      "             ReLU-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "           Conv2d-74          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "           ResNet-76          [-1, 256, 14, 14]               0\n",
      "           Conv2d-77          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "             ReLU-79          [-1, 256, 14, 14]               0\n",
      "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "           ResNet-82          [-1, 256, 14, 14]               0\n",
      "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
      "             ReLU-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "           ResNet-88          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-89            [-1, 256, 1, 1]               0\n",
      "          Flatten-90                  [-1, 256]               0\n",
      "           Linear-91                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,997,802\n",
      "Trainable params: 4,997,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 86.52\n",
      "Params size (MB): 19.07\n",
      "Estimated Total Size (MB): 106.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.relu(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        self.resblock2 = ResNet(32, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        self.resblock4=ResNet(64,64,stride=1)\n",
    "        self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,128,stride=2)\n",
    "        self.resblock7=ResNet(128,128,stride=1)\n",
    "        self.resblock8=ResNet(128,128,stride=1)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,256,stride=2)\n",
    "        self.resblock12=ResNet(256,256,stride=1)\n",
    "        self.resblock13=ResNet(256,256,stride=1)\n",
    "\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=256, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "      \n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 2.249457833102292 train acc per epoch=>  0.16941735933503838\n",
      "val loss per epoch =>  2.081274174436738 val acc per epoch => 0.21103639240506328 \n",
      "\n",
      "train loss per epoch =>  1 1.8071420662238469 train acc per epoch=>  0.30096707159600905\n",
      "val loss per epoch =>  1.6636155466490155 val acc per epoch => 0.36985759493670883 \n",
      "\n",
      "train loss per epoch =>  2 1.6235197267264052 train acc per epoch=>  0.384047314562761\n",
      "val loss per epoch =>  1.5933856737764576 val acc per epoch => 0.40456882911392406 \n",
      "\n",
      "train loss per epoch =>  3 1.4920595771516376 train acc per epoch=>  0.4496123721380063\n",
      "val loss per epoch =>  1.4111930192271365 val acc per epoch => 0.4885284810126582 \n",
      "\n",
      "train loss per epoch =>  4 1.4087366018148944 train acc per epoch=>  0.4860174233651222\n",
      "val loss per epoch =>  1.2826958034611955 val acc per epoch => 0.5269976265822784 \n",
      "\n",
      "train loss per epoch =>  5 1.3322532887349043 train acc per epoch=>  0.5195372442302801\n",
      "val loss per epoch =>  1.3274788268004791 val acc per epoch => 0.5218552215189873 \n",
      "\n",
      "train loss per epoch =>  6 1.2660004120043782 train acc per epoch=>  0.5442655051455778\n",
      "val loss per epoch =>  1.4312290375745749 val acc per epoch => 0.5234375 \n",
      "\n",
      "train loss per epoch =>  7 1.2072683463011251 train acc per epoch=>  0.5677309783218462\n",
      "val loss per epoch =>  1.1467578644993939 val acc per epoch => 0.5926621835443038 \n",
      "\n",
      "train loss per epoch =>  8 1.1683097164649183 train acc per epoch=>  0.5844709078979005\n",
      "val loss per epoch =>  1.1217390879800049 val acc per epoch => 0.611748417721519 \n",
      "\n",
      "train loss per epoch =>  9 1.1220588569750871 train acc per epoch=>  0.6041120523991792\n",
      "val loss per epoch =>  1.0836742618415929 val acc per epoch => 0.630439082278481 \n",
      "\n",
      "train loss per epoch =>  10 1.0817593653183764 train acc per epoch=>  0.6207720588540178\n",
      "val loss per epoch =>  1.0034980140154874 val acc per epoch => 0.6554588607594937 \n",
      "\n",
      "train loss per epoch =>  11 1.0490661514994433 train acc per epoch=>  0.6309702686031761\n",
      "val loss per epoch =>  0.9843664161766632 val acc per epoch => 0.6522943037974683 \n",
      "\n",
      "train loss per epoch =>  12 1.0243054082631455 train acc per epoch=>  0.6420156649311485\n",
      "val loss per epoch =>  1.0248165734206574 val acc per epoch => 0.6473496835443038 \n",
      "\n",
      "train loss per epoch =>  13 0.9853592987560555 train acc per epoch=>  0.6564418158262891\n",
      "val loss per epoch =>  0.9257020112834399 val acc per epoch => 0.6908623417721519 \n",
      "\n",
      "train loss per epoch =>  14 0.9682905733432916 train acc per epoch=>  0.664570012818212\n",
      "val loss per epoch =>  0.9284818564789205 val acc per epoch => 0.6769185126582279 \n",
      "\n",
      "train loss per epoch =>  15 0.936914786200999 train acc per epoch=>  0.6744804987517159\n",
      "val loss per epoch =>  0.8489143214648283 val acc per epoch => 0.7150909810126582 \n",
      "\n",
      "train loss per epoch =>  16 0.9248505570089726 train acc per epoch=>  0.680179028102504\n",
      "val loss per epoch =>  0.841502892820141 val acc per epoch => 0.7216178797468354 \n",
      "\n",
      "train loss per epoch =>  17 0.9077366887760894 train acc per epoch=>  0.688822730270493\n",
      "val loss per epoch =>  0.824167826507665 val acc per epoch => 0.719442246835443 \n",
      "\n",
      "train loss per epoch =>  18 0.8932018938576779 train acc per epoch=>  0.6946171676106465\n",
      "val loss per epoch =>  0.813061275814153 val acc per epoch => 0.7264636075949367 \n",
      "\n",
      "train loss per epoch =>  19 0.8639816440584714 train acc per epoch=>  0.7033367966447035\n",
      "val loss per epoch =>  0.8368817326388781 val acc per epoch => 0.7227056962025317 \n",
      "\n",
      "CPU times: user 2min 45s, sys: 33.5 s, total: 3min 19s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=20\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./adam_batch.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7264636075949367"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
