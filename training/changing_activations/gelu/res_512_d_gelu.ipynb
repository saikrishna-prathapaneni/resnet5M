{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              GELU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "           Dropout-7           [-1, 64, 56, 56]               0\n",
      "              GELU-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "           ResNet-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "          Dropout-14           [-1, 64, 56, 56]               0\n",
      "             GELU-15           [-1, 64, 56, 56]               0\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "           ResNet-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "          Dropout-21           [-1, 64, 56, 56]               0\n",
      "             GELU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
      "           ResNet-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "          Dropout-28          [-1, 128, 28, 28]               0\n",
      "             GELU-29          [-1, 128, 28, 28]               0\n",
      "           Conv2d-30          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "           Conv2d-32          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "           ResNet-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "          Dropout-37          [-1, 128, 28, 28]               0\n",
      "             GELU-38          [-1, 128, 28, 28]               0\n",
      "           Conv2d-39          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "           ResNet-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "          Dropout-44          [-1, 128, 28, 28]               0\n",
      "             GELU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "           ResNet-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "          Dropout-51          [-1, 128, 28, 28]               0\n",
      "             GELU-52          [-1, 128, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "           ResNet-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "          Dropout-58          [-1, 128, 28, 28]               0\n",
      "             GELU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "          Dropout-65          [-1, 128, 28, 28]               0\n",
      "             GELU-66          [-1, 128, 28, 28]               0\n",
      "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "           ResNet-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70          [-1, 512, 14, 14]         589,824\n",
      "      BatchNorm2d-71          [-1, 512, 14, 14]           1,024\n",
      "          Dropout-72          [-1, 512, 14, 14]               0\n",
      "             GELU-73          [-1, 512, 14, 14]               0\n",
      "           Conv2d-74          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-75          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-76          [-1, 512, 14, 14]          65,536\n",
      "      BatchNorm2d-77          [-1, 512, 14, 14]           1,024\n",
      "           ResNet-78          [-1, 512, 14, 14]               0\n",
      "AdaptiveAvgPool2d-79            [-1, 512, 1, 1]               0\n",
      "          Flatten-80                  [-1, 512]               0\n",
      "           Linear-81                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,961,610\n",
      "Trainable params: 4,961,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 92.65\n",
      "Params size (MB): 18.93\n",
      "Estimated Total Size (MB): 112.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.gelu(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        #self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        #self.resblock2 = ResNet(64, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        #self.resblock4=ResNet(64,64,stride=1)\n",
    "        #self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,64,stride=1)\n",
    "        self.resblock7=ResNet(64,64,stride=1)\n",
    "        self.resblock8=ResNet(64,128,stride=2)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,128,stride=1)\n",
    "        self.resblock12=ResNet(128,128,stride=1)\n",
    "        self.resblock13=ResNet(128,128,stride=1)\n",
    "        self.resblock14 =ResNet(128,512,stride=2)\n",
    "       #self.resblock15 =ResNet(512,512,stride=1)\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.gelu(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        #x = self.resblock1(x)\n",
    "        #x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        #x = self.resblock4(x)\n",
    "        #x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "        x= self.resblock14(x)\n",
    "        #x= self.resblock15(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 1.6302763940123342 train acc per epoch=>  0.4048313618925831\n",
      "val loss per epoch =>  1.4947601858573625 val acc per epoch => 0.45144382911392406 \n",
      "\n",
      "train loss per epoch =>  1 1.3099489995585683 train acc per epoch=>  0.5264905691146851\n",
      "val loss per epoch =>  1.5285672205912917 val acc per epoch => 0.5042523734177216 \n",
      "\n",
      "train loss per epoch =>  2 1.1712700360266448 train acc per epoch=>  0.5811381074778564\n",
      "val loss per epoch =>  1.0297530791427516 val acc per epoch => 0.629746835443038 \n",
      "\n",
      "train loss per epoch =>  3 1.0651087661838288 train acc per epoch=>  0.6214993606747874\n",
      "val loss per epoch =>  1.0137168470817277 val acc per epoch => 0.6497231012658228 \n",
      "\n",
      "train loss per epoch =>  4 0.9838995573770665 train acc per epoch=>  0.6517063618620949\n",
      "val loss per epoch =>  0.9867282668246499 val acc per epoch => 0.6677215189873418 \n",
      "\n",
      "train loss per epoch =>  5 0.9291321952324694 train acc per epoch=>  0.6712196292474751\n",
      "val loss per epoch =>  0.8306579831280286 val acc per epoch => 0.7097507911392406 \n",
      "\n",
      "train loss per epoch =>  6 0.8777110829682606 train acc per epoch=>  0.6904731458410278\n",
      "val loss per epoch =>  0.8341481851626046 val acc per epoch => 0.7063884493670886 \n",
      "\n",
      "train loss per epoch =>  7 0.833220899105072 train acc per epoch=>  0.7067095588540178\n",
      "val loss per epoch =>  0.8351602011089083 val acc per epoch => 0.7147943037974683 \n",
      "\n",
      "train loss per epoch =>  8 0.8057681939486042 train acc per epoch=>  0.7159446930641409\n",
      "val loss per epoch =>  0.7494883744777003 val acc per epoch => 0.7481210443037974 \n",
      "\n",
      "train loss per epoch =>  9 0.7762971935064896 train acc per epoch=>  0.7280450768178076\n",
      "val loss per epoch =>  0.7765164216862449 val acc per epoch => 0.7333860759493671 \n",
      "\n",
      "train loss per epoch =>  10 0.7477884998406901 train acc per epoch=>  0.7378436700462381\n",
      "val loss per epoch =>  0.7096176811411411 val acc per epoch => 0.7589003164556962 \n",
      "\n",
      "train loss per epoch =>  11 0.7250068603116838 train acc per epoch=>  0.7456441815856778\n",
      "val loss per epoch =>  0.6619961231569701 val acc per epoch => 0.7695806962025317 \n",
      "\n",
      "train loss per epoch =>  12 0.7037437902692029 train acc per epoch=>  0.7520060741992862\n",
      "val loss per epoch =>  0.6811324531518961 val acc per epoch => 0.7676028481012658 \n",
      "\n",
      "train loss per epoch =>  13 0.6851005244742879 train acc per epoch=>  0.7585437979234759\n",
      "val loss per epoch =>  0.6903403869158105 val acc per epoch => 0.7650316455696202 \n",
      "\n",
      "train loss per epoch =>  14 0.6630590349207144 train acc per epoch=>  0.7689018542199488\n",
      "val loss per epoch =>  0.6349620928492727 val acc per epoch => 0.7828322784810127 \n",
      "\n",
      "train loss per epoch =>  15 0.6486506354625877 train acc per epoch=>  0.7713954604190328\n",
      "val loss per epoch =>  0.6281474745726283 val acc per epoch => 0.7842167721518988 \n",
      "\n",
      "train loss per epoch =>  16 0.628156328902525 train acc per epoch=>  0.7794237531664426\n",
      "val loss per epoch =>  0.5847191916236395 val acc per epoch => 0.8032041139240507 \n",
      "\n",
      "train loss per epoch =>  17 0.6142216919328246 train acc per epoch=>  0.7840073529411765\n",
      "val loss per epoch =>  0.5853362441817417 val acc per epoch => 0.7946993670886076 \n",
      "\n",
      "train loss per epoch =>  18 0.6002092623649655 train acc per epoch=>  0.7900375639995956\n",
      "val loss per epoch =>  0.6342905869212332 val acc per epoch => 0.7848101265822784 \n",
      "\n",
      "train loss per epoch =>  19 0.58516343597256 train acc per epoch=>  0.7955442775240944\n",
      "val loss per epoch =>  0.5702708333353453 val acc per epoch => 0.8061708860759493 \n",
      "\n",
      "train loss per epoch =>  20 0.5672813453485289 train acc per epoch=>  0.8011309143222506\n",
      "val loss per epoch =>  0.5582928265197368 val acc per epoch => 0.8081487341772152 \n",
      "\n",
      "train loss per epoch =>  21 0.5616332464053503 train acc per epoch=>  0.8016464194983167\n",
      "val loss per epoch =>  0.5360559756997265 val acc per epoch => 0.8169501582278481 \n",
      "\n",
      "train loss per epoch =>  22 0.5467274349821193 train acc per epoch=>  0.8088874681221555\n",
      "val loss per epoch =>  0.5640502398527121 val acc per epoch => 0.8113132911392406 \n",
      "\n",
      "train loss per epoch =>  23 0.5315584717199321 train acc per epoch=>  0.8124760230788795\n",
      "val loss per epoch =>  0.5179117518135264 val acc per epoch => 0.8242681962025317 \n",
      "\n",
      "train loss per epoch =>  24 0.5183055241546972 train acc per epoch=>  0.814993606199084\n",
      "val loss per epoch =>  0.5526821960376788 val acc per epoch => 0.8152689873417721 \n",
      "\n",
      "train loss per epoch =>  25 0.5086082486088014 train acc per epoch=>  0.8209079284497234\n",
      "val loss per epoch =>  0.5231911728653726 val acc per epoch => 0.8246637658227848 \n",
      "\n",
      "train loss per epoch =>  26 0.5007901550711268 train acc per epoch=>  0.8218470269151966\n",
      "val loss per epoch =>  0.513655319998536 val acc per epoch => 0.8270371835443038 \n",
      "\n",
      "train loss per epoch =>  27 0.4883197560487196 train acc per epoch=>  0.8273897059738179\n",
      "val loss per epoch =>  0.5254877949062782 val acc per epoch => 0.8216969936708861 \n",
      "\n",
      "train loss per epoch =>  28 0.4731720119638516 train acc per epoch=>  0.8327365729510022\n",
      "val loss per epoch =>  0.5314885989020143 val acc per epoch => 0.8214992088607594 \n",
      "\n",
      "train loss per epoch =>  29 0.46877878187867383 train acc per epoch=>  0.8346867007977518\n",
      "val loss per epoch =>  0.5046563186222994 val acc per epoch => 0.8336629746835443 \n",
      "\n",
      "train loss per epoch =>  30 0.45466195782432167 train acc per epoch=>  0.83793957809658\n",
      "val loss per epoch =>  0.5114555834214899 val acc per epoch => 0.8305973101265823 \n",
      "\n",
      "train loss per epoch =>  31 0.448220095137501 train acc per epoch=>  0.8414322250639387\n",
      "val loss per epoch =>  0.4954908286468892 val acc per epoch => 0.8330696202531646 \n",
      "\n",
      "train loss per epoch =>  32 0.432498060681326 train acc per epoch=>  0.8467431266594421\n",
      "val loss per epoch =>  0.494529316319695 val acc per epoch => 0.838310917721519 \n",
      "\n",
      "train loss per epoch =>  33 0.42336198466513164 train acc per epoch=>  0.8513826726342711\n",
      "val loss per epoch =>  0.5029168193098865 val acc per epoch => 0.8325751582278481 \n",
      "\n",
      "train loss per epoch =>  34 0.4171938455242025 train acc per epoch=>  0.8501358695347291\n",
      "val loss per epoch =>  0.493342622548719 val acc per epoch => 0.8373219936708861 \n",
      "\n",
      "train loss per epoch =>  35 0.41194949175238305 train acc per epoch=>  0.8542998721227621\n",
      "val loss per epoch =>  0.48995752576031265 val acc per epoch => 0.8380142405063291 \n",
      "\n",
      "train loss per epoch =>  36 0.400493527617296 train acc per epoch=>  0.8585877557239874\n",
      "val loss per epoch =>  0.4840597910217092 val acc per epoch => 0.8395965189873418 \n",
      "\n",
      "train loss per epoch =>  37 0.3970558019474034 train acc per epoch=>  0.8584079283582585\n",
      "val loss per epoch =>  0.4858758547638036 val acc per epoch => 0.8408821202531646 \n",
      "\n",
      "train loss per epoch =>  38 0.3880538778841648 train acc per epoch=>  0.8622682225673705\n",
      "val loss per epoch =>  0.47808745013007636 val acc per epoch => 0.8439477848101266 \n",
      "\n",
      "train loss per epoch =>  39 0.38311380860598193 train acc per epoch=>  0.8634231137802534\n",
      "val loss per epoch =>  0.48547778891611704 val acc per epoch => 0.8424643987341772 \n",
      "\n",
      "train loss per epoch =>  40 0.37425810083403915 train acc per epoch=>  0.8681106139022066\n",
      "val loss per epoch =>  0.4747936378551435 val acc per epoch => 0.8440466772151899 \n",
      "\n",
      "train loss per epoch =>  41 0.3741748562020719 train acc per epoch=>  0.8681625639995956\n",
      "val loss per epoch =>  0.4745361714423457 val acc per epoch => 0.8438488924050633 \n",
      "\n",
      "train loss per epoch =>  42 0.36783980183741627 train acc per epoch=>  0.867127557544757\n",
      "val loss per epoch =>  0.46960177564922767 val acc per epoch => 0.8458267405063291 \n",
      "\n",
      "train loss per epoch =>  43 0.36213600898490234 train acc per epoch=>  0.870104699488491\n",
      "val loss per epoch =>  0.4743742848498912 val acc per epoch => 0.8447389240506329 \n",
      "\n",
      "train loss per epoch =>  44 0.35950401478716176 train acc per epoch=>  0.8705922314882888\n",
      "val loss per epoch =>  0.47196208374409737 val acc per epoch => 0.8453322784810127 \n",
      "\n",
      "train loss per epoch =>  45 0.34900592839169076 train acc per epoch=>  0.8762947570942247\n",
      "val loss per epoch =>  0.47130526055263566 val acc per epoch => 0.8460245253164557 \n",
      "\n",
      "train loss per epoch =>  46 0.35207297330927056 train acc per epoch=>  0.8724944053403557\n",
      "val loss per epoch =>  0.47033696574500844 val acc per epoch => 0.846123417721519 \n",
      "\n",
      "train loss per epoch =>  47 0.35481737817035003 train acc per epoch=>  0.8729459719584726\n",
      "val loss per epoch =>  0.46907207539564444 val acc per epoch => 0.8473101265822784 \n",
      "\n",
      "train loss per epoch =>  48 0.34936286371839625 train acc per epoch=>  0.8753876279077262\n",
      "val loss per epoch =>  0.47090813857090624 val acc per epoch => 0.8467167721518988 \n",
      "\n",
      "train loss per epoch =>  49 0.3534539943308477 train acc per epoch=>  0.8757432865364777\n",
      "val loss per epoch =>  0.47358641537684426 val acc per epoch => 0.8457278481012658 \n",
      "\n",
      "CPU times: user 4min 57s, sys: 1min 6s, total: 6min 3s\n",
      "Wall time: 7min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../../data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 50)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=50\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./res_512_d_leaky.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8473101265822784"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
