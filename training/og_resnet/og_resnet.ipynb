{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]           4,704\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              ReLU-3         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 32, 56, 56]               0\n",
      "            Conv2d-5           [-1, 32, 56, 56]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 56, 56]              64\n",
      "              ReLU-7           [-1, 32, 56, 56]               0\n",
      "            Conv2d-8           [-1, 32, 56, 56]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 56, 56]              64\n",
      "           ResNet-10           [-1, 32, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]          18,432\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "           Conv2d-16           [-1, 64, 56, 56]           2,048\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "           ResNet-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "           ResNet-24           [-1, 64, 56, 56]               0\n",
      "           Conv2d-25           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
      "             ReLU-27           [-1, 64, 56, 56]               0\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "           ResNet-30           [-1, 64, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "             ReLU-33           [-1, 64, 56, 56]               0\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "           ResNet-36           [-1, 64, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
      "             ReLU-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "           Conv2d-42          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "           ResNet-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "           ResNet-50          [-1, 128, 28, 28]               0\n",
      "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
      "             ReLU-53          [-1, 128, 28, 28]               0\n",
      "           Conv2d-54          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "           ResNet-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "           ResNet-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-70          [-1, 256, 14, 14]             512\n",
      "             ReLU-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "           Conv2d-74          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "           ResNet-76          [-1, 256, 14, 14]               0\n",
      "           Conv2d-77          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "             ReLU-79          [-1, 256, 14, 14]               0\n",
      "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "           ResNet-82          [-1, 256, 14, 14]               0\n",
      "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
      "             ReLU-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "           ResNet-88          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-89            [-1, 256, 1, 1]               0\n",
      "          Flatten-90                  [-1, 256]               0\n",
      "           Linear-91                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,997,802\n",
      "Trainable params: 4,997,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 86.52\n",
      "Params size (MB): 19.07\n",
      "Estimated Total Size (MB): 106.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.relu(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        self.resblock2 = ResNet(32, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        self.resblock4=ResNet(64,64,stride=1)\n",
    "        self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,128,stride=2)\n",
    "        self.resblock7=ResNet(128,128,stride=1)\n",
    "        self.resblock8=ResNet(128,128,stride=1)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,256,stride=2)\n",
    "        self.resblock12=ResNet(256,256,stride=1)\n",
    "        self.resblock13=ResNet(256,256,stride=1)\n",
    "\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=256, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "      \n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 1.756521003020694 train acc per epoch=>  0.35692135551396537\n",
      "val loss per epoch =>  1.7344972936412957 val acc per epoch => 0.36520965189873417 \n",
      "\n",
      "train loss per epoch =>  1 1.4279043342146422 train acc per epoch=>  0.4820052749665497\n",
      "val loss per epoch =>  1.5306808224207238 val acc per epoch => 0.4296875 \n",
      "\n",
      "train loss per epoch =>  2 1.2788957468688946 train acc per epoch=>  0.5411604859335039\n",
      "val loss per epoch =>  1.2494918688943115 val acc per epoch => 0.5677412974683544 \n",
      "\n",
      "train loss per epoch =>  3 1.1730881624514489 train acc per epoch=>  0.584011349074371\n",
      "val loss per epoch =>  1.0529260763639137 val acc per epoch => 0.619560917721519 \n",
      "\n",
      "train loss per epoch =>  4 1.0893587605727604 train acc per epoch=>  0.6107776535441504\n",
      "val loss per epoch =>  1.0238922940024846 val acc per epoch => 0.6393393987341772 \n",
      "\n",
      "train loss per epoch =>  5 1.026295450947169 train acc per epoch=>  0.6387787723480283\n",
      "val loss per epoch =>  1.0929551305650156 val acc per epoch => 0.6338014240506329 \n",
      "\n",
      "train loss per epoch =>  6 0.9639935591031829 train acc per epoch=>  0.6609255115089514\n",
      "val loss per epoch =>  1.0200557678560667 val acc per epoch => 0.6574367088607594 \n",
      "\n",
      "train loss per epoch =>  7 0.9201765467443734 train acc per epoch=>  0.6775135870479867\n",
      "val loss per epoch =>  1.076770249801346 val acc per epoch => 0.6411194620253164 \n",
      "\n",
      "train loss per epoch =>  8 0.8852126738604378 train acc per epoch=>  0.6896659207466008\n",
      "val loss per epoch =>  0.8309955891174606 val acc per epoch => 0.7166732594936709 \n",
      "\n",
      "train loss per epoch =>  9 0.8508716864354166 train acc per epoch=>  0.7027213875290073\n",
      "val loss per epoch =>  0.8482162258293056 val acc per epoch => 0.7119264240506329 \n",
      "\n",
      "train loss per epoch =>  10 0.8209205957324913 train acc per epoch=>  0.7125679347521204\n",
      "val loss per epoch =>  0.7979762772970562 val acc per epoch => 0.7210245253164557 \n",
      "\n",
      "train loss per epoch =>  11 0.7955528065524138 train acc per epoch=>  0.7227541560407185\n",
      "val loss per epoch =>  0.8040649060961567 val acc per epoch => 0.7237935126582279 \n",
      "\n",
      "train loss per epoch =>  12 0.769011123710886 train acc per epoch=>  0.7297074808489026\n",
      "val loss per epoch =>  0.7117702734621265 val acc per epoch => 0.7508900316455697 \n",
      "\n",
      "train loss per epoch =>  13 0.7491817327259142 train acc per epoch=>  0.7356617647363707\n",
      "val loss per epoch =>  0.7304047048091888 val acc per epoch => 0.750692246835443 \n",
      "\n",
      "train loss per epoch =>  14 0.7344007059893645 train acc per epoch=>  0.7420196610948314\n",
      "val loss per epoch =>  0.6779164773753926 val acc per epoch => 0.7658227848101266 \n",
      "\n",
      "train loss per epoch =>  15 0.7120345881223069 train acc per epoch=>  0.7515744885520252\n",
      "val loss per epoch =>  0.7537116182001331 val acc per epoch => 0.7466376582278481 \n",
      "\n",
      "train loss per epoch =>  16 0.6966822390513652 train acc per epoch=>  0.757209079344864\n",
      "val loss per epoch =>  0.709199728845041 val acc per epoch => 0.7587025316455697 \n",
      "\n",
      "train loss per epoch =>  17 0.681407393015864 train acc per epoch=>  0.7630434783218462\n",
      "val loss per epoch =>  0.6816443748111967 val acc per epoch => 0.7565268987341772 \n",
      "\n",
      "train loss per epoch =>  18 0.6652679860287005 train acc per epoch=>  0.7681226023017903\n",
      "val loss per epoch =>  0.6653928458690643 val acc per epoch => 0.7744264240506329 \n",
      "\n",
      "train loss per epoch =>  19 0.6523636472042259 train acc per epoch=>  0.7710437980454291\n",
      "val loss per epoch =>  0.7183927610705171 val acc per epoch => 0.7609770569620253 \n",
      "\n",
      "train loss per epoch =>  20 0.6460911965431155 train acc per epoch=>  0.7744565217391305\n",
      "val loss per epoch =>  0.6228932738304138 val acc per epoch => 0.7874802215189873 \n",
      "\n",
      "train loss per epoch =>  21 0.6267245060495098 train acc per epoch=>  0.7808383951711533\n",
      "val loss per epoch =>  0.6820151364501519 val acc per epoch => 0.7774920886075949 \n",
      "\n",
      "train loss per epoch =>  22 0.6158410106473566 train acc per epoch=>  0.7823569373706417\n",
      "val loss per epoch =>  0.7890880232370352 val acc per epoch => 0.746934335443038 \n",
      "\n",
      "train loss per epoch =>  23 0.6102574156678241 train acc per epoch=>  0.7871683185057872\n",
      "val loss per epoch =>  0.6907542891140226 val acc per epoch => 0.7687895569620253 \n",
      "\n",
      "train loss per epoch =>  24 0.5907539806097669 train acc per epoch=>  0.7921954923883423\n",
      "val loss per epoch =>  0.7680236991447739 val acc per epoch => 0.7689873417721519 \n",
      "\n",
      "train loss per epoch =>  25 0.5810630970141467 train acc per epoch=>  0.7959718670686493\n",
      "val loss per epoch =>  0.6803941349439984 val acc per epoch => 0.7709651898734177 \n",
      "\n",
      "train loss per epoch =>  26 0.5734489378721818 train acc per epoch=>  0.7977541560102301\n",
      "val loss per epoch =>  0.6020927240576925 val acc per epoch => 0.7978639240506329 \n",
      "\n",
      "train loss per epoch =>  27 0.5621016856349642 train acc per epoch=>  0.804084079344864\n",
      "val loss per epoch =>  0.6193762538553793 val acc per epoch => 0.7977650316455697 \n",
      "\n",
      "train loss per epoch =>  28 0.5571743245319942 train acc per epoch=>  0.8055426790891096\n",
      "val loss per epoch =>  0.7851104577885398 val acc per epoch => 0.763251582278481 \n",
      "\n",
      "train loss per epoch =>  29 0.5467244794454111 train acc per epoch=>  0.8080083121119253\n",
      "val loss per epoch =>  0.6409261245516282 val acc per epoch => 0.7820411392405063 \n",
      "\n",
      "train loss per epoch =>  30 0.5390125478777434 train acc per epoch=>  0.8109215153452686\n",
      "val loss per epoch =>  0.5962726160695281 val acc per epoch => 0.8042919303797469 \n",
      "\n",
      "train loss per epoch =>  31 0.5331681130639733 train acc per epoch=>  0.8114769821581633\n",
      "val loss per epoch =>  0.5631365764744675 val acc per epoch => 0.8117088607594937 \n",
      "\n",
      "train loss per epoch =>  32 0.520458014572368 train acc per epoch=>  0.8167639066801047\n",
      "val loss per epoch =>  0.5821314721922332 val acc per epoch => 0.807753164556962 \n",
      "\n",
      "train loss per epoch =>  33 0.5152828566863409 train acc per epoch=>  0.8191775896055314\n",
      "val loss per epoch =>  0.5787642439709434 val acc per epoch => 0.8105221518987342 \n",
      "\n",
      "train loss per epoch =>  34 0.5067543742601829 train acc per epoch=>  0.822678228930744\n",
      "val loss per epoch =>  0.6167794541467594 val acc per epoch => 0.8000395569620253 \n",
      "\n",
      "train loss per epoch =>  35 0.4998296526691798 train acc per epoch=>  0.8235613810436805\n",
      "val loss per epoch =>  0.578344663487205 val acc per epoch => 0.8086431962025317 \n",
      "\n",
      "train loss per epoch =>  36 0.49731479192633765 train acc per epoch=>  0.8254076086956522\n",
      "val loss per epoch =>  1.1200741122040567 val acc per epoch => 0.7783821202531646 \n",
      "\n",
      "train loss per epoch =>  37 0.4874332557858713 train acc per epoch=>  0.8270180626293583\n",
      "val loss per epoch =>  0.5983338597454603 val acc per epoch => 0.8018196202531646 \n",
      "\n",
      "train loss per epoch =>  38 0.4815470971109922 train acc per epoch=>  0.8303788363781122\n",
      "val loss per epoch =>  0.5576617340498333 val acc per epoch => 0.8177412974683544 \n",
      "\n",
      "train loss per epoch =>  39 0.4709735242149714 train acc per epoch=>  0.8346227622398025\n",
      "val loss per epoch =>  0.6081317131277881 val acc per epoch => 0.8027096518987342 \n",
      "\n",
      "train loss per epoch =>  40 0.4682692341182543 train acc per epoch=>  0.8346867007977518\n",
      "val loss per epoch =>  0.6722286683094653 val acc per epoch => 0.7990506329113924 \n",
      "\n",
      "train loss per epoch =>  41 0.45933509543728646 train acc per epoch=>  0.837108375928591\n",
      "val loss per epoch =>  0.5691812438300893 val acc per epoch => 0.8146756329113924 \n",
      "\n",
      "train loss per epoch =>  42 0.45599648341193527 train acc per epoch=>  0.8394701087566288\n",
      "val loss per epoch =>  0.6357384648504136 val acc per epoch => 0.8018196202531646 \n",
      "\n",
      "train loss per epoch =>  43 0.4474523108633583 train acc per epoch=>  0.8430506713554987\n",
      "val loss per epoch =>  0.578985757465604 val acc per epoch => 0.8151700949367089 \n",
      "\n",
      "train loss per epoch =>  44 0.4457303101906691 train acc per epoch=>  0.8425391624345804\n",
      "val loss per epoch =>  0.5595875499369223 val acc per epoch => 0.8173457278481012 \n",
      "\n",
      "train loss per epoch =>  45 0.43545178436409787 train acc per epoch=>  0.8457081202046036\n",
      "val loss per epoch =>  0.5506009206741671 val acc per epoch => 0.8262460443037974 \n",
      "\n",
      "train loss per epoch =>  46 0.4310021204945376 train acc per epoch=>  0.8458399935756498\n",
      "val loss per epoch =>  0.5402920391363434 val acc per epoch => 0.8308939873417721 \n",
      "\n",
      "train loss per epoch =>  47 0.4210612301326469 train acc per epoch=>  0.8493805946901326\n",
      "val loss per epoch =>  0.550218608937686 val acc per epoch => 0.8190268987341772 \n",
      "\n",
      "train loss per epoch =>  48 0.42142524179595203 train acc per epoch=>  0.8512388108026646\n",
      "val loss per epoch =>  0.5512924167928817 val acc per epoch => 0.8227848101265823 \n",
      "\n",
      "train loss per epoch =>  49 0.4143138404773629 train acc per epoch=>  0.8544397378516624\n",
      "val loss per epoch =>  0.572520041767555 val acc per epoch => 0.8187302215189873 \n",
      "\n",
      "train loss per epoch =>  50 0.410144600112115 train acc per epoch=>  0.8547993925831202\n",
      "val loss per epoch =>  0.6757263526131835 val acc per epoch => 0.7987539556962026 \n",
      "\n",
      "train loss per epoch =>  51 0.405804133399978 train acc per epoch=>  0.8539522058518646\n",
      "val loss per epoch =>  0.5503935334803183 val acc per epoch => 0.8227848101265823 \n",
      "\n",
      "train loss per epoch =>  52 0.4020368018189964 train acc per epoch=>  0.8560581841432225\n",
      "val loss per epoch =>  0.5504361481606206 val acc per epoch => 0.8248615506329114 \n",
      "\n",
      "train loss per epoch =>  53 0.3994034697561313 train acc per epoch=>  0.8583719629765777\n",
      "val loss per epoch =>  0.5483868020244792 val acc per epoch => 0.8285205696202531 \n",
      "\n",
      "train loss per epoch =>  54 0.39057772543729113 train acc per epoch=>  0.8629395780356034\n",
      "val loss per epoch =>  0.5389488305472121 val acc per epoch => 0.8271360759493671 \n",
      "\n",
      "train loss per epoch =>  55 0.3843795867908336 train acc per epoch=>  0.8629275895750431\n",
      "val loss per epoch =>  0.5759232413919666 val acc per epoch => 0.8216969936708861 \n",
      "\n",
      "train loss per epoch =>  56 0.3792208048998547 train acc per epoch=>  0.8652093989769821\n",
      "val loss per epoch =>  0.56777758085275 val acc per epoch => 0.8239715189873418 \n",
      "\n",
      "train loss per epoch =>  57 0.37540077183710036 train acc per epoch=>  0.8669077685421995\n",
      "val loss per epoch =>  0.5629108095470863 val acc per epoch => 0.8291139240506329 \n",
      "\n",
      "train loss per epoch =>  58 0.369356219573399 train acc per epoch=>  0.8675631394471659\n",
      "val loss per epoch =>  0.5771555357341525 val acc per epoch => 0.8270371835443038 \n",
      "\n",
      "train loss per epoch =>  59 0.3684739455618822 train acc per epoch=>  0.8693494246438946\n",
      "val loss per epoch =>  0.5423137587082537 val acc per epoch => 0.833564082278481 \n",
      "\n",
      "train loss per epoch =>  60 0.3665632163472188 train acc per epoch=>  0.8693414322860405\n",
      "val loss per epoch =>  0.5674731139140793 val acc per epoch => 0.8229825949367089 \n",
      "\n",
      "train loss per epoch =>  61 0.36031801404093233 train acc per epoch=>  0.8718749999695117\n",
      "val loss per epoch =>  0.5978784945946706 val acc per epoch => 0.821004746835443 \n",
      "\n",
      "train loss per epoch =>  62 0.35597234888149953 train acc per epoch=>  0.8732496803373937\n",
      "val loss per epoch =>  0.5591756543026695 val acc per epoch => 0.8289161392405063 \n",
      "\n",
      "train loss per epoch =>  63 0.3524556717528102 train acc per epoch=>  0.8750279731762683\n",
      "val loss per epoch =>  0.5308068900168696 val acc per epoch => 0.8374208860759493 \n",
      "\n",
      "train loss per epoch =>  64 0.3472906249334745 train acc per epoch=>  0.875187819723583\n",
      "val loss per epoch =>  0.5644529597668708 val acc per epoch => 0.8282238924050633 \n",
      "\n",
      "train loss per epoch =>  65 0.3416770769430853 train acc per epoch=>  0.8773697250334503\n",
      "val loss per epoch =>  0.5668263431591324 val acc per epoch => 0.8315862341772152 \n",
      "\n",
      "train loss per epoch =>  66 0.33628178107768986 train acc per epoch=>  0.8792878836012252\n",
      "val loss per epoch =>  0.5399622358853304 val acc per epoch => 0.8362341772151899 \n",
      "\n",
      "train loss per epoch =>  67 0.33363975824602427 train acc per epoch=>  0.8810581842346874\n",
      "val loss per epoch =>  0.6101570502866672 val acc per epoch => 0.8207080696202531 \n",
      "\n",
      "train loss per epoch =>  68 0.33021830708322014 train acc per epoch=>  0.8823409527159103\n",
      "val loss per epoch =>  0.5645275930815106 val acc per epoch => 0.8301028481012658 \n",
      "\n",
      "train loss per epoch =>  69 0.3257477204589283 train acc per epoch=>  0.8834758632628205\n",
      "val loss per epoch =>  0.5587069271486017 val acc per epoch => 0.8312895569620253 \n",
      "\n",
      "train loss per epoch =>  70 0.320223628598101 train acc per epoch=>  0.8847826086651639\n",
      "val loss per epoch =>  0.556098603749577 val acc per epoch => 0.8331685126582279 \n",
      "\n",
      "train loss per epoch =>  71 0.3205472642884535 train acc per epoch=>  0.8856857417489562\n",
      "val loss per epoch =>  0.5509716881604134 val acc per epoch => 0.8341574367088608 \n",
      "\n",
      "train loss per epoch =>  72 0.31194405943688835 train acc per epoch=>  0.8896259590792839\n",
      "val loss per epoch =>  0.5571610650307015 val acc per epoch => 0.8350474683544303 \n",
      "\n",
      "train loss per epoch =>  73 0.3090223248123818 train acc per epoch=>  0.8877957161430203\n",
      "val loss per epoch =>  0.562170266727858 val acc per epoch => 0.8370253164556962 \n",
      "\n",
      "train loss per epoch =>  74 0.30537284586740576 train acc per epoch=>  0.8911125319997978\n",
      "val loss per epoch =>  0.6007185670771177 val acc per epoch => 0.8278283227848101 \n",
      "\n",
      "train loss per epoch =>  75 0.30260613362502564 train acc per epoch=>  0.8915680947206209\n",
      "val loss per epoch =>  0.5562631393535228 val acc per epoch => 0.8341574367088608 \n",
      "\n",
      "train loss per epoch =>  76 0.2982443533361415 train acc per epoch=>  0.8938139385884375\n",
      "val loss per epoch =>  0.5444663668735118 val acc per epoch => 0.8402887658227848 \n",
      "\n",
      "train loss per epoch =>  77 0.29645718965688933 train acc per epoch=>  0.8934303069358591\n",
      "val loss per epoch =>  0.600726100840146 val acc per epoch => 0.833564082278481 \n",
      "\n",
      "train loss per epoch =>  78 0.28926249862174547 train acc per epoch=>  0.8965912723480283\n",
      "val loss per epoch =>  0.5829217596144616 val acc per epoch => 0.8314873417721519 \n",
      "\n",
      "train loss per epoch =>  79 0.288556173520015 train acc per epoch=>  0.8953884271404627\n",
      "val loss per epoch =>  0.5719483679608454 val acc per epoch => 0.8378164556962026 \n",
      "\n",
      "train loss per epoch =>  80 0.2870064184946172 train acc per epoch=>  0.8971587276214834\n",
      "val loss per epoch =>  0.5686675147919715 val acc per epoch => 0.8394976265822784 \n",
      "\n",
      "train loss per epoch =>  81 0.2836471294503078 train acc per epoch=>  0.8994245524601558\n",
      "val loss per epoch =>  0.5628685019438779 val acc per epoch => 0.8398931962025317 \n",
      "\n",
      "train loss per epoch =>  82 0.27525509063087766 train acc per epoch=>  0.90117886837791\n",
      "val loss per epoch =>  0.5779441214060481 val acc per epoch => 0.8334651898734177 \n",
      "\n",
      "train loss per epoch =>  83 0.2748397953827363 train acc per epoch=>  0.9024816176775471\n",
      "val loss per epoch =>  0.6177867730207081 val acc per epoch => 0.8268393987341772 \n",
      "\n",
      "train loss per epoch =>  84 0.27178992024239373 train acc per epoch=>  0.9020540281329923\n",
      "val loss per epoch =>  0.5672427762912798 val acc per epoch => 0.8378164556962026 \n",
      "\n",
      "train loss per epoch =>  85 0.26822728130137524 train acc per epoch=>  0.9042399296980075\n",
      "val loss per epoch =>  0.5793638110538072 val acc per epoch => 0.8398931962025317 \n",
      "\n",
      "train loss per epoch =>  86 0.26123330284796104 train acc per epoch=>  0.9056665601632784\n",
      "val loss per epoch =>  0.5816535225397423 val acc per epoch => 0.837618670886076 \n",
      "\n",
      "train loss per epoch =>  87 0.2641268832528073 train acc per epoch=>  0.905758471897496\n",
      "val loss per epoch =>  0.5798620917374575 val acc per epoch => 0.8402887658227848 \n",
      "\n",
      "train loss per epoch =>  88 0.2586726584969579 train acc per epoch=>  0.9077405690232201\n",
      "val loss per epoch =>  0.6005344198474402 val acc per epoch => 0.8373219936708861 \n",
      "\n",
      "train loss per epoch =>  89 0.2519344681745295 train acc per epoch=>  0.9105298913958128\n",
      "val loss per epoch =>  0.6150437619867204 val acc per epoch => 0.8365308544303798 \n",
      "\n",
      "train loss per epoch =>  90 0.24734734128350797 train acc per epoch=>  0.9104939258616903\n",
      "val loss per epoch =>  0.6038025662868838 val acc per epoch => 0.836629746835443 \n",
      "\n",
      "train loss per epoch =>  91 0.24488744063450552 train acc per epoch=>  0.9118526215138643\n",
      "val loss per epoch =>  0.6028120755394802 val acc per epoch => 0.8389042721518988 \n",
      "\n",
      "train loss per epoch =>  92 0.23895183409495122 train acc per epoch=>  0.9132912404396955\n",
      "val loss per epoch =>  0.5709593081021611 val acc per epoch => 0.8404865506329114 \n",
      "\n",
      "train loss per epoch =>  93 0.24059550311711744 train acc per epoch=>  0.9132872443369893\n",
      "val loss per epoch =>  0.6153596435921102 val acc per epoch => 0.8392998417721519 \n",
      "\n",
      "train loss per epoch =>  94 0.235761681614477 train acc per epoch=>  0.9158807544757033\n",
      "val loss per epoch =>  0.6354389058638222 val acc per epoch => 0.8356408227848101 \n",
      "\n",
      "train loss per epoch =>  95 0.23248279123278834 train acc per epoch=>  0.9167479220253733\n",
      "val loss per epoch =>  0.6569125414649143 val acc per epoch => 0.8284216772151899 \n",
      "\n",
      "train loss per epoch =>  96 0.23168758765968214 train acc per epoch=>  0.9172074808489026\n",
      "val loss per epoch =>  0.5933056858521474 val acc per epoch => 0.8395965189873418 \n",
      "\n",
      "train loss per epoch =>  97 0.22212521922405418 train acc per epoch=>  0.9196571291560103\n",
      "val loss per epoch =>  0.6292938141128684 val acc per epoch => 0.8372231012658228 \n",
      "\n",
      "train loss per epoch =>  98 0.22378741521054826 train acc per epoch=>  0.9186980498721228\n",
      "val loss per epoch =>  0.6278307441669174 val acc per epoch => 0.8343552215189873 \n",
      "\n",
      "train loss per epoch =>  99 0.22160836593116945 train acc per epoch=>  0.9201886188953429\n",
      "val loss per epoch =>  0.609282897997506 val acc per epoch => 0.8398931962025317 \n",
      "\n",
      "train loss per epoch =>  100 0.2190543895449175 train acc per epoch=>  0.9218909847157081\n",
      "val loss per epoch =>  0.6352234818512881 val acc per epoch => 0.8382120253164557 \n",
      "\n",
      "train loss per epoch =>  101 0.20981729285948722 train acc per epoch=>  0.9241168479175519\n",
      "val loss per epoch =>  0.6371241638177558 val acc per epoch => 0.8392998417721519 \n",
      "\n",
      "train loss per epoch =>  102 0.2125645280265442 train acc per epoch=>  0.9240049552124785\n",
      "val loss per epoch =>  0.6366646108748037 val acc per epoch => 0.8372231012658228 \n",
      "\n",
      "train loss per epoch =>  103 0.20731413036661075 train acc per epoch=>  0.9247522378516624\n",
      "val loss per epoch =>  0.6793778550021256 val acc per epoch => 0.829806170886076 \n",
      "\n",
      "train loss per epoch =>  104 0.20350648245543165 train acc per epoch=>  0.926474584490442\n",
      "val loss per epoch =>  0.6477596484407594 val acc per epoch => 0.8370253164556962 \n",
      "\n",
      "train loss per epoch =>  105 0.20428857184432048 train acc per epoch=>  0.9271699168797954\n",
      "val loss per epoch =>  0.6817186048513726 val acc per epoch => 0.833564082278481 \n",
      "\n",
      "train loss per epoch =>  106 0.2039288157201789 train acc per epoch=>  0.926530530690537\n",
      "val loss per epoch =>  0.637859916385216 val acc per epoch => 0.8384098101265823 \n",
      "\n",
      "train loss per epoch =>  107 0.2018434712306008 train acc per epoch=>  0.9272258632323321\n",
      "val loss per epoch =>  0.6469381523283222 val acc per epoch => 0.8362341772151899 \n",
      "\n",
      "train loss per epoch =>  108 0.19242085327806374 train acc per epoch=>  0.9298873081841432\n",
      "val loss per epoch =>  0.6450855034061626 val acc per epoch => 0.8355419303797469 \n",
      "\n",
      "train loss per epoch =>  109 0.1944402010773149 train acc per epoch=>  0.9308184142917624\n",
      "val loss per epoch =>  0.6496236780776253 val acc per epoch => 0.8412776898734177 \n",
      "\n",
      "train loss per epoch =>  110 0.18788626892944735 train acc per epoch=>  0.9329763426805091\n",
      "val loss per epoch =>  0.6409978855259811 val acc per epoch => 0.8424643987341772 \n",
      "\n",
      "train loss per epoch =>  111 0.18593793854003063 train acc per epoch=>  0.9326766304042943\n",
      "val loss per epoch =>  0.6349548938908155 val acc per epoch => 0.8415743670886076 \n",
      "\n",
      "train loss per epoch =>  112 0.1835472534894181 train acc per epoch=>  0.9323289642858383\n",
      "val loss per epoch =>  0.6645822823047638 val acc per epoch => 0.8431566455696202 \n",
      "\n",
      "train loss per epoch =>  113 0.1785406607305607 train acc per epoch=>  0.9369205562659847\n",
      "val loss per epoch =>  0.6550242912165726 val acc per epoch => 0.8397943037974683 \n",
      "\n",
      "train loss per epoch =>  114 0.18109695293257 train acc per epoch=>  0.9341751917853685\n",
      "val loss per epoch =>  0.6270490892325775 val acc per epoch => 0.8455300632911392 \n",
      "\n",
      "train loss per epoch =>  115 0.17534876234657928 train acc per epoch=>  0.9361093350688515\n",
      "val loss per epoch =>  0.6611061228227012 val acc per epoch => 0.8440466772151899 \n",
      "\n",
      "train loss per epoch =>  116 0.16711447410799962 train acc per epoch=>  0.9392023658203652\n",
      "val loss per epoch =>  0.6779376075992102 val acc per epoch => 0.8411787974683544 \n",
      "\n",
      "train loss per epoch =>  117 0.1671692265002319 train acc per epoch=>  0.9399976023017903\n",
      "val loss per epoch =>  0.6610452600672275 val acc per epoch => 0.8465189873417721 \n",
      "\n",
      "train loss per epoch =>  118 0.1654244354447288 train acc per epoch=>  0.9403013108331529\n",
      "val loss per epoch =>  0.6716393183303785 val acc per epoch => 0.8410799050632911 \n",
      "\n",
      "train loss per epoch =>  119 0.16342219428333174 train acc per epoch=>  0.9416160485933504\n",
      "val loss per epoch =>  0.698489798020713 val acc per epoch => 0.8362341772151899 \n",
      "\n",
      "train loss per epoch =>  120 0.16246449359504464 train acc per epoch=>  0.9421075766958544\n",
      "val loss per epoch =>  0.6634224321268782 val acc per epoch => 0.8446400316455697 \n",
      "\n",
      "train loss per epoch =>  121 0.15659500523220243 train acc per epoch=>  0.9438059462610718\n",
      "val loss per epoch =>  0.6866465425944026 val acc per epoch => 0.84375 \n",
      "\n",
      "train loss per epoch =>  122 0.15465838255365486 train acc per epoch=>  0.9445452365424017\n",
      "val loss per epoch =>  0.6753679227225388 val acc per epoch => 0.8456289556962026 \n",
      "\n",
      "train loss per epoch =>  123 0.15433186494633364 train acc per epoch=>  0.9442375320607744\n",
      "val loss per epoch =>  0.7054180268999897 val acc per epoch => 0.8441455696202531 \n",
      "\n",
      "train loss per epoch =>  124 0.15185530860062754 train acc per epoch=>  0.9459838554682329\n",
      "val loss per epoch =>  0.667223633090152 val acc per epoch => 0.8456289556962026 \n",
      "\n",
      "train loss per epoch =>  125 0.1485674563137924 train acc per epoch=>  0.9474504475703325\n",
      "val loss per epoch =>  0.6758237907403633 val acc per epoch => 0.8487935126582279 \n",
      "\n",
      "train loss per epoch =>  126 0.14778987466907867 train acc per epoch=>  0.9466432224759056\n",
      "val loss per epoch =>  0.717142516299139 val acc per epoch => 0.8421677215189873 \n",
      "\n",
      "train loss per epoch =>  127 0.14573489121921226 train acc per epoch=>  0.9481777493911021\n",
      "val loss per epoch =>  0.6889484671852256 val acc per epoch => 0.8439477848101266 \n",
      "\n",
      "train loss per epoch =>  128 0.13995848718049275 train acc per epoch=>  0.9496403452380539\n",
      "val loss per epoch =>  0.7074150096012067 val acc per epoch => 0.8449367088607594 \n",
      "\n",
      "train loss per epoch =>  129 0.14155562633596114 train acc per epoch=>  0.9492047634880866\n",
      "val loss per epoch =>  0.7159043614622913 val acc per epoch => 0.8426621835443038 \n",
      "\n",
      "train loss per epoch =>  130 0.13862450414187158 train acc per epoch=>  0.9499280690537084\n",
      "val loss per epoch =>  0.6921158753618409 val acc per epoch => 0.8472112341772152 \n",
      "\n",
      "train loss per epoch =>  131 0.1335902575050931 train acc per epoch=>  0.9516224425162196\n",
      "val loss per epoch =>  0.7163087027736857 val acc per epoch => 0.8470134493670886 \n",
      "\n",
      "train loss per epoch =>  132 0.12971656001589793 train acc per epoch=>  0.9526094949763754\n",
      "val loss per epoch =>  0.7057919570162327 val acc per epoch => 0.8485957278481012 \n",
      "\n",
      "train loss per epoch =>  133 0.13136706078220206 train acc per epoch=>  0.9519341432529947\n",
      "val loss per epoch =>  0.7243383673927452 val acc per epoch => 0.8475079113924051 \n",
      "\n",
      "train loss per epoch =>  134 0.12822316094394534 train acc per epoch=>  0.9542479220863498\n",
      "val loss per epoch =>  0.7163438334872451 val acc per epoch => 0.8497824367088608 \n",
      "\n",
      "train loss per epoch =>  135 0.1296847582463642 train acc per epoch=>  0.9550431585677749\n",
      "val loss per epoch =>  0.7310468652580357 val acc per epoch => 0.8472112341772152 \n",
      "\n",
      "train loss per epoch =>  136 0.12548647422696013 train acc per epoch=>  0.9550031969309463\n",
      "val loss per epoch =>  0.7533640778517421 val acc per epoch => 0.8493868670886076 \n",
      "\n",
      "train loss per epoch =>  137 0.1238494573346794 train acc per epoch=>  0.9558383952016416\n",
      "val loss per epoch =>  0.7463458493540559 val acc per epoch => 0.84375 \n",
      "\n",
      "train loss per epoch =>  138 0.12418823416256691 train acc per epoch=>  0.9562180307515137\n",
      "val loss per epoch =>  0.7434200893474531 val acc per epoch => 0.8496835443037974 \n",
      "\n",
      "train loss per epoch =>  139 0.12084290391439215 train acc per epoch=>  0.9562060422909534\n",
      "val loss per epoch =>  0.7118145974376534 val acc per epoch => 0.8503757911392406 \n",
      "\n",
      "train loss per epoch =>  140 0.11804564173340493 train acc per epoch=>  0.9576286765010765\n",
      "val loss per epoch =>  0.7354808264895331 val acc per epoch => 0.8481012658227848 \n",
      "\n",
      "train loss per epoch =>  141 0.1148275333692503 train acc per epoch=>  0.9590632993242015\n",
      "val loss per epoch =>  0.7324300623392757 val acc per epoch => 0.8500791139240507 \n",
      "\n",
      "train loss per epoch =>  142 0.1148334304561548 train acc per epoch=>  0.958164162495557\n",
      "val loss per epoch =>  0.7357067243207859 val acc per epoch => 0.8469145569620253 \n",
      "\n",
      "train loss per epoch =>  143 0.11225724966286699 train acc per epoch=>  0.9599504475398442\n",
      "val loss per epoch =>  0.7473494508598424 val acc per epoch => 0.8487935126582279 \n",
      "\n",
      "train loss per epoch =>  144 0.11032933566500158 train acc per epoch=>  0.9611293158262891\n",
      "val loss per epoch =>  0.7573210857337034 val acc per epoch => 0.8494857594936709 \n",
      "\n",
      "train loss per epoch =>  145 0.10751944030527874 train acc per epoch=>  0.9613690856472611\n",
      "val loss per epoch =>  0.7665292880957639 val acc per epoch => 0.8480023734177216 \n",
      "\n",
      "train loss per epoch =>  146 0.1090229951045321 train acc per epoch=>  0.9611932545366799\n",
      "val loss per epoch =>  0.7611190076870255 val acc per epoch => 0.8512658227848101 \n",
      "\n",
      "train loss per epoch =>  147 0.10136166767543539 train acc per epoch=>  0.9640265344963659\n",
      "val loss per epoch =>  0.7860266747353952 val acc per epoch => 0.8453322784810127 \n",
      "\n",
      "train loss per epoch =>  148 0.10163700201874956 train acc per epoch=>  0.9640185422909534\n",
      "val loss per epoch =>  0.7873850106438504 val acc per epoch => 0.8475079113924051 \n",
      "\n",
      "train loss per epoch =>  149 0.09994783087173843 train acc per epoch=>  0.9650295716722298\n",
      "val loss per epoch =>  0.7883642217780971 val acc per epoch => 0.8449367088607594 \n",
      "\n",
      "train loss per epoch =>  150 0.10188089160586866 train acc per epoch=>  0.9630115090123833\n",
      "val loss per epoch =>  0.7701141841803925 val acc per epoch => 0.8501780063291139 \n",
      "\n",
      "train loss per epoch =>  151 0.09561187773942947 train acc per epoch=>  0.9654052109669542\n",
      "val loss per epoch =>  0.7891076478777053 val acc per epoch => 0.8478045886075949 \n",
      "\n",
      "train loss per epoch =>  152 0.10087658591625635 train acc per epoch=>  0.9640984655646108\n",
      "val loss per epoch =>  0.7704133010363277 val acc per epoch => 0.8482001582278481 \n",
      "\n",
      "train loss per epoch =>  153 0.09575371114570466 train acc per epoch=>  0.9663802749665497\n",
      "val loss per epoch =>  0.7881543300574338 val acc per epoch => 0.8481012658227848 \n",
      "\n",
      "train loss per epoch =>  154 0.09364557705457559 train acc per epoch=>  0.9670835997137572\n",
      "val loss per epoch =>  0.7969634189635892 val acc per epoch => 0.8481012658227848 \n",
      "\n",
      "train loss per epoch =>  155 0.08897579318422186 train acc per epoch=>  0.9690617007367751\n",
      "val loss per epoch =>  0.8027424355850944 val acc per epoch => 0.8501780063291139 \n",
      "\n",
      "train loss per epoch =>  156 0.09122120089771803 train acc per epoch=>  0.9668198529716647\n",
      "val loss per epoch =>  0.8017934945565236 val acc per epoch => 0.8497824367088608 \n",
      "\n",
      "train loss per epoch =>  157 0.08965001868374665 train acc per epoch=>  0.9678348786080889\n",
      "val loss per epoch =>  0.8091277084018611 val acc per epoch => 0.8510680379746836 \n",
      "\n",
      "train loss per epoch =>  158 0.08790503893895528 train acc per epoch=>  0.9696371483680842\n",
      "val loss per epoch =>  0.8107099502901488 val acc per epoch => 0.8475079113924051 \n",
      "\n",
      "train loss per epoch =>  159 0.08566225635940614 train acc per epoch=>  0.970064737912639\n",
      "val loss per epoch =>  0.8253986956198004 val acc per epoch => 0.8485957278481012 \n",
      "\n",
      "train loss per epoch =>  160 0.08542107406746396 train acc per epoch=>  0.9699648338205674\n",
      "val loss per epoch =>  0.8174800910527193 val acc per epoch => 0.8478045886075949 \n",
      "\n",
      "train loss per epoch =>  161 0.0828177088828724 train acc per epoch=>  0.9705682544757033\n",
      "val loss per epoch =>  0.8262853414952 val acc per epoch => 0.8482990506329114 \n",
      "\n",
      "train loss per epoch =>  162 0.08543600721994553 train acc per epoch=>  0.9692335358970915\n",
      "val loss per epoch =>  0.8290520283994796 val acc per epoch => 0.8460245253164557 \n",
      "\n",
      "train loss per epoch =>  163 0.08188009630802952 train acc per epoch=>  0.9707560741992862\n",
      "val loss per epoch =>  0.8307682675651357 val acc per epoch => 0.8480023734177216 \n",
      "\n",
      "train loss per epoch =>  164 0.07926328026253701 train acc per epoch=>  0.9712515985569381\n",
      "val loss per epoch =>  0.8426659152477602 val acc per epoch => 0.8470134493670886 \n",
      "\n",
      "train loss per epoch =>  165 0.08054708648482552 train acc per epoch=>  0.9708000319388211\n",
      "val loss per epoch =>  0.8464148308657393 val acc per epoch => 0.846815664556962 \n",
      "\n",
      "train loss per epoch =>  166 0.07846007765987721 train acc per epoch=>  0.9718989769516089\n",
      "val loss per epoch =>  0.8389104514182368 val acc per epoch => 0.8485957278481012 \n",
      "\n",
      "train loss per epoch =>  167 0.07794901103143344 train acc per epoch=>  0.972474424582918\n",
      "val loss per epoch =>  0.8337124529518659 val acc per epoch => 0.8502768987341772 \n",
      "\n",
      "train loss per epoch =>  168 0.07909451053022881 train acc per epoch=>  0.9723825128487004\n",
      "val loss per epoch =>  0.8423114471797701 val acc per epoch => 0.8488924050632911 \n",
      "\n",
      "train loss per epoch =>  169 0.07567158452404277 train acc per epoch=>  0.9736093351298281\n",
      "val loss per epoch =>  0.8466938549204718 val acc per epoch => 0.8486946202531646 \n",
      "\n",
      "train loss per epoch =>  170 0.07394180266315217 train acc per epoch=>  0.9735014386799025\n",
      "val loss per epoch =>  0.8536475685578359 val acc per epoch => 0.849881329113924 \n",
      "\n",
      "train loss per epoch =>  171 0.07770600384744385 train acc per epoch=>  0.9723145780356034\n",
      "val loss per epoch =>  0.8445244420178329 val acc per epoch => 0.8488924050632911 \n",
      "\n",
      "train loss per epoch =>  172 0.06951531615641797 train acc per epoch=>  0.9749320652478796\n",
      "val loss per epoch =>  0.8587110242511653 val acc per epoch => 0.8475079113924051 \n",
      "\n",
      "train loss per epoch =>  173 0.07289436911625782 train acc per epoch=>  0.9748801151504907\n",
      "val loss per epoch =>  0.8587354078323026 val acc per epoch => 0.8488924050632911 \n",
      "\n",
      "train loss per epoch =>  174 0.07348172495718044 train acc per epoch=>  0.9740968669771843\n",
      "val loss per epoch =>  0.8539248961436597 val acc per epoch => 0.8486946202531646 \n",
      "\n",
      "train loss per epoch =>  175 0.07157082065387303 train acc per epoch=>  0.9754315857082376\n",
      "val loss per epoch =>  0.8626698798771146 val acc per epoch => 0.8489912974683544 \n",
      "\n",
      "train loss per epoch =>  176 0.06866034177486854 train acc per epoch=>  0.9752277814213882\n",
      "val loss per epoch =>  0.8770271901842914 val acc per epoch => 0.8493868670886076 \n",
      "\n",
      "train loss per epoch =>  177 0.06938733074032818 train acc per epoch=>  0.9745084719584726\n",
      "val loss per epoch =>  0.8746885302700574 val acc per epoch => 0.8480023734177216 \n",
      "\n",
      "train loss per epoch =>  178 0.07197151476722163 train acc per epoch=>  0.9752197890635341\n",
      "val loss per epoch =>  0.8653490875340715 val acc per epoch => 0.849189082278481 \n",
      "\n",
      "train loss per epoch =>  179 0.07121629962969162 train acc per epoch=>  0.9751598466082912\n",
      "val loss per epoch =>  0.8586813475512252 val acc per epoch => 0.8482001582278481 \n",
      "\n",
      "train loss per epoch =>  180 0.06685549688175359 train acc per epoch=>  0.9761908568079819\n",
      "val loss per epoch =>  0.8719447202320341 val acc per epoch => 0.8503757911392406 \n",
      "\n",
      "train loss per epoch =>  181 0.0682294172475405 train acc per epoch=>  0.9759550830897163\n",
      "val loss per epoch =>  0.8623382185078874 val acc per epoch => 0.8506724683544303 \n",
      "\n",
      "train loss per epoch =>  182 0.06973975909102108 train acc per epoch=>  0.9754635549872123\n",
      "val loss per epoch =>  0.8651575431039061 val acc per epoch => 0.8504746835443038 \n",
      "\n",
      "train loss per epoch =>  183 0.0679096635907431 train acc per epoch=>  0.9755514706187236\n",
      "val loss per epoch =>  0.8799581584296648 val acc per epoch => 0.8505735759493671 \n",
      "\n",
      "train loss per epoch =>  184 0.06667519468204368 train acc per epoch=>  0.9763906649921251\n",
      "val loss per epoch =>  0.8772668023652668 val acc per epoch => 0.8483979430379747 \n",
      "\n",
      "train loss per epoch =>  185 0.06274874345220798 train acc per epoch=>  0.9776814258312021\n",
      "val loss per epoch =>  0.8788810817501213 val acc per epoch => 0.8501780063291139 \n",
      "\n",
      "train loss per epoch =>  186 0.06668571027619835 train acc per epoch=>  0.9763067456157616\n",
      "val loss per epoch =>  0.8837965964516507 val acc per epoch => 0.8509691455696202 \n",
      "\n",
      "train loss per epoch =>  187 0.06519344706288384 train acc per epoch=>  0.9769421355498721\n",
      "val loss per epoch =>  0.8932734422291382 val acc per epoch => 0.8505735759493671 \n",
      "\n",
      "train loss per epoch =>  188 0.0640845204229512 train acc per epoch=>  0.9775895140969845\n",
      "val loss per epoch =>  0.878354397755635 val acc per epoch => 0.8496835443037974 \n",
      "\n",
      "train loss per epoch =>  189 0.06352701111484671 train acc per epoch=>  0.9775175831811812\n",
      "val loss per epoch =>  0.8868845655948301 val acc per epoch => 0.8507713607594937 \n",
      "\n",
      "train loss per epoch =>  190 0.06508265263960718 train acc per epoch=>  0.9767583120814369\n",
      "val loss per epoch =>  0.8788793641555158 val acc per epoch => 0.8499802215189873 \n",
      "\n",
      "train loss per epoch =>  191 0.06355606703816549 train acc per epoch=>  0.9776374680916672\n",
      "val loss per epoch =>  0.8771051575865927 val acc per epoch => 0.848496835443038 \n",
      "\n",
      "train loss per epoch =>  192 0.06638478808095465 train acc per epoch=>  0.9767902813604116\n",
      "val loss per epoch =>  0.8880991498126259 val acc per epoch => 0.849881329113924 \n",
      "\n",
      "train loss per epoch =>  193 0.0623660836752285 train acc per epoch=>  0.9781210038363172\n",
      "val loss per epoch =>  0.8811241124249711 val acc per epoch => 0.8505735759493671 \n",
      "\n",
      "train loss per epoch =>  194 0.06309424619645337 train acc per epoch=>  0.9777733375654196\n",
      "val loss per epoch =>  0.8835759611823891 val acc per epoch => 0.8497824367088608 \n",
      "\n",
      "train loss per epoch =>  195 0.06320717328530558 train acc per epoch=>  0.977189897728698\n",
      "val loss per epoch =>  0.8845044366921051 val acc per epoch => 0.8512658227848101 \n",
      "\n",
      "train loss per epoch =>  196 0.06473103601990453 train acc per epoch=>  0.9775775256364242\n",
      "val loss per epoch =>  0.8837495306624642 val acc per epoch => 0.8510680379746836 \n",
      "\n",
      "train loss per epoch =>  197 0.06512400981686685 train acc per epoch=>  0.9771059783523345\n",
      "val loss per epoch =>  0.8825721891620492 val acc per epoch => 0.8509691455696202 \n",
      "\n",
      "train loss per epoch =>  198 0.0641699448356505 train acc per epoch=>  0.9774896100049129\n",
      "val loss per epoch =>  0.8750300603576854 val acc per epoch => 0.8507713607594937 \n",
      "\n",
      "train loss per epoch =>  199 0.06420187894112009 train acc per epoch=>  0.9774096867312556\n",
      "val loss per epoch =>  0.8863181385058391 val acc per epoch => 0.8495846518987342 \n",
      "\n",
      "CPU times: user 26min 55s, sys: 6min 9s, total: 33min 5s\n",
      "Wall time: 35min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=200\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./og_resnet.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8512658227848101"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35692135551396537,\n",
       " 0.4820052749665497,\n",
       " 0.5411604859335039,\n",
       " 0.584011349074371,\n",
       " 0.6107776535441504,\n",
       " 0.6387787723480283,\n",
       " 0.6609255115089514,\n",
       " 0.6775135870479867,\n",
       " 0.6896659207466008,\n",
       " 0.7027213875290073,\n",
       " 0.7125679347521204,\n",
       " 0.7227541560407185,\n",
       " 0.7297074808489026,\n",
       " 0.7356617647363707,\n",
       " 0.7420196610948314,\n",
       " 0.7515744885520252,\n",
       " 0.757209079344864,\n",
       " 0.7630434783218462,\n",
       " 0.7681226023017903,\n",
       " 0.7710437980454291,\n",
       " 0.7744565217391305,\n",
       " 0.7808383951711533,\n",
       " 0.7823569373706417,\n",
       " 0.7871683185057872,\n",
       " 0.7921954923883423,\n",
       " 0.7959718670686493,\n",
       " 0.7977541560102301,\n",
       " 0.804084079344864,\n",
       " 0.8055426790891096,\n",
       " 0.8080083121119253,\n",
       " 0.8109215153452686,\n",
       " 0.8114769821581633,\n",
       " 0.8167639066801047,\n",
       " 0.8191775896055314,\n",
       " 0.822678228930744,\n",
       " 0.8235613810436805,\n",
       " 0.8254076086956522,\n",
       " 0.8270180626293583,\n",
       " 0.8303788363781122,\n",
       " 0.8346227622398025,\n",
       " 0.8346867007977518,\n",
       " 0.837108375928591,\n",
       " 0.8394701087566288,\n",
       " 0.8430506713554987,\n",
       " 0.8425391624345804,\n",
       " 0.8457081202046036,\n",
       " 0.8458399935756498,\n",
       " 0.8493805946901326,\n",
       " 0.8512388108026646,\n",
       " 0.8544397378516624,\n",
       " 0.8547993925831202,\n",
       " 0.8539522058518646,\n",
       " 0.8560581841432225,\n",
       " 0.8583719629765777,\n",
       " 0.8629395780356034,\n",
       " 0.8629275895750431,\n",
       " 0.8652093989769821,\n",
       " 0.8669077685421995,\n",
       " 0.8675631394471659,\n",
       " 0.8693494246438946,\n",
       " 0.8693414322860405,\n",
       " 0.8718749999695117,\n",
       " 0.8732496803373937,\n",
       " 0.8750279731762683,\n",
       " 0.875187819723583,\n",
       " 0.8773697250334503,\n",
       " 0.8792878836012252,\n",
       " 0.8810581842346874,\n",
       " 0.8823409527159103,\n",
       " 0.8834758632628205,\n",
       " 0.8847826086651639,\n",
       " 0.8856857417489562,\n",
       " 0.8896259590792839,\n",
       " 0.8877957161430203,\n",
       " 0.8911125319997978,\n",
       " 0.8915680947206209,\n",
       " 0.8938139385884375,\n",
       " 0.8934303069358591,\n",
       " 0.8965912723480283,\n",
       " 0.8953884271404627,\n",
       " 0.8971587276214834,\n",
       " 0.8994245524601558,\n",
       " 0.90117886837791,\n",
       " 0.9024816176775471,\n",
       " 0.9020540281329923,\n",
       " 0.9042399296980075,\n",
       " 0.9056665601632784,\n",
       " 0.905758471897496,\n",
       " 0.9077405690232201,\n",
       " 0.9105298913958128,\n",
       " 0.9104939258616903,\n",
       " 0.9118526215138643,\n",
       " 0.9132912404396955,\n",
       " 0.9132872443369893,\n",
       " 0.9158807544757033,\n",
       " 0.9167479220253733,\n",
       " 0.9172074808489026,\n",
       " 0.9196571291560103,\n",
       " 0.9186980498721228,\n",
       " 0.9201886188953429,\n",
       " 0.9218909847157081,\n",
       " 0.9241168479175519,\n",
       " 0.9240049552124785,\n",
       " 0.9247522378516624,\n",
       " 0.926474584490442,\n",
       " 0.9271699168797954,\n",
       " 0.926530530690537,\n",
       " 0.9272258632323321,\n",
       " 0.9298873081841432,\n",
       " 0.9308184142917624,\n",
       " 0.9329763426805091,\n",
       " 0.9326766304042943,\n",
       " 0.9323289642858383,\n",
       " 0.9369205562659847,\n",
       " 0.9341751917853685,\n",
       " 0.9361093350688515,\n",
       " 0.9392023658203652,\n",
       " 0.9399976023017903,\n",
       " 0.9403013108331529,\n",
       " 0.9416160485933504,\n",
       " 0.9421075766958544,\n",
       " 0.9438059462610718,\n",
       " 0.9445452365424017,\n",
       " 0.9442375320607744,\n",
       " 0.9459838554682329,\n",
       " 0.9474504475703325,\n",
       " 0.9466432224759056,\n",
       " 0.9481777493911021,\n",
       " 0.9496403452380539,\n",
       " 0.9492047634880866,\n",
       " 0.9499280690537084,\n",
       " 0.9516224425162196,\n",
       " 0.9526094949763754,\n",
       " 0.9519341432529947,\n",
       " 0.9542479220863498,\n",
       " 0.9550431585677749,\n",
       " 0.9550031969309463,\n",
       " 0.9558383952016416,\n",
       " 0.9562180307515137,\n",
       " 0.9562060422909534,\n",
       " 0.9576286765010765,\n",
       " 0.9590632993242015,\n",
       " 0.958164162495557,\n",
       " 0.9599504475398442,\n",
       " 0.9611293158262891,\n",
       " 0.9613690856472611,\n",
       " 0.9611932545366799,\n",
       " 0.9640265344963659,\n",
       " 0.9640185422909534,\n",
       " 0.9650295716722298,\n",
       " 0.9630115090123833,\n",
       " 0.9654052109669542,\n",
       " 0.9640984655646108,\n",
       " 0.9663802749665497,\n",
       " 0.9670835997137572,\n",
       " 0.9690617007367751,\n",
       " 0.9668198529716647,\n",
       " 0.9678348786080889,\n",
       " 0.9696371483680842,\n",
       " 0.970064737912639,\n",
       " 0.9699648338205674,\n",
       " 0.9705682544757033,\n",
       " 0.9692335358970915,\n",
       " 0.9707560741992862,\n",
       " 0.9712515985569381,\n",
       " 0.9708000319388211,\n",
       " 0.9718989769516089,\n",
       " 0.972474424582918,\n",
       " 0.9723825128487004,\n",
       " 0.9736093351298281,\n",
       " 0.9735014386799025,\n",
       " 0.9723145780356034,\n",
       " 0.9749320652478796,\n",
       " 0.9748801151504907,\n",
       " 0.9740968669771843,\n",
       " 0.9754315857082376,\n",
       " 0.9752277814213882,\n",
       " 0.9745084719584726,\n",
       " 0.9752197890635341,\n",
       " 0.9751598466082912,\n",
       " 0.9761908568079819,\n",
       " 0.9759550830897163,\n",
       " 0.9754635549872123,\n",
       " 0.9755514706187236,\n",
       " 0.9763906649921251,\n",
       " 0.9776814258312021,\n",
       " 0.9763067456157616,\n",
       " 0.9769421355498721,\n",
       " 0.9775895140969845,\n",
       " 0.9775175831811812,\n",
       " 0.9767583120814369,\n",
       " 0.9776374680916672,\n",
       " 0.9767902813604116,\n",
       " 0.9781210038363172,\n",
       " 0.9777733375654196,\n",
       " 0.977189897728698,\n",
       " 0.9775775256364242,\n",
       " 0.9771059783523345,\n",
       " 0.9774896100049129,\n",
       " 0.9774096867312556]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
