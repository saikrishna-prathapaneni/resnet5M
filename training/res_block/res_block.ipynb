{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]           4,704\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              ReLU-3         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 32, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          18,432\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "           Conv2d-13           [-1, 64, 56, 56]           2,048\n",
      "      BatchNorm2d-14           [-1, 64, 56, 56]             128\n",
      "           ResNet-15           [-1, 64, 56, 56]               0\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "             ReLU-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "           ResNet-24           [-1, 64, 56, 56]               0\n",
      "           Conv2d-25           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
      "             ReLU-27           [-1, 64, 56, 56]               0\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "             ReLU-30           [-1, 64, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "           ResNet-33           [-1, 64, 56, 56]               0\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "             ReLU-36           [-1, 64, 56, 56]               0\n",
      "           Conv2d-37           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-38           [-1, 64, 56, 56]             128\n",
      "             ReLU-39           [-1, 64, 56, 56]               0\n",
      "           Conv2d-40           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-41           [-1, 64, 56, 56]             128\n",
      "           ResNet-42           [-1, 64, 56, 56]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "             ReLU-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "           Conv2d-51          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
      "           ResNet-53          [-1, 128, 28, 28]               0\n",
      "           Conv2d-54          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "           ResNet-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "             ReLU-74          [-1, 256, 14, 14]               0\n",
      "           Conv2d-75          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "           Conv2d-78          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-79          [-1, 256, 14, 14]             512\n",
      "           Conv2d-80          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "           ResNet-82          [-1, 256, 14, 14]               0\n",
      "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
      "             ReLU-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "           ResNet-91          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-92            [-1, 256, 1, 1]               0\n",
      "          Flatten-93                  [-1, 256]               0\n",
      "           Linear-94                   [-1, 10]           2,570\n",
      "          Softmax-95                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 4,979,498\n",
      "Trainable params: 4,979,498\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 98.00\n",
      "Params size (MB): 19.00\n",
      "Estimated Total Size (MB): 117.57\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNetF(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU()\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (resblock1): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (skip_conv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (skip_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (resblock2): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblock3): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblock5): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblock6): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (skip_conv): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skip_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (resblock7): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblock10): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (resblock11): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (skip_conv): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "    (skip_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (resblock12): ResNet(\n",
       "    (block): Sequential(\n",
       "      (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (soft): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define a ResNet block with the Residual block change\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "        )\n",
    "        \n",
    "        # If the input and output channels are different or the stride is 2, we need to apply a 1x1 convolution\n",
    "        # to the input to match the output dimensions. This is called a skip connection.\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            # If there is a skip connection, add it to the output of the block\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            # Otherwise, add the input to the output of the block (identity mapping)\n",
    "            out += x\n",
    "        out = F.relu(out.clone())\n",
    "        return out\n",
    "\n",
    "# Define the ResNet architecture with multiple ResNet blocks\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define the initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        \n",
    "        # Define multiple ResNet blocks with different input and output channels\n",
    "        self.resblock1 = ResNet(32, 64,stride=1)\n",
    "        self.resblock2 = ResNet(64, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,128,stride=2)\n",
    "        self.resblock7=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,256,stride=2)\n",
    "        self.resblock12=ResNet(256,256,stride=1)\n",
    "        \n",
    "        # Define the final layers for classification\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "#initialize the model with kaiming_normal\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "#\n",
    "#training the model the model\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "#evaluating the model\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 2.121805954772188 train acc per epoch=>  0.3313419117647059\n",
      "val loss per epoch =>  2.061382332934609 val acc per epoch => 0.3942840189873418 \n",
      "\n",
      "train loss per epoch =>  1 2.0462236505030367 train acc per epoch=>  0.40917119566741805\n",
      "val loss per epoch =>  2.0248137742658203 val acc per epoch => 0.43037974683544306 \n",
      "\n",
      "train loss per epoch =>  2 2.0157427799976086 train acc per epoch=>  0.4408367966751918\n",
      "val loss per epoch =>  1.9786574976353706 val acc per epoch => 0.478935917721519 \n",
      "\n",
      "train loss per epoch =>  3 1.9940991108984594 train acc per epoch=>  0.46217631074168797\n",
      "val loss per epoch =>  1.992312131048758 val acc per epoch => 0.46489319620253167 \n",
      "\n",
      "train loss per epoch =>  4 1.9743512710342017 train acc per epoch=>  0.48233695652173914\n",
      "val loss per epoch =>  1.9444244949123528 val acc per epoch => 0.5117681962025317 \n",
      "\n",
      "train loss per epoch =>  5 1.9559028246213712 train acc per epoch=>  0.5011069373706417\n",
      "val loss per epoch =>  1.944606216647957 val acc per epoch => 0.5111748417721519 \n",
      "\n",
      "train loss per epoch =>  6 1.9393678809065953 train acc per epoch=>  0.5194013746803069\n",
      "val loss per epoch =>  1.958547596690021 val acc per epoch => 0.4978243670886076 \n",
      "\n",
      "train loss per epoch =>  7 1.9314871700218572 train acc per epoch=>  0.5260909527463986\n",
      "val loss per epoch =>  1.9711780382108084 val acc per epoch => 0.4861550632911392 \n",
      "\n",
      "train loss per epoch =>  8 1.916296695504347 train acc per epoch=>  0.5412284207466008\n",
      "val loss per epoch =>  1.9145862935464593 val acc per epoch => 0.5439082278481012 \n",
      "\n",
      "train loss per epoch =>  9 1.9012099318492137 train acc per epoch=>  0.5569733056570868\n",
      "val loss per epoch =>  1.888119845450679 val acc per epoch => 0.5689280063291139 \n",
      "\n",
      "train loss per epoch =>  10 1.8950423749206622 train acc per epoch=>  0.5627597506393862\n",
      "val loss per epoch =>  1.9003657102584839 val acc per epoch => 0.5557753164556962 \n",
      "\n",
      "train loss per epoch =>  11 1.8886812379598008 train acc per epoch=>  0.5700367647973473\n",
      "val loss per epoch =>  1.8407871994791152 val acc per epoch => 0.6168908227848101 \n",
      "\n",
      "train loss per epoch =>  12 1.8783070580733707 train acc per epoch=>  0.5805826406649617\n",
      "val loss per epoch =>  1.857307156430015 val acc per epoch => 0.6018591772151899 \n",
      "\n",
      "train loss per epoch =>  13 1.8732500173856534 train acc per epoch=>  0.5850663363476238\n",
      "val loss per epoch =>  1.8470299093029168 val acc per epoch => 0.6127373417721519 \n",
      "\n",
      "train loss per epoch =>  14 1.8621361712970392 train acc per epoch=>  0.5959478900255755\n",
      "val loss per epoch =>  1.8217440119272545 val acc per epoch => 0.6363726265822784 \n",
      "\n",
      "train loss per epoch =>  15 1.8554486082033124 train acc per epoch=>  0.6026054988126925\n",
      "val loss per epoch =>  1.8386458173582825 val acc per epoch => 0.6198575949367089 \n",
      "\n",
      "train loss per epoch =>  16 1.8484663850511127 train acc per epoch=>  0.6097546355498721\n",
      "val loss per epoch =>  1.8250498575500296 val acc per epoch => 0.6325158227848101 \n",
      "\n",
      "train loss per epoch =>  17 1.8412308229509826 train acc per epoch=>  0.6185581841127342\n",
      "val loss per epoch =>  1.8245360594761522 val acc per epoch => 0.6348892405063291 \n",
      "\n",
      "train loss per epoch =>  18 1.8381581251578563 train acc per epoch=>  0.6198929028437875\n",
      "val loss per epoch =>  1.8109547231770768 val acc per epoch => 0.6488330696202531 \n",
      "\n",
      "train loss per epoch =>  19 1.8295655116400755 train acc per epoch=>  0.6290601023932552\n",
      "val loss per epoch =>  1.799503193625921 val acc per epoch => 0.6590189873417721 \n",
      "\n",
      "train loss per epoch =>  20 1.8238712317498444 train acc per epoch=>  0.6344269501888539\n",
      "val loss per epoch =>  1.8039443055285682 val acc per epoch => 0.6532832278481012 \n",
      "\n",
      "train loss per epoch =>  21 1.822535831300194 train acc per epoch=>  0.6356657609305418\n",
      "val loss per epoch =>  1.8351111170611805 val acc per epoch => 0.6247033227848101 \n",
      "\n",
      "train loss per epoch =>  22 1.819030683668678 train acc per epoch=>  0.6397578324503301\n",
      "val loss per epoch =>  1.797062037866327 val acc per epoch => 0.6629746835443038 \n",
      "\n",
      "train loss per epoch =>  23 1.8144207522082512 train acc per epoch=>  0.6444013746803069\n",
      "val loss per epoch =>  1.7886237449283842 val acc per epoch => 0.6702927215189873 \n",
      "\n",
      "train loss per epoch =>  24 1.8087747706781567 train acc per epoch=>  0.6493206522653779\n",
      "val loss per epoch =>  1.7872886672804627 val acc per epoch => 0.6710838607594937 \n",
      "\n",
      "train loss per epoch =>  25 1.8045792540016077 train acc per epoch=>  0.6538682865059894\n",
      "val loss per epoch =>  1.8113774830781961 val acc per epoch => 0.6474485759493671 \n",
      "\n",
      "train loss per epoch =>  26 1.8026159715164654 train acc per epoch=>  0.6555946292474751\n",
      "val loss per epoch =>  1.7552633949472933 val acc per epoch => 0.7046083860759493 \n",
      "\n",
      "train loss per epoch =>  27 1.797919756616168 train acc per epoch=>  0.6607536765010765\n",
      "val loss per epoch =>  1.772605923157704 val acc per epoch => 0.6873022151898734 \n",
      "\n",
      "train loss per epoch =>  28 1.7939862119572243 train acc per epoch=>  0.664681905370844\n",
      "val loss per epoch =>  1.7772800862034666 val acc per epoch => 0.6816653481012658 \n",
      "\n",
      "train loss per epoch =>  29 1.7902196484148656 train acc per epoch=>  0.6693094628851127\n",
      "val loss per epoch =>  1.7593449972852875 val acc per epoch => 0.7005537974683544 \n",
      "\n",
      "train loss per epoch =>  30 1.7854510253042821 train acc per epoch=>  0.673741208470386\n",
      "val loss per epoch =>  1.7581635155255282 val acc per epoch => 0.7013449367088608 \n",
      "\n",
      "train loss per epoch =>  31 1.7837880834594102 train acc per epoch=>  0.6750879156619997\n",
      "val loss per epoch =>  1.768726590313489 val acc per epoch => 0.6903678797468354 \n",
      "\n",
      "train loss per epoch =>  32 1.7793346259295177 train acc per epoch=>  0.6800671355498721\n",
      "val loss per epoch =>  1.758024149303195 val acc per epoch => 0.7017405063291139 \n",
      "\n",
      "train loss per epoch =>  33 1.7770352400172398 train acc per epoch=>  0.6820452365728901\n",
      "val loss per epoch =>  1.7583743605432631 val acc per epoch => 0.7005537974683544 \n",
      "\n",
      "train loss per epoch =>  34 1.7756397766835244 train acc per epoch=>  0.6832400895750431\n",
      "val loss per epoch =>  1.764272166203849 val acc per epoch => 0.6965981012658228 \n",
      "\n",
      "train loss per epoch =>  35 1.7735671253155565 train acc per epoch=>  0.6858695652478796\n",
      "val loss per epoch =>  1.7507076867019074 val acc per epoch => 0.7077729430379747 \n",
      "\n",
      "train loss per epoch =>  36 1.7697083648208463 train acc per epoch=>  0.6895140665571403\n",
      "val loss per epoch =>  1.7492892923234384 val acc per epoch => 0.710245253164557 \n",
      "\n",
      "train loss per epoch =>  37 1.7646489256178326 train acc per epoch=>  0.69495684152369\n",
      "val loss per epoch =>  1.7322447571573378 val acc per epoch => 0.727254746835443 \n",
      "\n",
      "train loss per epoch =>  38 1.7657445153921767 train acc per epoch=>  0.6930107097796467\n",
      "val loss per epoch =>  1.7526846611047093 val acc per epoch => 0.7058939873417721 \n",
      "\n",
      "train loss per epoch =>  39 1.7630243755667412 train acc per epoch=>  0.6964593990074704\n",
      "val loss per epoch =>  1.735214598571198 val acc per epoch => 0.7246835443037974 \n",
      "\n",
      "train loss per epoch =>  40 1.7612947149349905 train acc per epoch=>  0.6980578644806162\n",
      "val loss per epoch =>  1.7400642029846771 val acc per epoch => 0.7200356012658228 \n",
      "\n",
      "train loss per epoch =>  41 1.7553012782655408 train acc per epoch=>  0.7044876919073217\n",
      "val loss per epoch =>  1.7283261637144451 val acc per epoch => 0.7316060126582279 \n",
      "\n",
      "train loss per epoch =>  42 1.7560786722261277 train acc per epoch=>  0.703065057544757\n",
      "val loss per epoch =>  1.742437620706196 val acc per epoch => 0.715684335443038 \n",
      "\n",
      "train loss per epoch =>  43 1.755117837120505 train acc per epoch=>  0.7045875959993934\n",
      "val loss per epoch =>  1.740808532207827 val acc per epoch => 0.7182555379746836 \n",
      "\n",
      "train loss per epoch =>  44 1.750928755306527 train acc per epoch=>  0.7092111572585142\n",
      "val loss per epoch =>  1.732307485387295 val acc per epoch => 0.7284414556962026 \n",
      "\n",
      "train loss per epoch =>  45 1.7469240823365233 train acc per epoch=>  0.7123281649311485\n",
      "val loss per epoch =>  1.7360356804690784 val acc per epoch => 0.7228045886075949 \n",
      "\n",
      "train loss per epoch =>  46 1.7479033942722604 train acc per epoch=>  0.711504955273455\n",
      "val loss per epoch =>  1.732027743436113 val acc per epoch => 0.7268591772151899 \n",
      "\n",
      "train loss per epoch =>  47 1.7432927963373912 train acc per epoch=>  0.7164801790586213\n",
      "val loss per epoch =>  1.7174525125117241 val acc per epoch => 0.742879746835443 \n",
      "\n",
      "train loss per epoch =>  48 1.7428548183587507 train acc per epoch=>  0.7164162405006721\n",
      "val loss per epoch =>  1.713184940664074 val acc per epoch => 0.7460443037974683 \n",
      "\n",
      "train loss per epoch =>  49 1.7419949151061076 train acc per epoch=>  0.7174992007977518\n",
      "val loss per epoch =>  1.7144907531859 val acc per epoch => 0.7455498417721519 \n",
      "\n",
      "train loss per epoch =>  50 1.7421213830523479 train acc per epoch=>  0.7177909207161125\n",
      "val loss per epoch =>  1.7400815849062763 val acc per epoch => 0.7193433544303798 \n",
      "\n",
      "train loss per epoch =>  51 1.7401977295765791 train acc per epoch=>  0.7195212595603045\n",
      "val loss per epoch =>  1.7236546579795549 val acc per epoch => 0.7362539556962026 \n",
      "\n",
      "train loss per epoch =>  52 1.7347322531673304 train acc per epoch=>  0.7249160805321715\n",
      "val loss per epoch =>  1.7252624985537952 val acc per epoch => 0.7348694620253164 \n",
      "\n",
      "train loss per epoch =>  53 1.7366492973873988 train acc per epoch=>  0.7231457800511509\n",
      "val loss per epoch =>  1.7144781124742725 val acc per epoch => 0.7451542721518988 \n",
      "\n",
      "train loss per epoch =>  54 1.7344026041152838 train acc per epoch=>  0.7248601343320764\n",
      "val loss per epoch =>  1.707329796839364 val acc per epoch => 0.7534612341772152 \n",
      "\n",
      "train loss per epoch =>  55 1.7318313289481355 train acc per epoch=>  0.7274016943734015\n",
      "val loss per epoch =>  1.7105272027510632 val acc per epoch => 0.748318829113924 \n",
      "\n",
      "train loss per epoch =>  56 1.72983294618709 train acc per epoch=>  0.7299272698514602\n",
      "val loss per epoch =>  1.7084299069416673 val acc per epoch => 0.7514833860759493 \n",
      "\n",
      "train loss per epoch =>  57 1.727589957854327 train acc per epoch=>  0.7324288682559567\n",
      "val loss per epoch =>  1.7099174532709243 val acc per epoch => 0.7497033227848101 \n",
      "\n",
      "train loss per epoch =>  58 1.7238964415572184 train acc per epoch=>  0.7359095269151966\n",
      "val loss per epoch =>  1.7093048820012733 val acc per epoch => 0.7511867088607594 \n",
      "\n",
      "train loss per epoch =>  59 1.7204284350890333 train acc per epoch=>  0.7391743926745852\n",
      "val loss per epoch =>  1.6981626960295666 val acc per epoch => 0.7607792721518988 \n",
      "\n",
      "train loss per epoch =>  60 1.720241167051408 train acc per epoch=>  0.7400095907928389\n",
      "val loss per epoch =>  1.7071916030932077 val acc per epoch => 0.7512856012658228 \n",
      "\n",
      "train loss per epoch =>  61 1.718365622908258 train acc per epoch=>  0.7417199488186166\n",
      "val loss per epoch =>  1.7132123769084109 val acc per epoch => 0.7457476265822784 \n",
      "\n",
      "train loss per epoch =>  62 1.718810676918615 train acc per epoch=>  0.741076566526652\n",
      "val loss per epoch =>  1.6914735489253756 val acc per epoch => 0.7689873417721519 \n",
      "\n",
      "train loss per epoch =>  63 1.7179417518703528 train acc per epoch=>  0.741951726434176\n",
      "val loss per epoch =>  1.7064134637011756 val acc per epoch => 0.7535601265822784 \n",
      "\n",
      "train loss per epoch =>  64 1.7140504316905576 train acc per epoch=>  0.7454683504750966\n",
      "val loss per epoch =>  1.732429092443442 val acc per epoch => 0.7278481012658228 \n",
      "\n",
      "train loss per epoch =>  65 1.711835513334445 train acc per epoch=>  0.7485533887163147\n",
      "val loss per epoch =>  1.6916444331784792 val acc per epoch => 0.7676028481012658 \n",
      "\n",
      "train loss per epoch =>  66 1.7135347594385562 train acc per epoch=>  0.7465912724090049\n",
      "val loss per epoch =>  1.688497756100908 val acc per epoch => 0.7708662974683544 \n",
      "\n",
      "train loss per epoch =>  67 1.7077353034178009 train acc per epoch=>  0.752469629125522\n",
      "val loss per epoch =>  1.7018851751013646 val acc per epoch => 0.758504746835443 \n",
      "\n",
      "train loss per epoch =>  68 1.710818237965674 train acc per epoch=>  0.7488451087871171\n",
      "val loss per epoch =>  1.6983276967760883 val acc per epoch => 0.7604825949367089 \n",
      "\n",
      "train loss per epoch =>  69 1.708525218622154 train acc per epoch=>  0.7513866688284423\n",
      "val loss per epoch =>  1.6952132892005052 val acc per epoch => 0.7651305379746836 \n",
      "\n",
      "train loss per epoch =>  70 1.7063946040999858 train acc per epoch=>  0.753300831293511\n",
      "val loss per epoch =>  1.6961868895760066 val acc per epoch => 0.7643393987341772 \n",
      "\n",
      "train loss per epoch =>  71 1.7074223262879549 train acc per epoch=>  0.7519541241018973\n",
      "val loss per epoch =>  1.6910419750817214 val acc per epoch => 0.7684928797468354 \n",
      "\n",
      "train loss per epoch =>  72 1.7010837400051029 train acc per epoch=>  0.7583879476313091\n",
      "val loss per epoch =>  1.6849942388413828 val acc per epoch => 0.7757120253164557 \n",
      "\n",
      "train loss per epoch =>  73 1.7022293461558153 train acc per epoch=>  0.7577485614420508\n",
      "val loss per epoch =>  1.685480387904976 val acc per epoch => 0.7744264240506329 \n",
      "\n",
      "train loss per epoch =>  74 1.702385132270091 train acc per epoch=>  0.7578244884605603\n",
      "val loss per epoch =>  1.6854280308832097 val acc per epoch => 0.7740308544303798 \n",
      "\n",
      "train loss per epoch =>  75 1.6986133762637672 train acc per epoch=>  0.7617287404092071\n",
      "val loss per epoch =>  1.6812002009983305 val acc per epoch => 0.7798655063291139 \n",
      "\n",
      "train loss per epoch =>  76 1.6975632476074922 train acc per epoch=>  0.762707800511509\n",
      "val loss per epoch =>  1.682879721062093 val acc per epoch => 0.7770965189873418 \n",
      "\n",
      "train loss per epoch =>  77 1.696034501580631 train acc per epoch=>  0.7641903773293166\n",
      "val loss per epoch =>  1.6838570606859424 val acc per epoch => 0.7763053797468354 \n",
      "\n",
      "train loss per epoch =>  78 1.6961824412236128 train acc per epoch=>  0.7635310103216439\n",
      "val loss per epoch =>  1.6826179148275642 val acc per epoch => 0.7777887658227848 \n",
      "\n",
      "train loss per epoch =>  79 1.6963371010997412 train acc per epoch=>  0.7634191176165706\n",
      "val loss per epoch =>  1.6789130742036844 val acc per epoch => 0.7808544303797469 \n",
      "\n",
      "train loss per epoch =>  80 1.6942475768916136 train acc per epoch=>  0.7654891305262476\n",
      "val loss per epoch =>  1.6764667713189427 val acc per epoch => 0.7830300632911392 \n",
      "\n",
      "train loss per epoch =>  81 1.6930391809824483 train acc per epoch=>  0.766923753196931\n",
      "val loss per epoch =>  1.6791746058041537 val acc per epoch => 0.7796677215189873 \n",
      "\n",
      "train loss per epoch =>  82 1.6863911179325464 train acc per epoch=>  0.7734654731762683\n",
      "val loss per epoch =>  1.684598895567882 val acc per epoch => 0.7743275316455697 \n",
      "\n",
      "train loss per epoch =>  83 1.6856316062800414 train acc per epoch=>  0.7744445332785701\n",
      "val loss per epoch =>  1.6794208952143221 val acc per epoch => 0.7799643987341772 \n",
      "\n",
      "train loss per epoch =>  84 1.6843496628124695 train acc per epoch=>  0.7760789642858383\n",
      "val loss per epoch =>  1.685938640485836 val acc per epoch => 0.774129746835443 \n",
      "\n",
      "train loss per epoch =>  85 1.6862264356344863 train acc per epoch=>  0.7736133312630227\n",
      "val loss per epoch =>  1.6771595281890677 val acc per epoch => 0.7830300632911392 \n",
      "\n",
      "train loss per epoch =>  86 1.6849432921470584 train acc per epoch=>  0.7759271099439362\n",
      "val loss per epoch =>  1.6730725689779353 val acc per epoch => 0.7871835443037974 \n",
      "\n",
      "train loss per epoch =>  87 1.684775504919574 train acc per epoch=>  0.7748601343625646\n",
      "val loss per epoch =>  1.6777152049390576 val acc per epoch => 0.7810522151898734 \n",
      "\n",
      "train loss per epoch =>  88 1.6832740751983564 train acc per epoch=>  0.7768302430277285\n",
      "val loss per epoch =>  1.6685608474514153 val acc per epoch => 0.7904469936708861 \n",
      "\n",
      "train loss per epoch =>  89 1.6812399686754818 train acc per epoch=>  0.7786005435087492\n",
      "val loss per epoch =>  1.6719885808003099 val acc per epoch => 0.7863924050632911 \n",
      "\n",
      "train loss per epoch =>  90 1.6784076550427605 train acc per epoch=>  0.7821491368286445\n",
      "val loss per epoch =>  1.6740984192377404 val acc per epoch => 0.7861946202531646 \n",
      "\n",
      "train loss per epoch =>  91 1.6801414709261921 train acc per epoch=>  0.7802189898003092\n",
      "val loss per epoch =>  1.6724001697347135 val acc per epoch => 0.786689082278481 \n",
      "\n",
      "train loss per epoch =>  92 1.678783439614279 train acc per epoch=>  0.7814657928998513\n",
      "val loss per epoch =>  1.6668054106869274 val acc per epoch => 0.7929193037974683 \n",
      "\n",
      "train loss per epoch =>  93 1.676970640106884 train acc per epoch=>  0.7831481777493606\n",
      "val loss per epoch =>  1.6698699767076517 val acc per epoch => 0.7896558544303798 \n",
      "\n",
      "train loss per epoch =>  94 1.67512646416569 train acc per epoch=>  0.7846787084094093\n",
      "val loss per epoch =>  1.6702612955358964 val acc per epoch => 0.7903481012658228 \n",
      "\n",
      "train loss per epoch =>  95 1.6731869707936826 train acc per epoch=>  0.787468030782002\n",
      "val loss per epoch =>  1.6714809287952472 val acc per epoch => 0.7867879746835443 \n",
      "\n",
      "train loss per epoch =>  96 1.673870149170956 train acc per epoch=>  0.7866568094324273\n",
      "val loss per epoch =>  1.6645976727521872 val acc per epoch => 0.7966772151898734 \n",
      "\n",
      "train loss per epoch =>  97 1.6704821754294588 train acc per epoch=>  0.7899016943734015\n",
      "val loss per epoch =>  1.665269611757013 val acc per epoch => 0.7956882911392406 \n",
      "\n",
      "train loss per epoch =>  98 1.6727964765275531 train acc per epoch=>  0.7879555626293583\n",
      "val loss per epoch =>  1.6639329813703705 val acc per epoch => 0.7965783227848101 \n",
      "\n",
      "train loss per epoch =>  99 1.6700268316146967 train acc per epoch=>  0.7902453645415928\n",
      "val loss per epoch =>  1.6658473120460027 val acc per epoch => 0.7946004746835443 \n",
      "\n",
      "train loss per epoch =>  100 1.667544640848399 train acc per epoch=>  0.7923673273962172\n",
      "val loss per epoch =>  1.6655129058451592 val acc per epoch => 0.793809335443038 \n",
      "\n",
      "train loss per epoch =>  101 1.6690406524921622 train acc per epoch=>  0.7910605818414322\n",
      "val loss per epoch =>  1.6671526748922807 val acc per epoch => 0.7926226265822784 \n",
      "\n",
      "train loss per epoch =>  102 1.667400435718429 train acc per epoch=>  0.7925231778408255\n",
      "val loss per epoch =>  1.6665388496616218 val acc per epoch => 0.7936115506329114 \n",
      "\n",
      "train loss per epoch =>  103 1.6656723821254642 train acc per epoch=>  0.7938459079588771\n",
      "val loss per epoch =>  1.666243215150471 val acc per epoch => 0.7942049050632911 \n",
      "\n",
      "train loss per epoch =>  104 1.6635199687669955 train acc per epoch=>  0.7970468350078749\n",
      "val loss per epoch =>  1.6651019205020954 val acc per epoch => 0.7933148734177216 \n",
      "\n",
      "train loss per epoch =>  105 1.667057256869343 train acc per epoch=>  0.7929667519486469\n",
      "val loss per epoch =>  1.6647709064845797 val acc per epoch => 0.7944026898734177 \n",
      "\n",
      "train loss per epoch =>  106 1.6620402394048392 train acc per epoch=>  0.7984335038363172\n",
      "val loss per epoch =>  1.6658305005182195 val acc per epoch => 0.7943037974683544 \n",
      "\n",
      "train loss per epoch =>  107 1.6617302507390757 train acc per epoch=>  0.7985214194678285\n",
      "val loss per epoch =>  1.6605635851244382 val acc per epoch => 0.7989517405063291 \n",
      "\n",
      "train loss per epoch =>  108 1.6626546718275454 train acc per epoch=>  0.7976262787418902\n",
      "val loss per epoch =>  1.6587575523159173 val acc per epoch => 0.8022151898734177 \n",
      "\n",
      "train loss per epoch =>  109 1.658303982766388 train acc per epoch=>  0.8015105498721228\n",
      "val loss per epoch =>  1.6632512521140184 val acc per epoch => 0.7969738924050633 \n",
      "\n",
      "train loss per epoch =>  110 1.6579841421083417 train acc per epoch=>  0.8026934144137156\n",
      "val loss per epoch =>  1.6560812086998662 val acc per epoch => 0.8036985759493671 \n",
      "\n",
      "train loss per epoch =>  111 1.6590278020600224 train acc per epoch=>  0.8009111253196931\n",
      "val loss per epoch =>  1.6575592816630496 val acc per epoch => 0.8014240506329114 \n",
      "\n",
      "train loss per epoch =>  112 1.6575536450461659 train acc per epoch=>  0.8021898976982097\n",
      "val loss per epoch =>  1.6564350671406034 val acc per epoch => 0.8021162974683544 \n",
      "\n",
      "train loss per epoch =>  113 1.6547752432810985 train acc per epoch=>  0.8053268861892583\n",
      "val loss per epoch =>  1.6592912673950195 val acc per epoch => 0.8012262658227848 \n",
      "\n",
      "train loss per epoch =>  114 1.6540629467390993 train acc per epoch=>  0.8057744565522275\n",
      "val loss per epoch =>  1.6533845074569122 val acc per epoch => 0.8074564873417721 \n",
      "\n",
      "train loss per epoch =>  115 1.6537790816763174 train acc per epoch=>  0.8064897699124368\n",
      "val loss per epoch =>  1.6616083323201047 val acc per epoch => 0.798556170886076 \n",
      "\n",
      "train loss per epoch =>  116 1.6514974911804394 train acc per epoch=>  0.808903452685422\n",
      "val loss per epoch =>  1.6551245375524593 val acc per epoch => 0.805379746835443 \n",
      "\n",
      "train loss per epoch =>  117 1.6505055802557476 train acc per epoch=>  0.8096387468640457\n",
      "val loss per epoch =>  1.6565546385849579 val acc per epoch => 0.8034018987341772 \n",
      "\n",
      "train loss per epoch =>  118 1.6503209518959454 train acc per epoch=>  0.8097706202350919\n",
      "val loss per epoch =>  1.6505181985565378 val acc per epoch => 0.8090387658227848 \n",
      "\n",
      "train loss per epoch =>  119 1.648411453837324 train acc per epoch=>  0.8121723145475168\n",
      "val loss per epoch =>  1.657788318923757 val acc per epoch => 0.801621835443038 \n",
      "\n",
      "train loss per epoch =>  120 1.6511629661330787 train acc per epoch=>  0.8092351342406114\n",
      "val loss per epoch =>  1.652441810957993 val acc per epoch => 0.8079509493670886 \n",
      "\n",
      "train loss per epoch =>  121 1.6467217234394433 train acc per epoch=>  0.8141823850019508\n",
      "val loss per epoch =>  1.6502699882169314 val acc per epoch => 0.8090387658227848 \n",
      "\n",
      "train loss per epoch =>  122 1.6437066171480261 train acc per epoch=>  0.8169037724090049\n",
      "val loss per epoch =>  1.6583231566827508 val acc per epoch => 0.8015229430379747 \n",
      "\n",
      "train loss per epoch =>  123 1.6443054526663192 train acc per epoch=>  0.8159646739435318\n",
      "val loss per epoch =>  1.6498532265047483 val acc per epoch => 0.8109177215189873 \n",
      "\n",
      "train loss per epoch =>  124 1.6447036583405321 train acc per epoch=>  0.8157888426805091\n",
      "val loss per epoch =>  1.6492685411549821 val acc per epoch => 0.8109177215189873 \n",
      "\n",
      "train loss per epoch =>  125 1.6447766013157643 train acc per epoch=>  0.8157728581172426\n",
      "val loss per epoch =>  1.6486205134210707 val acc per epoch => 0.8116099683544303 \n",
      "\n",
      "train loss per epoch =>  126 1.6429241849943195 train acc per epoch=>  0.8173913044088027\n",
      "val loss per epoch =>  1.6514872116378592 val acc per epoch => 0.8085443037974683 \n",
      "\n",
      "train loss per epoch =>  127 1.6395738948031764 train acc per epoch=>  0.8209359016259918\n",
      "val loss per epoch =>  1.649419734749613 val acc per epoch => 0.8095332278481012 \n",
      "\n",
      "train loss per epoch =>  128 1.6420849217173388 train acc per epoch=>  0.8183104220558616\n",
      "val loss per epoch =>  1.6430299810216398 val acc per epoch => 0.8174446202531646 \n",
      "\n",
      "train loss per epoch =>  129 1.6385854696068922 train acc per epoch=>  0.8217511189258312\n",
      "val loss per epoch =>  1.6531105041503906 val acc per epoch => 0.8066653481012658 \n",
      "\n",
      "train loss per epoch =>  130 1.6394454137138699 train acc per epoch=>  0.8210038362866472\n",
      "val loss per epoch =>  1.6469679799260972 val acc per epoch => 0.8125988924050633 \n",
      "\n",
      "train loss per epoch =>  131 1.6400303685146829 train acc per epoch=>  0.8199288683474216\n",
      "val loss per epoch =>  1.6462898782536954 val acc per epoch => 0.8134889240506329 \n",
      "\n",
      "train loss per epoch =>  132 1.6379217455149306 train acc per epoch=>  0.822993925922667\n",
      "val loss per epoch =>  1.6432078850420215 val acc per epoch => 0.8175435126582279 \n",
      "\n",
      "train loss per epoch =>  133 1.6372034882035706 train acc per epoch=>  0.8229779412069589\n",
      "val loss per epoch =>  1.6411282231536093 val acc per epoch => 0.819620253164557 \n",
      "\n",
      "train loss per epoch =>  134 1.6368624417068403 train acc per epoch=>  0.823433503927782\n",
      "val loss per epoch =>  1.6434092506577698 val acc per epoch => 0.8162579113924051 \n",
      "\n",
      "train loss per epoch =>  135 1.6361789810078224 train acc per epoch=>  0.8246403452380539\n",
      "val loss per epoch =>  1.6409359762940225 val acc per epoch => 0.8188291139240507 \n",
      "\n",
      "train loss per epoch =>  136 1.634437282981775 train acc per epoch=>  0.8263546995189793\n",
      "val loss per epoch =>  1.6462472299986248 val acc per epoch => 0.8137856012658228 \n",
      "\n",
      "train loss per epoch =>  137 1.633168665649336 train acc per epoch=>  0.8270859974424553\n",
      "val loss per epoch =>  1.6415219835088224 val acc per epoch => 0.818631329113924 \n",
      "\n",
      "train loss per epoch =>  138 1.6350880254564992 train acc per epoch=>  0.825019980787926\n",
      "val loss per epoch =>  1.64252671259868 val acc per epoch => 0.8177412974683544 \n",
      "\n",
      "train loss per epoch =>  139 1.6319597587561059 train acc per epoch=>  0.8289322250944269\n",
      "val loss per epoch =>  1.6452603023263472 val acc per epoch => 0.8144778481012658 \n",
      "\n",
      "train loss per epoch =>  140 1.6311473221425206 train acc per epoch=>  0.829379795457396\n",
      "val loss per epoch =>  1.6431514175632331 val acc per epoch => 0.8165545886075949 \n",
      "\n",
      "train loss per epoch =>  141 1.6293195086671872 train acc per epoch=>  0.8312659847461964\n",
      "val loss per epoch =>  1.6400474732435202 val acc per epoch => 0.8202136075949367 \n",
      "\n",
      "train loss per epoch =>  142 1.6276740512579604 train acc per epoch=>  0.8324608375959079\n",
      "val loss per epoch =>  1.6432642076588884 val acc per epoch => 0.8167523734177216 \n",
      "\n",
      "train loss per epoch =>  143 1.6298816237608185 train acc per epoch=>  0.8307185102911556\n",
      "val loss per epoch =>  1.6411894424052178 val acc per epoch => 0.8189280063291139 \n",
      "\n",
      "train loss per epoch =>  144 1.628503579312883 train acc per epoch=>  0.8321771100354012\n",
      "val loss per epoch =>  1.6411635981330388 val acc per epoch => 0.818631329113924 \n",
      "\n",
      "train loss per epoch =>  145 1.6268841470293987 train acc per epoch=>  0.8335437980454291\n",
      "val loss per epoch =>  1.6406057077118112 val acc per epoch => 0.8194224683544303 \n",
      "\n",
      "train loss per epoch =>  146 1.624682084678689 train acc per epoch=>  0.836261189349777\n",
      "val loss per epoch =>  1.6390375864656666 val acc per epoch => 0.8218947784810127 \n",
      "\n",
      "train loss per epoch =>  147 1.6237645301672503 train acc per epoch=>  0.8368566176470589\n",
      "val loss per epoch =>  1.6417611520501632 val acc per epoch => 0.8193235759493671 \n",
      "\n",
      "train loss per epoch =>  148 1.6257389789956915 train acc per epoch=>  0.8345987851662404\n",
      "val loss per epoch =>  1.6421130774896355 val acc per epoch => 0.8180379746835443 \n",
      "\n",
      "train loss per epoch =>  149 1.6237581940868018 train acc per epoch=>  0.8366488171050616\n",
      "val loss per epoch =>  1.6392165241362173 val acc per epoch => 0.8208069620253164 \n",
      "\n",
      "train loss per epoch =>  150 1.6211850569986017 train acc per epoch=>  0.8396819054013323\n",
      "val loss per epoch =>  1.6395157605786868 val acc per epoch => 0.8208069620253164 \n",
      "\n",
      "train loss per epoch =>  151 1.6221746166648767 train acc per epoch=>  0.838303228930744\n",
      "val loss per epoch =>  1.6364031381244901 val acc per epoch => 0.8238726265822784 \n",
      "\n",
      "train loss per epoch =>  152 1.6206953162732332 train acc per epoch=>  0.8399096867617439\n",
      "val loss per epoch =>  1.63932465601571 val acc per epoch => 0.8208069620253164 \n",
      "\n",
      "train loss per epoch =>  153 1.6239049815765731 train acc per epoch=>  0.8367167519181585\n",
      "val loss per epoch =>  1.6381228075751775 val acc per epoch => 0.8224881329113924 \n",
      "\n",
      "train loss per epoch =>  154 1.6222949640830155 train acc per epoch=>  0.8381034207466008\n",
      "val loss per epoch =>  1.6369854181627683 val acc per epoch => 0.8225870253164557 \n",
      "\n",
      "train loss per epoch =>  155 1.6214198044803747 train acc per epoch=>  0.8390305307515137\n",
      "val loss per epoch =>  1.6368985462792311 val acc per epoch => 0.823378164556962 \n",
      "\n",
      "train loss per epoch =>  156 1.6203114038233257 train acc per epoch=>  0.8405490729510022\n",
      "val loss per epoch =>  1.6382736390150046 val acc per epoch => 0.8221914556962026 \n",
      "\n",
      "train loss per epoch =>  157 1.618183912218684 train acc per epoch=>  0.8420796036110509\n",
      "val loss per epoch =>  1.6395279093633723 val acc per epoch => 0.8212025316455697 \n",
      "\n",
      "train loss per epoch =>  158 1.619332138229819 train acc per epoch=>  0.8410885550481889\n",
      "val loss per epoch =>  1.6345314783386038 val acc per epoch => 0.826443829113924 \n",
      "\n",
      "train loss per epoch =>  159 1.6184554929318635 train acc per epoch=>  0.841967711058419\n",
      "val loss per epoch =>  1.6356677843045584 val acc per epoch => 0.8245648734177216 \n",
      "\n",
      "train loss per epoch =>  160 1.6184410865959304 train acc per epoch=>  0.8422634270794861\n",
      "val loss per epoch =>  1.636195101315462 val acc per epoch => 0.8232792721518988 \n",
      "\n",
      "train loss per epoch =>  161 1.6163937100364119 train acc per epoch=>  0.8442814897393327\n",
      "val loss per epoch =>  1.6341574101508418 val acc per epoch => 0.825751582278481 \n",
      "\n",
      "train loss per epoch =>  162 1.6163292696408909 train acc per epoch=>  0.8446611252892048\n",
      "val loss per epoch =>  1.6364217453365084 val acc per epoch => 0.8235759493670886 \n",
      "\n",
      "train loss per epoch =>  163 1.614765542547416 train acc per epoch=>  0.8459119246438946\n",
      "val loss per epoch =>  1.6316119704065444 val acc per epoch => 0.8294106012658228 \n",
      "\n",
      "train loss per epoch =>  164 1.6168389701477401 train acc per epoch=>  0.8434862532579076\n",
      "val loss per epoch =>  1.633499264717102 val acc per epoch => 0.8261471518987342 \n",
      "\n",
      "train loss per epoch =>  165 1.6154795144220142 train acc per epoch=>  0.8453804347521204\n",
      "val loss per epoch =>  1.6321320926086813 val acc per epoch => 0.8282238924050633 \n",
      "\n",
      "train loss per epoch =>  166 1.6167268704270463 train acc per epoch=>  0.8440257353550943\n",
      "val loss per epoch =>  1.6338694759562045 val acc per epoch => 0.8272349683544303 \n",
      "\n",
      "train loss per epoch =>  167 1.6140874846816977 train acc per epoch=>  0.8465513108331529\n",
      "val loss per epoch =>  1.6318931504140926 val acc per epoch => 0.828125 \n",
      "\n",
      "train loss per epoch =>  168 1.6148718318061146 train acc per epoch=>  0.8459119246438946\n",
      "val loss per epoch =>  1.6329806273496603 val acc per epoch => 0.8271360759493671 \n",
      "\n",
      "train loss per epoch =>  169 1.6139664092027317 train acc per epoch=>  0.8468150575752453\n",
      "val loss per epoch =>  1.6334884664680385 val acc per epoch => 0.8269382911392406 \n",
      "\n",
      "train loss per epoch =>  170 1.6137349328116688 train acc per epoch=>  0.8477181905065961\n",
      "val loss per epoch =>  1.631816249859484 val acc per epoch => 0.8283227848101266 \n",
      "\n",
      "train loss per epoch =>  171 1.6124158631200376 train acc per epoch=>  0.8486772697904835\n",
      "val loss per epoch =>  1.6312432289123535 val acc per epoch => 0.8289161392405063 \n",
      "\n",
      "train loss per epoch =>  172 1.6131172265543048 train acc per epoch=>  0.8477141944038898\n",
      "val loss per epoch =>  1.6311911555785168 val acc per epoch => 0.8299050632911392 \n",
      "\n",
      "train loss per epoch =>  173 1.6124123313542826 train acc per epoch=>  0.8481257992327366\n",
      "val loss per epoch =>  1.6309445296661764 val acc per epoch => 0.8295094936708861 \n",
      "\n",
      "train loss per epoch =>  174 1.6111166510740509 train acc per epoch=>  0.8499320652478796\n",
      "val loss per epoch =>  1.6331001505067078 val acc per epoch => 0.8271360759493671 \n",
      "\n",
      "train loss per epoch =>  175 1.611110102185203 train acc per epoch=>  0.8498161764400999\n",
      "val loss per epoch =>  1.632280063025559 val acc per epoch => 0.8283227848101266 \n",
      "\n",
      "train loss per epoch =>  176 1.6123300339559765 train acc per epoch=>  0.8480898338510557\n",
      "val loss per epoch =>  1.6319713471811028 val acc per epoch => 0.8284216772151899 \n",
      "\n",
      "train loss per epoch =>  177 1.6117424730144803 train acc per epoch=>  0.8486492967666568\n",
      "val loss per epoch =>  1.6317972472951383 val acc per epoch => 0.8289161392405063 \n",
      "\n",
      "train loss per epoch =>  178 1.6098852395401586 train acc per epoch=>  0.8508991368896212\n",
      "val loss per epoch =>  1.632374025598357 val acc per epoch => 0.8270371835443038 \n",
      "\n",
      "train loss per epoch =>  179 1.609825275743099 train acc per epoch=>  0.8508551789976447\n",
      "val loss per epoch =>  1.631296464159519 val acc per epoch => 0.8291139240506329 \n",
      "\n",
      "train loss per epoch =>  180 1.608307775329141 train acc per epoch=>  0.8528652494520788\n",
      "val loss per epoch =>  1.6324202984194212 val acc per epoch => 0.8275316455696202 \n",
      "\n",
      "train loss per epoch =>  181 1.6080647673448334 train acc per epoch=>  0.8526094949153988\n",
      "val loss per epoch =>  1.6314436981949625 val acc per epoch => 0.8294106012658228 \n",
      "\n",
      "train loss per epoch =>  182 1.6086649245313367 train acc per epoch=>  0.8521459399891631\n",
      "val loss per epoch =>  1.631488578229011 val acc per epoch => 0.8291139240506329 \n",
      "\n",
      "train loss per epoch =>  183 1.6085222444266005 train acc per epoch=>  0.8523976982706953\n",
      "val loss per epoch =>  1.6314569228812108 val acc per epoch => 0.828817246835443 \n",
      "\n",
      "train loss per epoch =>  184 1.6077014171253994 train acc per epoch=>  0.8535445972781657\n",
      "val loss per epoch =>  1.6310988845704477 val acc per epoch => 0.8291139240506329 \n",
      "\n",
      "train loss per epoch =>  185 1.6089546149953857 train acc per epoch=>  0.851706361984048\n",
      "val loss per epoch =>  1.6316669802122479 val acc per epoch => 0.8285205696202531 \n",
      "\n",
      "train loss per epoch =>  186 1.6068996550786832 train acc per epoch=>  0.8539002557544757\n",
      "val loss per epoch =>  1.6309872729868828 val acc per epoch => 0.829806170886076 \n",
      "\n",
      "train loss per epoch =>  187 1.60810240089436 train acc per epoch=>  0.8528892263731993\n",
      "val loss per epoch =>  1.6313425211966792 val acc per epoch => 0.8300039556962026 \n",
      "\n",
      "train loss per epoch =>  188 1.6078901970782853 train acc per epoch=>  0.8529491688284423\n",
      "val loss per epoch =>  1.6310947107363352 val acc per epoch => 0.8299050632911392 \n",
      "\n",
      "train loss per epoch =>  189 1.6068754263241272 train acc per epoch=>  0.8542599104859335\n",
      "val loss per epoch =>  1.6315687185601344 val acc per epoch => 0.8296083860759493 \n",
      "\n",
      "train loss per epoch =>  190 1.6088417997140714 train acc per epoch=>  0.8521779092681377\n",
      "val loss per epoch =>  1.6305840513374232 val acc per epoch => 0.8308939873417721 \n",
      "\n",
      "train loss per epoch =>  191 1.6081979039989773 train acc per epoch=>  0.8524296675496699\n",
      "val loss per epoch =>  1.6306250397163102 val acc per epoch => 0.8303006329113924 \n",
      "\n",
      "train loss per epoch =>  192 1.6083621176917229 train acc per epoch=>  0.8527213874680307\n",
      "val loss per epoch =>  1.629390131069135 val acc per epoch => 0.8320806962025317 \n",
      "\n",
      "train loss per epoch =>  193 1.6091223787468718 train acc per epoch=>  0.8517423273657289\n",
      "val loss per epoch =>  1.6298614740371704 val acc per epoch => 0.8309928797468354 \n",
      "\n",
      "train loss per epoch =>  194 1.6083954941586156 train acc per epoch=>  0.851698369626194\n",
      "val loss per epoch =>  1.6300366498246979 val acc per epoch => 0.8310917721518988 \n",
      "\n",
      "train loss per epoch =>  195 1.6088616308348869 train acc per epoch=>  0.8515585038972937\n",
      "val loss per epoch =>  1.6300539668602279 val acc per epoch => 0.8312895569620253 \n",
      "\n",
      "train loss per epoch =>  196 1.6100774018660835 train acc per epoch=>  0.8506313938923808\n",
      "val loss per epoch =>  1.6297136982784997 val acc per epoch => 0.8322784810126582 \n",
      "\n",
      "train loss per epoch =>  197 1.6075597367323269 train acc per epoch=>  0.8531409846547314\n",
      "val loss per epoch =>  1.6300453143783762 val acc per epoch => 0.8303006329113924 \n",
      "\n",
      "train loss per epoch =>  198 1.6075143622010566 train acc per epoch=>  0.8529891304652709\n",
      "val loss per epoch =>  1.6300874707065052 val acc per epoch => 0.829806170886076 \n",
      "\n",
      "train loss per epoch =>  199 1.6064552638841711 train acc per epoch=>  0.854719469309463\n",
      "val loss per epoch =>  1.6301497522788713 val acc per epoch => 0.8303006329113924 \n",
      "\n",
      "CPU times: user 28min 20s, sys: 5min 47s, total: 34min 7s\n",
      "Wall time: 36min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#transformations\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# cosine annealing learning rate scheduler\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=200\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    # evaluating the results\n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./res_block.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8322784810126582"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc_history) #get the max accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results of training\n",
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
