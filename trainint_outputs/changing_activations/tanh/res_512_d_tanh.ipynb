{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              Tanh-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "           Dropout-7           [-1, 64, 56, 56]               0\n",
      "              Tanh-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "           ResNet-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "          Dropout-14           [-1, 64, 56, 56]               0\n",
      "             Tanh-15           [-1, 64, 56, 56]               0\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "           ResNet-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "          Dropout-21           [-1, 64, 56, 56]               0\n",
      "             Tanh-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
      "           ResNet-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "          Dropout-28          [-1, 128, 28, 28]               0\n",
      "             Tanh-29          [-1, 128, 28, 28]               0\n",
      "           Conv2d-30          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "           Conv2d-32          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "           ResNet-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "          Dropout-37          [-1, 128, 28, 28]               0\n",
      "             Tanh-38          [-1, 128, 28, 28]               0\n",
      "           Conv2d-39          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "           ResNet-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "          Dropout-44          [-1, 128, 28, 28]               0\n",
      "             Tanh-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "           ResNet-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "          Dropout-51          [-1, 128, 28, 28]               0\n",
      "             Tanh-52          [-1, 128, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "           ResNet-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "          Dropout-58          [-1, 128, 28, 28]               0\n",
      "             Tanh-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "          Dropout-65          [-1, 128, 28, 28]               0\n",
      "             Tanh-66          [-1, 128, 28, 28]               0\n",
      "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "           ResNet-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70          [-1, 512, 14, 14]         589,824\n",
      "      BatchNorm2d-71          [-1, 512, 14, 14]           1,024\n",
      "          Dropout-72          [-1, 512, 14, 14]               0\n",
      "             Tanh-73          [-1, 512, 14, 14]               0\n",
      "           Conv2d-74          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-75          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-76          [-1, 512, 14, 14]          65,536\n",
      "      BatchNorm2d-77          [-1, 512, 14, 14]           1,024\n",
      "           ResNet-78          [-1, 512, 14, 14]               0\n",
      "AdaptiveAvgPool2d-79            [-1, 512, 1, 1]               0\n",
      "          Flatten-80                  [-1, 512]               0\n",
      "           Linear-81                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,961,610\n",
      "Trainable params: 4,961,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 92.65\n",
      "Params size (MB): 18.93\n",
      "Estimated Total Size (MB): 112.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Tanh(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.tanh(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        #self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        #self.resblock2 = ResNet(64, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        #self.resblock4=ResNet(64,64,stride=1)\n",
    "        #self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,64,stride=1)\n",
    "        self.resblock7=ResNet(64,64,stride=1)\n",
    "        self.resblock8=ResNet(64,128,stride=2)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,128,stride=1)\n",
    "        self.resblock12=ResNet(128,128,stride=1)\n",
    "        self.resblock13=ResNet(128,128,stride=1)\n",
    "        self.resblock14 =ResNet(128,512,stride=2)\n",
    "       #self.resblock15 =ResNet(512,512,stride=1)\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.tanh(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        #x = self.resblock1(x)\n",
    "        #x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        #x = self.resblock4(x)\n",
    "        #x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "        x= self.resblock14(x)\n",
    "        #x= self.resblock15(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 1.789175451564057 train acc per epoch=>  0.34353021099744246\n",
      "val loss per epoch =>  1.5551857646507552 val acc per epoch => 0.4176226265822785 \n",
      "\n",
      "train loss per epoch =>  1 1.5210574539116277 train acc per epoch=>  0.4478980179333016\n",
      "val loss per epoch =>  1.4984430859360514 val acc per epoch => 0.4584651898734177 \n",
      "\n",
      "train loss per epoch =>  2 1.4079318650238348 train acc per epoch=>  0.491572090808083\n",
      "val loss per epoch =>  1.3859931336173528 val acc per epoch => 0.5099881329113924 \n",
      "\n",
      "train loss per epoch =>  3 1.3338186347576053 train acc per epoch=>  0.5204283887772914\n",
      "val loss per epoch =>  1.3797310651103152 val acc per epoch => 0.5083069620253164 \n",
      "\n",
      "train loss per epoch =>  4 1.2715807723267305 train acc per epoch=>  0.5396019820971867\n",
      "val loss per epoch =>  1.2274968269505078 val acc per epoch => 0.5640822784810127 \n",
      "\n",
      "train loss per epoch =>  5 1.223933695832177 train acc per epoch=>  0.5630274936366264\n",
      "val loss per epoch =>  1.526631850230543 val acc per epoch => 0.47854034810126583 \n",
      "\n",
      "train loss per epoch =>  6 1.177374512185831 train acc per epoch=>  0.5780930307210254\n",
      "val loss per epoch =>  1.2503183959405633 val acc per epoch => 0.5543908227848101 \n",
      "\n",
      "train loss per epoch =>  7 1.140387721073902 train acc per epoch=>  0.5927229859030155\n",
      "val loss per epoch =>  1.2592917967446242 val acc per epoch => 0.5724881329113924 \n",
      "\n",
      "train loss per epoch =>  8 1.0980285236902554 train acc per epoch=>  0.6067615090428716\n",
      "val loss per epoch =>  1.1415484924859638 val acc per epoch => 0.6051226265822784 \n",
      "\n",
      "train loss per epoch =>  9 1.0690797680174298 train acc per epoch=>  0.6197010870174984\n",
      "val loss per epoch =>  1.0335857498494885 val acc per epoch => 0.6333069620253164 \n",
      "\n",
      "train loss per epoch =>  10 1.0390734326504076 train acc per epoch=>  0.6308823529716647\n",
      "val loss per epoch =>  0.9951218547700327 val acc per epoch => 0.6487341772151899 \n",
      "\n",
      "train loss per epoch =>  11 1.017634310685765 train acc per epoch=>  0.6389905691451734\n",
      "val loss per epoch =>  1.0259852326368983 val acc per epoch => 0.6368670886075949 \n",
      "\n",
      "train loss per epoch =>  12 0.9969621601007174 train acc per epoch=>  0.6445971867617439\n",
      "val loss per epoch =>  1.0761517648455463 val acc per epoch => 0.6314280063291139 \n",
      "\n",
      "train loss per epoch =>  13 0.9694919517582945 train acc per epoch=>  0.657065217452281\n",
      "val loss per epoch =>  0.9218577470960496 val acc per epoch => 0.6754351265822784 \n",
      "\n",
      "train loss per epoch =>  14 0.948520304449379 train acc per epoch=>  0.662883631713555\n",
      "val loss per epoch =>  0.9068583295315127 val acc per epoch => 0.6815664556962026 \n",
      "\n",
      "train loss per epoch =>  15 0.9342951631302114 train acc per epoch=>  0.6678828325722833\n",
      "val loss per epoch =>  0.9360446650770646 val acc per epoch => 0.6741495253164557 \n",
      "\n",
      "train loss per epoch =>  16 0.9135653545789402 train acc per epoch=>  0.6746283568384702\n",
      "val loss per epoch =>  0.8713845444630973 val acc per epoch => 0.6986748417721519 \n",
      "\n",
      "train loss per epoch =>  17 0.892358626398589 train acc per epoch=>  0.6846387468640457\n",
      "val loss per epoch =>  0.8473667660845986 val acc per epoch => 0.7111352848101266 \n",
      "\n",
      "train loss per epoch =>  18 0.881923485290059 train acc per epoch=>  0.6896779092071611\n",
      "val loss per epoch =>  0.9324150266526621 val acc per epoch => 0.681368670886076 \n",
      "\n",
      "train loss per epoch =>  19 0.8650008186964733 train acc per epoch=>  0.6941096547924345\n",
      "val loss per epoch =>  0.8511373619490032 val acc per epoch => 0.7028283227848101 \n",
      "\n",
      "train loss per epoch =>  20 0.8517669667978116 train acc per epoch=>  0.6981497762148338\n",
      "val loss per epoch =>  0.8088696878167647 val acc per epoch => 0.7145965189873418 \n",
      "\n",
      "train loss per epoch =>  21 0.8339517802533591 train acc per epoch=>  0.7053508632018438\n",
      "val loss per epoch =>  0.8662666670883759 val acc per epoch => 0.7012460443037974 \n",
      "\n",
      "train loss per epoch =>  22 0.8215811168751144 train acc per epoch=>  0.7093310421690002\n",
      "val loss per epoch =>  0.811926980561848 val acc per epoch => 0.7231012658227848 \n",
      "\n",
      "train loss per epoch =>  23 0.8062265242457085 train acc per epoch=>  0.7137228261174449\n",
      "val loss per epoch =>  0.7893845167341111 val acc per epoch => 0.7277492088607594 \n",
      "\n",
      "train loss per epoch =>  24 0.7914057236803157 train acc per epoch=>  0.7198129796311069\n",
      "val loss per epoch =>  0.7696150745017619 val acc per epoch => 0.7349683544303798 \n",
      "\n",
      "train loss per epoch =>  25 0.7783507785528821 train acc per epoch=>  0.7244804987822042\n",
      "val loss per epoch =>  0.7698816626886779 val acc per epoch => 0.7353639240506329 \n",
      "\n",
      "train loss per epoch =>  26 0.7694156185135512 train acc per epoch=>  0.728360773657289\n",
      "val loss per epoch =>  0.735717388648021 val acc per epoch => 0.7451542721518988 \n",
      "\n",
      "train loss per epoch =>  27 0.7599149568916281 train acc per epoch=>  0.7313139386799025\n",
      "val loss per epoch =>  0.7189635264722607 val acc per epoch => 0.7498022151898734 \n",
      "\n",
      "train loss per epoch =>  28 0.74275763946421 train acc per epoch=>  0.7362851662099209\n",
      "val loss per epoch =>  0.7252229655845256 val acc per epoch => 0.749307753164557 \n",
      "\n",
      "train loss per epoch =>  29 0.7356264803872998 train acc per epoch=>  0.7412484015345269\n",
      "val loss per epoch =>  0.7454071656058107 val acc per epoch => 0.7451542721518988 \n",
      "\n",
      "train loss per epoch =>  30 0.7204725490811535 train acc per epoch=>  0.7457560741383097\n",
      "val loss per epoch =>  0.7151908086070532 val acc per epoch => 0.753065664556962 \n",
      "\n",
      "train loss per epoch =>  31 0.7088600462659851 train acc per epoch=>  0.7477621483375959\n",
      "val loss per epoch =>  0.7296007992346075 val acc per epoch => 0.754746835443038 \n",
      "\n",
      "train loss per epoch =>  32 0.6999707578698082 train acc per epoch=>  0.7529211957436388\n",
      "val loss per epoch =>  0.6803774207453185 val acc per epoch => 0.7662183544303798 \n",
      "\n",
      "train loss per epoch =>  33 0.6889391158852736 train acc per epoch=>  0.756777493697603\n",
      "val loss per epoch =>  0.7022898559328876 val acc per epoch => 0.7593947784810127 \n",
      "\n",
      "train loss per epoch =>  34 0.6843691512446879 train acc per epoch=>  0.7569573210633319\n",
      "val loss per epoch =>  0.6702575717545762 val acc per epoch => 0.7713607594936709 \n",
      "\n",
      "train loss per epoch =>  35 0.6688496404139282 train acc per epoch=>  0.7647858056265985\n",
      "val loss per epoch =>  0.6661229231689549 val acc per epoch => 0.7693829113924051 \n",
      "\n",
      "train loss per epoch =>  36 0.6631868566241106 train acc per epoch=>  0.7653372761843454\n",
      "val loss per epoch =>  0.668005785987347 val acc per epoch => 0.7685917721518988 \n",
      "\n",
      "train loss per epoch =>  37 0.6595757413093392 train acc per epoch=>  0.7675551470283353\n",
      "val loss per epoch =>  0.6504281687585614 val acc per epoch => 0.7779865506329114 \n",
      "\n",
      "train loss per epoch =>  38 0.6525142490101592 train acc per epoch=>  0.7706801470588235\n",
      "val loss per epoch =>  0.6521912420852275 val acc per epoch => 0.7786787974683544 \n",
      "\n",
      "train loss per epoch =>  39 0.6445205373989652 train acc per epoch=>  0.7726942136159638\n",
      "val loss per epoch =>  0.6402943568139137 val acc per epoch => 0.7823378164556962 \n",
      "\n",
      "train loss per epoch =>  40 0.6373974442329553 train acc per epoch=>  0.7742167519181585\n",
      "val loss per epoch =>  0.6408488218542896 val acc per epoch => 0.7815466772151899 \n",
      "\n",
      "train loss per epoch =>  41 0.6325591917690414 train acc per epoch=>  0.7738291240104324\n",
      "val loss per epoch =>  0.6348131276384185 val acc per epoch => 0.7807555379746836 \n",
      "\n",
      "train loss per epoch =>  42 0.6250561930791801 train acc per epoch=>  0.7786085358666032\n",
      "val loss per epoch =>  0.639238207023355 val acc per epoch => 0.7794699367088608 \n",
      "\n",
      "train loss per epoch =>  43 0.6236723575293256 train acc per epoch=>  0.7789242327060846\n",
      "val loss per epoch =>  0.63611309098292 val acc per epoch => 0.7834256329113924 \n",
      "\n",
      "train loss per epoch =>  44 0.6177164619536046 train acc per epoch=>  0.7801470588845061\n",
      "val loss per epoch =>  0.6279292031179501 val acc per epoch => 0.7861946202531646 \n",
      "\n",
      "train loss per epoch =>  45 0.6123741831620941 train acc per epoch=>  0.7827965153757569\n",
      "val loss per epoch =>  0.6282187613505351 val acc per epoch => 0.7848101265822784 \n",
      "\n",
      "train loss per epoch =>  46 0.6129101872291711 train acc per epoch=>  0.7841911765620532\n",
      "val loss per epoch =>  0.6277962549577786 val acc per epoch => 0.7872824367088608 \n",
      "\n",
      "train loss per epoch =>  47 0.6101085897296896 train acc per epoch=>  0.784550831293511\n",
      "val loss per epoch =>  0.6266680661636063 val acc per epoch => 0.7874802215189873 \n",
      "\n",
      "train loss per epoch =>  48 0.6074011607853043 train acc per epoch=>  0.7850623402144293\n",
      "val loss per epoch =>  0.6278134637241122 val acc per epoch => 0.7868868670886076 \n",
      "\n",
      "train loss per epoch =>  49 0.609032714382157 train acc per epoch=>  0.7860533887772914\n",
      "val loss per epoch =>  0.6271278511119794 val acc per epoch => 0.7867879746835443 \n",
      "\n",
      "CPU times: user 4min 48s, sys: 1min 2s, total: 5min 51s\n",
      "Wall time: 7min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../../data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 50)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=50\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./res_512_d_tanh.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7874802215189873"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
