{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]           4,704\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "              ReLU-3         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 32, 56, 56]               0\n",
      "            Conv2d-5           [-1, 32, 56, 56]           9,216\n",
      "       BatchNorm2d-6           [-1, 32, 56, 56]              64\n",
      "              ReLU-7           [-1, 32, 56, 56]               0\n",
      "            Conv2d-8           [-1, 32, 56, 56]           9,216\n",
      "       BatchNorm2d-9           [-1, 32, 56, 56]              64\n",
      "           ResNet-10           [-1, 32, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]          18,432\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "           Conv2d-16           [-1, 64, 56, 56]           2,048\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "           ResNet-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "           Conv2d-22           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-23           [-1, 64, 56, 56]             128\n",
      "           ResNet-24           [-1, 64, 56, 56]               0\n",
      "           Conv2d-25           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-26           [-1, 64, 56, 56]             128\n",
      "             ReLU-27           [-1, 64, 56, 56]               0\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "           ResNet-30           [-1, 64, 56, 56]               0\n",
      "           Conv2d-31           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-32           [-1, 64, 56, 56]             128\n",
      "             ReLU-33           [-1, 64, 56, 56]               0\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "           ResNet-36           [-1, 64, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
      "             ReLU-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "           Conv2d-42          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "           ResNet-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-46          [-1, 128, 28, 28]             256\n",
      "             ReLU-47          [-1, 128, 28, 28]               0\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "           ResNet-50          [-1, 128, 28, 28]               0\n",
      "           Conv2d-51          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-52          [-1, 128, 28, 28]             256\n",
      "             ReLU-53          [-1, 128, 28, 28]               0\n",
      "           Conv2d-54          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-55          [-1, 128, 28, 28]             256\n",
      "           ResNet-56          [-1, 128, 28, 28]               0\n",
      "           Conv2d-57          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-58          [-1, 128, 28, 28]             256\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "           ResNet-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-70          [-1, 256, 14, 14]             512\n",
      "             ReLU-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "           Conv2d-74          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "           ResNet-76          [-1, 256, 14, 14]               0\n",
      "           Conv2d-77          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "             ReLU-79          [-1, 256, 14, 14]               0\n",
      "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "           ResNet-82          [-1, 256, 14, 14]               0\n",
      "           Conv2d-83          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-84          [-1, 256, 14, 14]             512\n",
      "             ReLU-85          [-1, 256, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "           ResNet-88          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-89            [-1, 256, 1, 1]               0\n",
      "          Flatten-90                  [-1, 256]               0\n",
      "           Linear-91                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,997,802\n",
      "Trainable params: 4,997,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 86.52\n",
      "Params size (MB): 19.07\n",
      "Estimated Total Size (MB): 106.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.relu(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        self.resblock2 = ResNet(32, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        self.resblock4=ResNet(64,64,stride=1)\n",
    "        self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,128,stride=2)\n",
    "        self.resblock7=ResNet(128,128,stride=1)\n",
    "        self.resblock8=ResNet(128,128,stride=1)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,256,stride=2)\n",
    "        self.resblock12=ResNet(256,256,stride=1)\n",
    "        self.resblock13=ResNet(256,256,stride=1)\n",
    "\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=256, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        x = self.resblock1(x)\n",
    "        x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "      \n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 1.618155569066782 train acc per epoch=>  0.41683184147795754\n",
      "val loss per epoch =>  1.432601601262636 val acc per epoch => 0.5059335443037974 \n",
      "\n",
      "train loss per epoch =>  1 1.2458699456871014 train acc per epoch=>  0.5526334719584726\n",
      "val loss per epoch =>  1.380646903303605 val acc per epoch => 0.5145371835443038 \n",
      "\n",
      "train loss per epoch =>  2 1.0852812034699617 train acc per epoch=>  0.6168558184448105\n",
      "val loss per epoch =>  1.113792039925539 val acc per epoch => 0.6261867088607594 \n",
      "\n",
      "train loss per epoch =>  3 0.9718092654062354 train acc per epoch=>  0.6579403772073633\n",
      "val loss per epoch =>  0.9567733248577842 val acc per epoch => 0.6596123417721519 \n",
      "\n",
      "train loss per epoch =>  4 0.8954425213282066 train acc per epoch=>  0.686457001339749\n",
      "val loss per epoch =>  1.035807015020636 val acc per epoch => 0.655557753164557 \n",
      "\n",
      "train loss per epoch =>  5 0.8295614501399458 train acc per epoch=>  0.7105338875290073\n",
      "val loss per epoch =>  0.9941224395474301 val acc per epoch => 0.6618868670886076 \n",
      "\n",
      "train loss per epoch =>  6 0.7755604668346512 train acc per epoch=>  0.7284327045730923\n",
      "val loss per epoch =>  0.8199426214906234 val acc per epoch => 0.7191455696202531 \n",
      "\n",
      "train loss per epoch =>  7 0.7348817990868902 train acc per epoch=>  0.742874840183941\n",
      "val loss per epoch =>  0.896386599993404 val acc per epoch => 0.6940268987341772 \n",
      "\n",
      "train loss per epoch =>  8 0.6962862824997329 train acc per epoch=>  0.7560022378821507\n",
      "val loss per epoch =>  0.858188708371754 val acc per epoch => 0.7111352848101266 \n",
      "\n",
      "train loss per epoch =>  9 0.6692634133426735 train acc per epoch=>  0.7671954922968774\n",
      "val loss per epoch =>  0.7830033355121371 val acc per epoch => 0.7321993670886076 \n",
      "\n",
      "train loss per epoch =>  10 0.6348125303492826 train acc per epoch=>  0.7768622123067032\n",
      "val loss per epoch =>  0.7141201398040675 val acc per epoch => 0.7564280063291139 \n",
      "\n",
      "train loss per epoch =>  11 0.6136653636727492 train acc per epoch=>  0.7857137148642479\n",
      "val loss per epoch =>  0.6798020171213753 val acc per epoch => 0.7637460443037974 \n",
      "\n",
      "train loss per epoch =>  12 0.5848476430949043 train acc per epoch=>  0.7968949808184144\n",
      "val loss per epoch =>  0.7413718368433699 val acc per epoch => 0.7527689873417721 \n",
      "\n",
      "train loss per epoch =>  13 0.5630042501117872 train acc per epoch=>  0.8027373721532505\n",
      "val loss per epoch =>  0.6827377420437487 val acc per epoch => 0.7678006329113924 \n",
      "\n",
      "train loss per epoch =>  14 0.5405287065774279 train acc per epoch=>  0.8110573849714625\n",
      "val loss per epoch =>  0.6797373528721966 val acc per epoch => 0.7731408227848101 \n",
      "\n",
      "train loss per epoch =>  15 0.5264136595341861 train acc per epoch=>  0.8157608696566824\n",
      "val loss per epoch =>  0.6853817358047147 val acc per epoch => 0.7702729430379747 \n",
      "\n",
      "train loss per epoch =>  16 0.49920427479097607 train acc per epoch=>  0.8237212275909951\n",
      "val loss per epoch =>  0.7498584238788749 val acc per epoch => 0.7546479430379747 \n",
      "\n",
      "train loss per epoch =>  17 0.4834781769291519 train acc per epoch=>  0.8287603900865521\n",
      "val loss per epoch =>  0.8282276693778702 val acc per epoch => 0.7615704113924051 \n",
      "\n",
      "train loss per epoch =>  18 0.467289704343547 train acc per epoch=>  0.8361612852577054\n",
      "val loss per epoch =>  0.7114210792734653 val acc per epoch => 0.771064082278481 \n",
      "\n",
      "train loss per epoch =>  19 0.4491927064288303 train acc per epoch=>  0.8432344949763754\n",
      "val loss per epoch =>  0.6776506436776512 val acc per epoch => 0.7774920886075949 \n",
      "\n",
      "train loss per epoch =>  20 0.4300979766851801 train acc per epoch=>  0.8486013427719741\n",
      "val loss per epoch =>  0.7371539447126509 val acc per epoch => 0.7698773734177216 \n",
      "\n",
      "train loss per epoch =>  21 0.41994999009934836 train acc per epoch=>  0.8524456522653779\n",
      "val loss per epoch =>  0.7109186453155324 val acc per epoch => 0.775810917721519 \n",
      "\n",
      "train loss per epoch =>  22 0.4038003128584084 train acc per epoch=>  0.8574528453295188\n",
      "val loss per epoch =>  0.7242578984815863 val acc per epoch => 0.7666139240506329 \n",
      "\n",
      "train loss per epoch =>  23 0.3889786227203696 train acc per epoch=>  0.8628276854829715\n",
      "val loss per epoch =>  0.7160998590384857 val acc per epoch => 0.7747231012658228 \n",
      "\n",
      "train loss per epoch =>  24 0.37777317591640347 train acc per epoch=>  0.8666879795396419\n",
      "val loss per epoch =>  0.8657660544673099 val acc per epoch => 0.7503955696202531 \n",
      "\n",
      "train loss per epoch =>  25 0.36575613428107306 train acc per epoch=>  0.869181585738726\n",
      "val loss per epoch =>  0.6888280808925629 val acc per epoch => 0.7855023734177216 \n",
      "\n",
      "train loss per epoch =>  26 0.35473622145402767 train acc per epoch=>  0.874432544726545\n",
      "val loss per epoch =>  0.6516100553017629 val acc per epoch => 0.7964794303797469 \n",
      "\n",
      "train loss per epoch =>  27 0.34123775930813205 train acc per epoch=>  0.8791879795091536\n",
      "val loss per epoch =>  0.6909176209304906 val acc per epoch => 0.7912381329113924 \n",
      "\n",
      "train loss per epoch =>  28 0.32666292188265134 train acc per epoch=>  0.8858375959384167\n",
      "val loss per epoch =>  0.7630656504932838 val acc per epoch => 0.7817444620253164 \n",
      "\n",
      "train loss per epoch =>  29 0.31423927656829814 train acc per epoch=>  0.8892822890635341\n",
      "val loss per epoch =>  0.7353176814091357 val acc per epoch => 0.7841178797468354 \n",
      "\n",
      "train loss per epoch =>  30 0.3092162138055962 train acc per epoch=>  0.8913283248996491\n",
      "val loss per epoch =>  0.7293175032621697 val acc per epoch => 0.7767998417721519 \n",
      "\n",
      "train loss per epoch =>  31 0.2940065471831795 train acc per epoch=>  0.8956122122457265\n",
      "val loss per epoch =>  0.7332377965691723 val acc per epoch => 0.7941060126582279 \n",
      "\n",
      "train loss per epoch =>  32 0.2797692114358668 train acc per epoch=>  0.9004675511204069\n",
      "val loss per epoch =>  0.7312117977232873 val acc per epoch => 0.7903481012658228 \n",
      "\n",
      "train loss per epoch =>  33 0.27419973330577013 train acc per epoch=>  0.9042798913348361\n",
      "val loss per epoch =>  0.7532261327097688 val acc per epoch => 0.7857001582278481 \n",
      "\n",
      "train loss per epoch =>  34 0.26507425756024583 train acc per epoch=>  0.9054267903423066\n",
      "val loss per epoch =>  0.7343561879441708 val acc per epoch => 0.7931170886075949 \n",
      "\n",
      "train loss per epoch =>  35 0.2544288031966485 train acc per epoch=>  0.9092671035805626\n",
      "val loss per epoch =>  0.7251747574987291 val acc per epoch => 0.7965783227848101 \n",
      "\n",
      "train loss per epoch =>  36 0.2482922921514572 train acc per epoch=>  0.9103420716722298\n",
      "val loss per epoch =>  0.7448476236077803 val acc per epoch => 0.7987539556962026 \n",
      "\n",
      "train loss per epoch =>  37 0.23596273273077156 train acc per epoch=>  0.9150335677444478\n",
      "val loss per epoch =>  0.7379067702383935 val acc per epoch => 0.7946993670886076 \n",
      "\n",
      "train loss per epoch =>  38 0.22589839669063572 train acc per epoch=>  0.9206641624345804\n",
      "val loss per epoch =>  0.732508436411242 val acc per epoch => 0.8006329113924051 \n",
      "\n",
      "train loss per epoch =>  39 0.21921548634157767 train acc per epoch=>  0.9228500639995956\n",
      "val loss per epoch =>  0.7649004278303702 val acc per epoch => 0.7963805379746836 \n",
      "\n",
      "train loss per epoch =>  40 0.21460978000822578 train acc per epoch=>  0.924852141943734\n",
      "val loss per epoch =>  0.7924301386634006 val acc per epoch => 0.7917325949367089 \n",
      "\n",
      "train loss per epoch =>  41 0.20658770474174137 train acc per epoch=>  0.9270220587930411\n",
      "val loss per epoch =>  0.7998447893541071 val acc per epoch => 0.796182753164557 \n",
      "\n",
      "train loss per epoch =>  42 0.20056525789334645 train acc per epoch=>  0.929128037084399\n",
      "val loss per epoch =>  0.8392621165589441 val acc per epoch => 0.7903481012658228 \n",
      "\n",
      "train loss per epoch =>  43 0.1892454135600868 train acc per epoch=>  0.9333200128487004\n",
      "val loss per epoch =>  0.7780674778962438 val acc per epoch => 0.7996439873417721 \n",
      "\n",
      "train loss per epoch =>  44 0.18361165220170375 train acc per epoch=>  0.9352901215138643\n",
      "val loss per epoch =>  0.8080083418496048 val acc per epoch => 0.7923259493670886 \n",
      "\n",
      "train loss per epoch =>  45 0.1774281371013283 train acc per epoch=>  0.9387068414627133\n",
      "val loss per epoch =>  0.8215818242936195 val acc per epoch => 0.7982594936708861 \n",
      "\n",
      "train loss per epoch =>  46 0.17505272000532626 train acc per epoch=>  0.938550991018105\n",
      "val loss per epoch =>  0.9049809228015852 val acc per epoch => 0.7809533227848101 \n",
      "\n",
      "train loss per epoch =>  47 0.16661301922157903 train acc per epoch=>  0.9419397379431274\n",
      "val loss per epoch =>  0.7937915215763864 val acc per epoch => 0.8078520569620253 \n",
      "\n",
      "train loss per epoch =>  48 0.1619262411580671 train acc per epoch=>  0.9425431585982632\n",
      "val loss per epoch =>  0.8581041565424279 val acc per epoch => 0.7948971518987342 \n",
      "\n",
      "train loss per epoch =>  49 0.15185110204283844 train acc per epoch=>  0.9468110613810742\n",
      "val loss per epoch =>  0.8300682193116297 val acc per epoch => 0.8003362341772152 \n",
      "\n",
      "train loss per epoch =>  50 0.14849576071057174 train acc per epoch=>  0.9476262788333552\n",
      "val loss per epoch =>  0.8483769127839729 val acc per epoch => 0.7969738924050633 \n",
      "\n",
      "train loss per epoch =>  51 0.14298170844993324 train acc per epoch=>  0.9497202685117112\n",
      "val loss per epoch =>  0.8475378951694392 val acc per epoch => 0.8024129746835443 \n",
      "\n",
      "train loss per epoch =>  52 0.13952462357061599 train acc per epoch=>  0.9503196930641409\n",
      "val loss per epoch =>  0.888293587708775 val acc per epoch => 0.7989517405063291 \n",
      "\n",
      "train loss per epoch =>  53 0.13449583528444287 train acc per epoch=>  0.9521299553344317\n",
      "val loss per epoch =>  0.9931716549245617 val acc per epoch => 0.7793710443037974 \n",
      "\n",
      "train loss per epoch =>  54 0.13032040833626563 train acc per epoch=>  0.9535685742602629\n",
      "val loss per epoch =>  0.8881938585752174 val acc per epoch => 0.8033030063291139 \n",
      "\n",
      "train loss per epoch =>  55 0.12317681846106449 train acc per epoch=>  0.9567255435697258\n",
      "val loss per epoch =>  0.8649867840960056 val acc per epoch => 0.8031052215189873 \n",
      "\n",
      "train loss per epoch =>  56 0.11863752183459146 train acc per epoch=>  0.956849424582918\n",
      "val loss per epoch =>  0.9339871519728552 val acc per epoch => 0.7997428797468354 \n",
      "\n",
      "train loss per epoch =>  57 0.12243850646383317 train acc per epoch=>  0.9562180307515137\n",
      "val loss per epoch =>  0.8859170433841174 val acc per epoch => 0.7969738924050633 \n",
      "\n",
      "train loss per epoch =>  58 0.11436109585911416 train acc per epoch=>  0.9598065857082376\n",
      "val loss per epoch =>  0.8781484425822391 val acc per epoch => 0.8040941455696202 \n",
      "\n",
      "train loss per epoch =>  59 0.10587687324494352 train acc per epoch=>  0.9630754475703325\n",
      "val loss per epoch =>  0.8584291308741027 val acc per epoch => 0.8134889240506329 \n",
      "\n",
      "train loss per epoch =>  60 0.10764694018075076 train acc per epoch=>  0.9620204604495212\n",
      "val loss per epoch =>  0.9113436077214494 val acc per epoch => 0.8050830696202531 \n",
      "\n",
      "train loss per epoch =>  61 0.10448256234073883 train acc per epoch=>  0.9634271099439362\n",
      "val loss per epoch =>  0.9025822017766252 val acc per epoch => 0.8109177215189873 \n",
      "\n",
      "train loss per epoch =>  62 0.09863015816158727 train acc per epoch=>  0.9657289003167311\n",
      "val loss per epoch =>  0.9516173699234105 val acc per epoch => 0.8040941455696202 \n",
      "\n",
      "train loss per epoch =>  63 0.09743623703222751 train acc per epoch=>  0.9659087276824599\n",
      "val loss per epoch =>  0.9739309375799154 val acc per epoch => 0.7912381329113924 \n",
      "\n",
      "train loss per epoch =>  64 0.09153493791532791 train acc per epoch=>  0.9677030050846012\n",
      "val loss per epoch =>  0.9336406573464598 val acc per epoch => 0.8059731012658228 \n",
      "\n",
      "train loss per epoch =>  65 0.08865284008900527 train acc per epoch=>  0.9686740729814906\n",
      "val loss per epoch =>  1.0454052037830595 val acc per epoch => 0.7943037974683544 \n",
      "\n",
      "train loss per epoch =>  66 0.09064330140133496 train acc per epoch=>  0.9681505754475703\n",
      "val loss per epoch =>  0.926571431793744 val acc per epoch => 0.8090387658227848 \n",
      "\n",
      "train loss per epoch =>  67 0.08307937374981621 train acc per epoch=>  0.9695931906285493\n",
      "val loss per epoch =>  0.9892705467682851 val acc per epoch => 0.8007318037974683 \n",
      "\n",
      "train loss per epoch =>  68 0.0791819440034192 train acc per epoch=>  0.971863011569928\n",
      "val loss per epoch =>  0.964675866350343 val acc per epoch => 0.8061708860759493 \n",
      "\n",
      "train loss per epoch =>  69 0.08095864362328711 train acc per epoch=>  0.971795076756831\n",
      "val loss per epoch =>  1.0098620698421816 val acc per epoch => 0.8035007911392406 \n",
      "\n",
      "train loss per epoch =>  70 0.07883768832034733 train acc per epoch=>  0.9720748082146315\n",
      "val loss per epoch =>  1.0120807474927058 val acc per epoch => 0.8058742088607594 \n",
      "\n",
      "train loss per epoch =>  71 0.07323247930297004 train acc per epoch=>  0.9755914322555522\n",
      "val loss per epoch =>  0.9830257100395009 val acc per epoch => 0.8051819620253164 \n",
      "\n",
      "train loss per epoch =>  72 0.07110382057726383 train acc per epoch=>  0.9752078006029739\n",
      "val loss per epoch =>  0.9835640233528765 val acc per epoch => 0.8065664556962026 \n",
      "\n",
      "train loss per epoch =>  73 0.06738917394291105 train acc per epoch=>  0.976019021800107\n",
      "val loss per epoch =>  1.0419071045102952 val acc per epoch => 0.8058742088607594 \n",
      "\n",
      "train loss per epoch =>  74 0.06848054166282992 train acc per epoch=>  0.9766783888077797\n",
      "val loss per epoch =>  1.0244112754169898 val acc per epoch => 0.8051819620253164 \n",
      "\n",
      "train loss per epoch =>  75 0.06632763083519229 train acc per epoch=>  0.9763786765315648\n",
      "val loss per epoch =>  1.0690817953665046 val acc per epoch => 0.807753164556962 \n",
      "\n",
      "train loss per epoch =>  76 0.06376018623828583 train acc per epoch=>  0.9781969310072682\n",
      "val loss per epoch =>  1.0236984476258484 val acc per epoch => 0.805379746835443 \n",
      "\n",
      "train loss per epoch =>  77 0.06308380534629458 train acc per epoch=>  0.9777253837231785\n",
      "val loss per epoch =>  1.041105040266544 val acc per epoch => 0.8068631329113924 \n",
      "\n",
      "train loss per epoch =>  78 0.05694320539841452 train acc per epoch=>  0.980167039672432\n",
      "val loss per epoch =>  1.0409008445619028 val acc per epoch => 0.8083465189873418 \n",
      "\n",
      "train loss per epoch =>  79 0.05447728242939505 train acc per epoch=>  0.9804947251249152\n",
      "val loss per epoch =>  1.0301559129847755 val acc per epoch => 0.8104232594936709 \n",
      "\n",
      "train loss per epoch =>  80 0.055436497813810014 train acc per epoch=>  0.9808343990379588\n",
      "val loss per epoch =>  1.0903240842155264 val acc per epoch => 0.8097310126582279 \n",
      "\n",
      "train loss per epoch =>  81 0.05450187704842681 train acc per epoch=>  0.9815057545061916\n",
      "val loss per epoch =>  1.0376237872280651 val acc per epoch => 0.8072587025316456 \n",
      "\n",
      "train loss per epoch =>  82 0.05127104747411616 train acc per epoch=>  0.9826326726952477\n",
      "val loss per epoch =>  1.1027133649663081 val acc per epoch => 0.8045886075949367 \n",
      "\n",
      "train loss per epoch =>  83 0.04900261386454848 train acc per epoch=>  0.9829443734320228\n",
      "val loss per epoch =>  1.1134797010240676 val acc per epoch => 0.8058742088607594 \n",
      "\n",
      "train loss per epoch =>  84 0.04792448066894318 train acc per epoch=>  0.9831321931556057\n",
      "val loss per epoch =>  1.1134280407730537 val acc per epoch => 0.8045886075949367 \n",
      "\n",
      "train loss per epoch =>  85 0.045588848036963996 train acc per epoch=>  0.9842431266289537\n",
      "val loss per epoch =>  1.0831191630303105 val acc per epoch => 0.8082476265822784 \n",
      "\n",
      "train loss per epoch =>  86 0.04614164570198797 train acc per epoch=>  0.9838395141579611\n",
      "val loss per epoch =>  1.0920496160470987 val acc per epoch => 0.8060719936708861 \n",
      "\n",
      "train loss per epoch =>  87 0.04136616683593663 train acc per epoch=>  0.9859814578309998\n",
      "val loss per epoch =>  1.1130997900721393 val acc per epoch => 0.805379746835443 \n",
      "\n",
      "train loss per epoch =>  88 0.04090265223227651 train acc per epoch=>  0.9857896420047106\n",
      "val loss per epoch =>  1.0876323388347142 val acc per epoch => 0.8086431962025317 \n",
      "\n",
      "train loss per epoch =>  89 0.03847098543756473 train acc per epoch=>  0.9869085678359126\n",
      "val loss per epoch =>  1.1437633663793154 val acc per epoch => 0.8087420886075949 \n",
      "\n",
      "train loss per epoch =>  90 0.039327165146436915 train acc per epoch=>  0.9861213235599001\n",
      "val loss per epoch =>  1.0961040326311617 val acc per epoch => 0.810126582278481 \n",
      "\n",
      "train loss per epoch =>  91 0.03725156481699337 train acc per epoch=>  0.98686061384123\n",
      "val loss per epoch =>  1.145047578630568 val acc per epoch => 0.8064675632911392 \n",
      "\n",
      "train loss per epoch =>  92 0.03579293075940617 train acc per epoch=>  0.9880594629460894\n",
      "val loss per epoch =>  1.125625975524323 val acc per epoch => 0.8110166139240507 \n",
      "\n",
      "train loss per epoch =>  93 0.032641750444179336 train acc per epoch=>  0.9887907608695652\n",
      "val loss per epoch =>  1.1682348032540912 val acc per epoch => 0.8111155063291139 \n",
      "\n",
      "train loss per epoch =>  94 0.0354121464210541 train acc per epoch=>  0.9872522378516624\n",
      "val loss per epoch =>  1.1155419704280323 val acc per epoch => 0.8093354430379747 \n",
      "\n",
      "train loss per epoch =>  95 0.03017477254333246 train acc per epoch=>  0.9898577366033783\n",
      "val loss per epoch =>  1.2038819269288945 val acc per epoch => 0.807753164556962 \n",
      "\n",
      "train loss per epoch =>  96 0.02997433061501883 train acc per epoch=>  0.9899896099744245\n",
      "val loss per epoch =>  1.1846655833570263 val acc per epoch => 0.8088409810126582 \n",
      "\n",
      "train loss per epoch =>  97 0.03054027869919901 train acc per epoch=>  0.9892303388746803\n",
      "val loss per epoch =>  1.1817445174048218 val acc per epoch => 0.8115110759493671 \n",
      "\n",
      "train loss per epoch =>  98 0.029044194436987 train acc per epoch=>  0.9899976023322786\n",
      "val loss per epoch =>  1.2440219785593734 val acc per epoch => 0.8106210443037974 \n",
      "\n",
      "train loss per epoch =>  99 0.030809642021160794 train acc per epoch=>  0.9896978900560638\n",
      "val loss per epoch =>  1.126610132712352 val acc per epoch => 0.8122033227848101 \n",
      "\n",
      "train loss per epoch =>  100 0.02641955520326505 train acc per epoch=>  0.9908647698819485\n",
      "val loss per epoch =>  1.217593058000637 val acc per epoch => 0.8097310126582279 \n",
      "\n",
      "train loss per epoch =>  101 0.0266623183560279 train acc per epoch=>  0.9905290920716112\n",
      "val loss per epoch =>  1.1854562080359157 val acc per epoch => 0.8161590189873418 \n",
      "\n",
      "train loss per epoch =>  102 0.022492172153335533 train acc per epoch=>  0.9919157609000535\n",
      "val loss per epoch =>  1.2452581713471231 val acc per epoch => 0.8132911392405063 \n",
      "\n",
      "train loss per epoch =>  103 0.02465637571647611 train acc per epoch=>  0.9915081521739131\n",
      "val loss per epoch =>  1.2424183985855006 val acc per epoch => 0.8082476265822784 \n",
      "\n",
      "train loss per epoch =>  104 0.02434080273902658 train acc per epoch=>  0.9915281329923273\n",
      "val loss per epoch =>  1.2434573452683944 val acc per epoch => 0.8097310126582279 \n",
      "\n",
      "train loss per epoch =>  105 0.02180280054093498 train acc per epoch=>  0.9926270780051151\n",
      "val loss per epoch =>  1.230226633669455 val acc per epoch => 0.8139833860759493 \n",
      "\n",
      "train loss per epoch =>  106 0.022182772427325703 train acc per epoch=>  0.9927429668128948\n",
      "val loss per epoch =>  1.225058035005497 val acc per epoch => 0.8084454113924051 \n",
      "\n",
      "train loss per epoch =>  107 0.019728751237834078 train acc per epoch=>  0.9932145140969845\n",
      "val loss per epoch =>  1.250675021093103 val acc per epoch => 0.8091376582278481 \n",
      "\n",
      "train loss per epoch =>  108 0.021005432062726967 train acc per epoch=>  0.992934782639184\n",
      "val loss per epoch =>  1.26765328796604 val acc per epoch => 0.8098299050632911 \n",
      "\n",
      "train loss per epoch =>  109 0.019657598544463584 train acc per epoch=>  0.9930826407259382\n",
      "val loss per epoch =>  1.2790627607816383 val acc per epoch => 0.8105221518987342 \n",
      "\n",
      "train loss per epoch =>  110 0.017644261113306046 train acc per epoch=>  0.9938738811046571\n",
      "val loss per epoch =>  1.3066053926190244 val acc per epoch => 0.8112143987341772 \n",
      "\n",
      "train loss per epoch =>  111 0.01764369848773212 train acc per epoch=>  0.9941256393861893\n",
      "val loss per epoch =>  1.274195282519618 val acc per epoch => 0.811807753164557 \n",
      "\n",
      "train loss per epoch =>  112 0.015608938816396514 train acc per epoch=>  0.9948049872122762\n",
      "val loss per epoch =>  1.3261207975918734 val acc per epoch => 0.8117088607594937 \n",
      "\n",
      "train loss per epoch =>  113 0.015562625137715103 train acc per epoch=>  0.9950607417489562\n",
      "val loss per epoch =>  1.3270456232602084 val acc per epoch => 0.8078520569620253 \n",
      "\n",
      "train loss per epoch =>  114 0.016639810566858287 train acc per epoch=>  0.9942535166545292\n",
      "val loss per epoch =>  1.333102190796333 val acc per epoch => 0.8133900316455697 \n",
      "\n",
      "train loss per epoch =>  115 0.014997410603448787 train acc per epoch=>  0.994621163743841\n",
      "val loss per epoch =>  1.34983222318601 val acc per epoch => 0.8168512658227848 \n",
      "\n",
      "train loss per epoch =>  116 0.015231810395083036 train acc per epoch=>  0.9947450447570333\n",
      "val loss per epoch =>  1.335429432271402 val acc per epoch => 0.8151700949367089 \n",
      "\n",
      "train loss per epoch =>  117 0.012901174566706123 train acc per epoch=>  0.995724104859335\n",
      "val loss per epoch =>  1.3503197484378573 val acc per epoch => 0.811807753164557 \n",
      "\n",
      "train loss per epoch =>  118 0.014320952579682179 train acc per epoch=>  0.995164641943734\n",
      "val loss per epoch =>  1.3379873739013188 val acc per epoch => 0.8135878164556962 \n",
      "\n",
      "train loss per epoch =>  119 0.01118513450170617 train acc per epoch=>  0.9960637787723785\n",
      "val loss per epoch =>  1.340171884132337 val acc per epoch => 0.815565664556962 \n",
      "\n",
      "train loss per epoch =>  120 0.011908995067623928 train acc per epoch=>  0.9958839514066496\n",
      "val loss per epoch =>  1.3584901992278764 val acc per epoch => 0.8146756329113924 \n",
      "\n",
      "train loss per epoch =>  121 0.010663957596245005 train acc per epoch=>  0.9965233375959079\n",
      "val loss per epoch =>  1.399767513516583 val acc per epoch => 0.8138844936708861 \n",
      "\n",
      "train loss per epoch =>  122 0.011627149755370986 train acc per epoch=>  0.9960637787723785\n",
      "val loss per epoch =>  1.3516697574265395 val acc per epoch => 0.8154667721518988 \n",
      "\n",
      "train loss per epoch =>  123 0.011505769693579696 train acc per epoch=>  0.9961437020460358\n",
      "val loss per epoch =>  1.3640492762191385 val acc per epoch => 0.8192246835443038 \n",
      "\n",
      "train loss per epoch =>  124 0.010327204914420874 train acc per epoch=>  0.9963035485933504\n",
      "val loss per epoch =>  1.3853913470159602 val acc per epoch => 0.8162579113924051 \n",
      "\n",
      "train loss per epoch =>  125 0.009476347454249516 train acc per epoch=>  0.9969109655036341\n",
      "val loss per epoch =>  1.4165485003326512 val acc per epoch => 0.8161590189873418 \n",
      "\n",
      "train loss per epoch =>  126 0.00949518176459182 train acc per epoch=>  0.9970628196930946\n",
      "val loss per epoch =>  1.4148714316042164 val acc per epoch => 0.8161590189873418 \n",
      "\n",
      "train loss per epoch =>  127 0.008989786560176169 train acc per epoch=>  0.9969309463220484\n",
      "val loss per epoch =>  1.4435376728637308 val acc per epoch => 0.8153678797468354 \n",
      "\n",
      "train loss per epoch =>  128 0.009094280669304287 train acc per epoch=>  0.996843030690537\n",
      "val loss per epoch =>  1.4068392832067949 val acc per epoch => 0.8161590189873418 \n",
      "\n",
      "train loss per epoch =>  129 0.008178247624504751 train acc per epoch=>  0.997090792869363\n",
      "val loss per epoch =>  1.4139904334575315 val acc per epoch => 0.8174446202531646 \n",
      "\n",
      "train loss per epoch =>  130 0.008267666936641835 train acc per epoch=>  0.9971627237851662\n",
      "val loss per epoch =>  1.4626894646053072 val acc per epoch => 0.8146756329113924 \n",
      "\n",
      "train loss per epoch =>  131 0.0075482218784777046 train acc per epoch=>  0.9972226662404092\n",
      "val loss per epoch =>  1.4595684507225133 val acc per epoch => 0.8151700949367089 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:46\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 47\u001b[0m y_pred \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m     49\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y)\n\u001b[1;32m     51\u001b[0m acc \u001b[39m=\u001b[39m calculate_accuracy(y_pred, y)\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 76\u001b[0m, in \u001b[0;36mResNetF.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblock6(x)\n\u001b[1;32m     75\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblock7(x)\n\u001b[0;32m---> 76\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresblock8(x)\n\u001b[1;32m     77\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblock9(x)\n\u001b[1;32m     78\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresblock10(x)\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 28\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 28\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(x)\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip:\n\u001b[1;32m     30\u001b[0m         out \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_bn(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_conv(x))\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/software/anaconda3/envs/cuda/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "    #  transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../../data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=200\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./og_resnet.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
