{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "           ResNet-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "             ReLU-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "           ResNet-16           [-1, 64, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "           ResNet-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "             ReLU-25          [-1, 128, 28, 28]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "           Conv2d-28          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "           ResNet-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "           Conv2d-34          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-35          [-1, 128, 28, 28]             256\n",
      "           ResNet-36          [-1, 128, 28, 28]               0\n",
      "           Conv2d-37          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
      "             ReLU-39          [-1, 128, 28, 28]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "           ResNet-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "           ResNet-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "           ResNet-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-56          [-1, 256, 14, 14]             512\n",
      "             ReLU-57          [-1, 256, 14, 14]               0\n",
      "           Conv2d-58          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-59          [-1, 256, 14, 14]             512\n",
      "           Conv2d-60          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-61          [-1, 256, 14, 14]             512\n",
      "           ResNet-62          [-1, 256, 14, 14]               0\n",
      "           Conv2d-63          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-64          [-1, 256, 14, 14]             512\n",
      "             ReLU-65          [-1, 256, 14, 14]               0\n",
      "           Conv2d-66          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-67          [-1, 256, 14, 14]             512\n",
      "           ResNet-68          [-1, 256, 14, 14]               0\n",
      "           Conv2d-69          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-70          [-1, 256, 14, 14]             512\n",
      "             ReLU-71          [-1, 256, 14, 14]               0\n",
      "           Conv2d-72          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-73          [-1, 256, 14, 14]             512\n",
      "           ResNet-74          [-1, 256, 14, 14]               0\n",
      "AdaptiveAvgPool2d-75            [-1, 256, 1, 1]               0\n",
      "          Flatten-76                  [-1, 256]               0\n",
      "           Linear-77                   [-1, 10]           2,570\n",
      "================================================================\n",
      "Total params: 4,926,282\n",
      "Trainable params: 4,926,282\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 79.63\n",
      "Params size (MB): 18.79\n",
      "Estimated Total Size (MB): 99.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.relu(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        # self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        # self.resblock2 = ResNet(32, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        self.resblock4=ResNet(64,64,stride=1)\n",
    "        self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,128,stride=2)\n",
    "        self.resblock7=ResNet(128,128,stride=1)\n",
    "        self.resblock8=ResNet(128,128,stride=1)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,256,stride=2)\n",
    "        self.resblock12=ResNet(256,256,stride=1)\n",
    "        self.resblock13=ResNet(256,256,stride=1)\n",
    "\n",
    "\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=256, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        # x = self.resblock1(x)\n",
    "        # x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        x = self.resblock4(x)\n",
    "        x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "      \n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 1.7296719160836067 train acc per epoch=>  0.3741368286292571\n",
      "val loss per epoch =>  1.4474624787704855 val acc per epoch => 0.4720134493670886 \n",
      "\n",
      "train loss per epoch =>  1 1.366224498699998 train acc per epoch=>  0.5011349104706894\n",
      "val loss per epoch =>  1.5532082379618777 val acc per epoch => 0.46439873417721517 \n",
      "\n",
      "train loss per epoch =>  2 1.2070742418698948 train acc per epoch=>  0.5647178708439897\n",
      "val loss per epoch =>  1.0206795817689052 val acc per epoch => 0.6347903481012658 \n",
      "\n",
      "train loss per epoch =>  3 1.094783064654416 train acc per epoch=>  0.6083200128791887\n",
      "val loss per epoch =>  1.193721808210204 val acc per epoch => 0.5914754746835443 \n",
      "\n",
      "train loss per epoch =>  4 1.0254176617278468 train acc per epoch=>  0.6387148337900791\n",
      "val loss per epoch =>  0.9763172351861302 val acc per epoch => 0.654568829113924 \n",
      "\n",
      "train loss per epoch =>  5 0.9585778313829466 train acc per epoch=>  0.6598185741383097\n",
      "val loss per epoch =>  0.9473308385172977 val acc per epoch => 0.665743670886076 \n",
      "\n",
      "train loss per epoch =>  6 0.9155089681410729 train acc per epoch=>  0.6782888427109974\n",
      "val loss per epoch =>  1.0343037587177903 val acc per epoch => 0.6606012658227848 \n",
      "\n",
      "train loss per epoch =>  7 0.8675958912085999 train acc per epoch=>  0.6955362852577054\n",
      "val loss per epoch =>  0.9773543824123431 val acc per epoch => 0.6818631329113924 \n",
      "\n",
      "train loss per epoch =>  8 0.8348315966403698 train acc per epoch=>  0.7095867967056801\n",
      "val loss per epoch =>  0.8685116767883301 val acc per epoch => 0.7071795886075949 \n",
      "\n",
      "train loss per epoch =>  9 0.8057841882681298 train acc per epoch=>  0.7183144182500327\n",
      "val loss per epoch =>  0.8378285954270182 val acc per epoch => 0.7096518987341772 \n",
      "\n",
      "train loss per epoch =>  10 0.779123456276896 train acc per epoch=>  0.7271379476313091\n",
      "val loss per epoch =>  0.7585692760310595 val acc per epoch => 0.7359572784810127 \n",
      "\n",
      "train loss per epoch =>  11 0.7449151125862775 train acc per epoch=>  0.7401374680611789\n",
      "val loss per epoch =>  0.7617639763445794 val acc per epoch => 0.7419897151898734 \n",
      "\n",
      "train loss per epoch =>  12 0.7248222000153778 train acc per epoch=>  0.7483735613505859\n",
      "val loss per epoch =>  0.7449982385092144 val acc per epoch => 0.7482199367088608 \n",
      "\n",
      "train loss per epoch =>  13 0.7087932747343312 train acc per epoch=>  0.7517703004810207\n",
      "val loss per epoch =>  0.7325058205972744 val acc per epoch => 0.7513844936708861 \n",
      "\n",
      "train loss per epoch =>  14 0.6897305365261215 train acc per epoch=>  0.7575127877237852\n",
      "val loss per epoch =>  0.8357595922071722 val acc per epoch => 0.725870253164557 \n",
      "\n",
      "train loss per epoch =>  15 0.6737704629178547 train acc per epoch=>  0.7641464194373402\n",
      "val loss per epoch =>  0.7046412735045711 val acc per epoch => 0.7637460443037974 \n",
      "\n",
      "train loss per epoch =>  16 0.6565955225616464 train acc per epoch=>  0.7711956522348896\n",
      "val loss per epoch =>  0.6834041962895212 val acc per epoch => 0.7724485759493671 \n",
      "\n",
      "train loss per epoch =>  17 0.6392594386092232 train acc per epoch=>  0.7760429987517159\n",
      "val loss per epoch =>  0.7145954464809804 val acc per epoch => 0.7551424050632911 \n",
      "\n",
      "train loss per epoch =>  18 0.6261646268160447 train acc per epoch=>  0.7806585678054244\n",
      "val loss per epoch =>  0.6739546041699904 val acc per epoch => 0.7732397151898734 \n",
      "\n",
      "train loss per epoch =>  19 0.6123719101061906 train acc per epoch=>  0.7849544437645036\n",
      "val loss per epoch =>  0.6759195433387274 val acc per epoch => 0.7764042721518988 \n",
      "\n",
      "train loss per epoch =>  20 0.6010927187512293 train acc per epoch=>  0.7908367967361685\n",
      "val loss per epoch =>  0.6149030569233472 val acc per epoch => 0.7924248417721519 \n",
      "\n",
      "train loss per epoch =>  21 0.5920677998334246 train acc per epoch=>  0.7910845589149943\n",
      "val loss per epoch =>  0.6177953545805774 val acc per epoch => 0.7927215189873418 \n",
      "\n",
      "train loss per epoch =>  22 0.5760272171948572 train acc per epoch=>  0.7977781330837923\n",
      "val loss per epoch =>  0.6278202031986623 val acc per epoch => 0.7865901898734177 \n",
      "\n",
      "train loss per epoch =>  23 0.56250555169247 train acc per epoch=>  0.8039162404396955\n",
      "val loss per epoch =>  0.6331077580210529 val acc per epoch => 0.7870846518987342 \n",
      "\n",
      "train loss per epoch =>  24 0.554346608764985 train acc per epoch=>  0.8052829284497234\n",
      "val loss per epoch =>  0.6373087578936468 val acc per epoch => 0.7922270569620253 \n",
      "\n",
      "train loss per epoch =>  25 0.5458523261425136 train acc per epoch=>  0.8080083121119253\n",
      "val loss per epoch =>  0.623307669464546 val acc per epoch => 0.7923259493670886 \n",
      "\n",
      "train loss per epoch =>  26 0.5374795226642238 train acc per epoch=>  0.8096267584034854\n",
      "val loss per epoch =>  0.5752517030208926 val acc per epoch => 0.8099287974683544 \n",
      "\n",
      "train loss per epoch =>  27 0.5278270605884855 train acc per epoch=>  0.8149536445622554\n",
      "val loss per epoch =>  0.6114760778750046 val acc per epoch => 0.7974683544303798 \n",
      "\n",
      "train loss per epoch =>  28 0.5209724461788412 train acc per epoch=>  0.8149616369201095\n",
      "val loss per epoch =>  0.586429061014441 val acc per epoch => 0.8083465189873418 \n",
      "\n",
      "train loss per epoch =>  29 0.5099065403651704 train acc per epoch=>  0.8213075448180098\n",
      "val loss per epoch =>  0.6194518295269978 val acc per epoch => 0.7991495253164557 \n",
      "\n",
      "train loss per epoch =>  30 0.5001619139595714 train acc per epoch=>  0.8237052430277285\n",
      "val loss per epoch =>  0.6226886009113698 val acc per epoch => 0.7932159810126582 \n",
      "\n",
      "train loss per epoch =>  31 0.4928836647201987 train acc per epoch=>  0.8244485294117647\n",
      "val loss per epoch =>  0.5301698329327982 val acc per epoch => 0.8245648734177216 \n",
      "\n",
      "train loss per epoch =>  32 0.4838158386899992 train acc per epoch=>  0.8296715153757569\n",
      "val loss per epoch =>  0.5750476997109908 val acc per epoch => 0.8116099683544303 \n",
      "\n",
      "train loss per epoch =>  33 0.47911809007530015 train acc per epoch=>  0.8313499041225599\n",
      "val loss per epoch =>  0.6792224577710598 val acc per epoch => 0.7839200949367089 \n",
      "\n",
      "train loss per epoch =>  34 0.47319392040562447 train acc per epoch=>  0.8330762468640457\n",
      "val loss per epoch =>  0.5609117384198345 val acc per epoch => 0.8165545886075949 \n",
      "\n",
      "train loss per epoch =>  35 0.46369254268953564 train acc per epoch=>  0.8361333120814369\n",
      "val loss per epoch =>  0.5342857275582567 val acc per epoch => 0.8221914556962026 \n",
      "\n",
      "train loss per epoch =>  36 0.4569179565095536 train acc per epoch=>  0.8381513747412835\n",
      "val loss per epoch =>  0.5260145535951928 val acc per epoch => 0.8286194620253164 \n",
      "\n",
      "train loss per epoch =>  37 0.44695757455228236 train acc per epoch=>  0.8409686701377029\n",
      "val loss per epoch =>  0.5318426757673674 val acc per epoch => 0.8305973101265823 \n",
      "\n",
      "train loss per epoch =>  38 0.4432037594678152 train acc per epoch=>  0.8439138428024624\n",
      "val loss per epoch =>  0.5446951528893241 val acc per epoch => 0.8238726265822784 \n",
      "\n",
      "train loss per epoch =>  39 0.4354690829734973 train acc per epoch=>  0.8459638747412835\n",
      "val loss per epoch =>  0.5927504003047943 val acc per epoch => 0.8109177215189873 \n",
      "\n",
      "train loss per epoch =>  40 0.4282987762213973 train acc per epoch=>  0.8473265664961637\n",
      "val loss per epoch =>  0.5579574772074253 val acc per epoch => 0.8195213607594937 \n",
      "\n",
      "train loss per epoch =>  41 0.4258276964240062 train acc per epoch=>  0.8503996164292631\n",
      "val loss per epoch =>  0.5694699347773685 val acc per epoch => 0.8185324367088608 \n",
      "\n",
      "train loss per epoch =>  42 0.4144337547328466 train acc per epoch=>  0.8539202365728901\n",
      "val loss per epoch =>  0.5199426567252678 val acc per epoch => 0.828125 \n",
      "\n",
      "train loss per epoch =>  43 0.40809412487327595 train acc per epoch=>  0.8542718989464938\n",
      "val loss per epoch =>  0.5435935413535637 val acc per epoch => 0.8273338607594937 \n",
      "\n",
      "train loss per epoch =>  44 0.4067185911376153 train acc per epoch=>  0.8548833121119253\n",
      "val loss per epoch =>  0.5396194691899456 val acc per epoch => 0.8260482594936709 \n",
      "\n",
      "train loss per epoch =>  45 0.39891728640669755 train acc per epoch=>  0.8571171675191815\n",
      "val loss per epoch =>  0.5125845710310755 val acc per epoch => 0.8346518987341772 \n",
      "\n",
      "train loss per epoch =>  46 0.3922327177603836 train acc per epoch=>  0.8595148657289002\n",
      "val loss per epoch =>  0.5401210935809945 val acc per epoch => 0.8245648734177216 \n",
      "\n",
      "train loss per epoch =>  47 0.3853607604570706 train acc per epoch=>  0.8643342392218997\n",
      "val loss per epoch =>  0.5321180677112145 val acc per epoch => 0.8362341772151899 \n",
      "\n",
      "train loss per epoch =>  48 0.38121240218277175 train acc per epoch=>  0.8640465154062451\n",
      "val loss per epoch =>  0.6025464300867878 val acc per epoch => 0.8122033227848101 \n",
      "\n",
      "train loss per epoch =>  49 0.37410521876933933 train acc per epoch=>  0.868002717452281\n",
      "val loss per epoch =>  0.5199362314954589 val acc per epoch => 0.8377175632911392 \n",
      "\n",
      "train loss per epoch =>  50 0.370050197748272 train acc per epoch=>  0.8693334399281866\n",
      "val loss per epoch =>  0.5676901261263256 val acc per epoch => 0.825751582278481 \n",
      "\n",
      "train loss per epoch =>  51 0.3659804345625441 train acc per epoch=>  0.8699248721227621\n",
      "val loss per epoch =>  0.5354321287779868 val acc per epoch => 0.8348496835443038 \n",
      "\n",
      "train loss per epoch =>  52 0.35693647013143504 train acc per epoch=>  0.8731897378821507\n",
      "val loss per epoch =>  0.5558857204793375 val acc per epoch => 0.8317840189873418 \n",
      "\n",
      "train loss per epoch =>  53 0.35712322101111305 train acc per epoch=>  0.8704963234989235\n",
      "val loss per epoch =>  0.5824558599085747 val acc per epoch => 0.8214992088607594 \n",
      "\n",
      "train loss per epoch =>  54 0.35279221649822373 train acc per epoch=>  0.8724704284192352\n",
      "val loss per epoch =>  0.5473274901697908 val acc per epoch => 0.834256329113924 \n",
      "\n",
      "train loss per epoch =>  55 0.3478892263015518 train acc per epoch=>  0.87561141301299\n",
      "val loss per epoch =>  0.5318152251877363 val acc per epoch => 0.8321795886075949 \n",
      "\n",
      "train loss per epoch =>  56 0.3382515267795309 train acc per epoch=>  0.8801470587930411\n",
      "val loss per epoch =>  0.5324725825575334 val acc per epoch => 0.8337618670886076 \n",
      "\n",
      "train loss per epoch =>  57 0.3348639887159743 train acc per epoch=>  0.879775415601023\n",
      "val loss per epoch =>  0.5873571920998489 val acc per epoch => 0.8231803797468354 \n",
      "\n",
      "train loss per epoch =>  58 0.3284296941040727 train acc per epoch=>  0.882304987181788\n",
      "val loss per epoch =>  0.5457537909851798 val acc per epoch => 0.8386075949367089 \n",
      "\n",
      "train loss per epoch =>  59 0.3209113060970745 train acc per epoch=>  0.8857376918463451\n",
      "val loss per epoch =>  0.6216163276871548 val acc per epoch => 0.8197191455696202 \n",
      "\n",
      "train loss per epoch =>  60 0.3215770690566134 train acc per epoch=>  0.8850303708439897\n",
      "val loss per epoch =>  0.5598258994802644 val acc per epoch => 0.8333662974683544 \n",
      "\n",
      "train loss per epoch =>  61 0.3134854312443062 train acc per epoch=>  0.8874999999695117\n",
      "val loss per epoch =>  0.5221074464954908 val acc per epoch => 0.8447389240506329 \n",
      "\n",
      "train loss per epoch =>  62 0.31732562325342234 train acc per epoch=>  0.8870564258616903\n",
      "val loss per epoch =>  0.5478173690506175 val acc per epoch => 0.8398931962025317 \n",
      "\n",
      "train loss per epoch =>  63 0.31168851763238686 train acc per epoch=>  0.888259271069256\n",
      "val loss per epoch =>  0.5374195026823237 val acc per epoch => 0.8393987341772152 \n",
      "\n",
      "train loss per epoch =>  64 0.29884839431404153 train acc per epoch=>  0.8929547633966217\n",
      "val loss per epoch =>  0.5512009187589718 val acc per epoch => 0.8381131329113924 \n",
      "\n",
      "train loss per epoch =>  65 0.2976522653761422 train acc per epoch=>  0.8925191816466543\n",
      "val loss per epoch =>  0.5381142598918721 val acc per epoch => 0.8423655063291139 \n",
      "\n",
      "train loss per epoch =>  66 0.30031843102344163 train acc per epoch=>  0.891795875928591\n",
      "val loss per epoch =>  0.5223164547093307 val acc per epoch => 0.8469145569620253 \n",
      "\n",
      "train loss per epoch =>  67 0.28857464217545126 train acc per epoch=>  0.8959838555901861\n",
      "val loss per epoch =>  0.5753232557562333 val acc per epoch => 0.8306962025316456 \n",
      "\n",
      "train loss per epoch =>  68 0.28736092809521024 train acc per epoch=>  0.8969309462610718\n",
      "val loss per epoch =>  0.5784871495222743 val acc per epoch => 0.833564082278481 \n",
      "\n",
      "train loss per epoch =>  69 0.27855715426185246 train acc per epoch=>  0.9006353900255755\n",
      "val loss per epoch =>  0.5474504964261115 val acc per epoch => 0.8401898734177216 \n",
      "\n",
      "train loss per epoch =>  70 0.2834395822661612 train acc per epoch=>  0.8997882032943199\n",
      "val loss per epoch =>  0.5673389768675913 val acc per epoch => 0.8402887658227848 \n",
      "\n",
      "train loss per epoch =>  71 0.27279293876322336 train acc per epoch=>  0.9017063618620949\n",
      "val loss per epoch =>  0.5670868716662443 val acc per epoch => 0.8372231012658228 \n",
      "\n",
      "train loss per epoch =>  72 0.2725955302757985 train acc per epoch=>  0.9018222506698745\n",
      "val loss per epoch =>  0.5551654477662678 val acc per epoch => 0.8389042721518988 \n",
      "\n",
      "train loss per epoch =>  73 0.264009757305655 train acc per epoch=>  0.9042119565217391\n",
      "val loss per epoch =>  0.5701836449435994 val acc per epoch => 0.8397943037974683 \n",
      "\n",
      "train loss per epoch =>  74 0.262662217268706 train acc per epoch=>  0.9065936701681913\n",
      "val loss per epoch =>  0.5432832109022744 val acc per epoch => 0.846123417721519 \n",
      "\n",
      "train loss per epoch =>  75 0.25634021891276243 train acc per epoch=>  0.9074168798258847\n",
      "val loss per epoch =>  0.5752865238280236 val acc per epoch => 0.8332674050632911 \n",
      "\n",
      "train loss per epoch =>  76 0.2574264376288485 train acc per epoch=>  0.9070412403787188\n",
      "val loss per epoch =>  0.5744244807128664 val acc per epoch => 0.8392009493670886 \n",
      "\n",
      "train loss per epoch =>  77 0.24831471571227168 train acc per epoch=>  0.9108855498721228\n",
      "val loss per epoch =>  0.5895894675315181 val acc per epoch => 0.8400909810126582 \n",
      "\n",
      "train loss per epoch =>  78 0.24681286868231986 train acc per epoch=>  0.911780690598061\n",
      "val loss per epoch =>  0.5662600484829915 val acc per epoch => 0.8391020569620253 \n",
      "\n",
      "train loss per epoch =>  79 0.24588310653748718 train acc per epoch=>  0.912280211058419\n",
      "val loss per epoch =>  0.5916976596735701 val acc per epoch => 0.8382120253164557 \n",
      "\n",
      "train loss per epoch =>  80 0.2412763779501781 train acc per epoch=>  0.9126638427109974\n",
      "val loss per epoch =>  0.5807390279030498 val acc per epoch => 0.8400909810126582 \n",
      "\n",
      "train loss per epoch =>  81 0.23482202241182937 train acc per epoch=>  0.9158328004810207\n",
      "val loss per epoch =>  0.5630027512206307 val acc per epoch => 0.8436511075949367 \n",
      "\n",
      "train loss per epoch =>  82 0.2274649008879881 train acc per epoch=>  0.9190617008587284\n",
      "val loss per epoch =>  0.5642499569096143 val acc per epoch => 0.8446400316455697 \n",
      "\n",
      "train loss per epoch =>  83 0.22995512587639988 train acc per epoch=>  0.9177789322250639\n",
      "val loss per epoch =>  0.6429089153114753 val acc per epoch => 0.8308939873417721 \n",
      "\n",
      "train loss per epoch =>  84 0.22729671807469004 train acc per epoch=>  0.9185262148642479\n",
      "val loss per epoch =>  0.5799420751348326 val acc per epoch => 0.8442444620253164 \n",
      "\n",
      "train loss per epoch =>  85 0.2202063793187861 train acc per epoch=>  0.9203444693399512\n",
      "val loss per epoch =>  0.5908549588315094 val acc per epoch => 0.843057753164557 \n",
      "\n",
      "train loss per epoch =>  86 0.21941867297339013 train acc per epoch=>  0.9207360933503836\n",
      "val loss per epoch =>  0.6213175409202334 val acc per epoch => 0.8377175632911392 \n",
      "\n",
      "train loss per epoch =>  87 0.21442677623704268 train acc per epoch=>  0.9229060101996908\n",
      "val loss per epoch =>  0.5631050837190845 val acc per epoch => 0.849189082278481 \n",
      "\n",
      "train loss per epoch =>  88 0.20953387075372973 train acc per epoch=>  0.9245804028437875\n",
      "val loss per epoch =>  0.6086314305474486 val acc per epoch => 0.8439477848101266 \n",
      "\n",
      "train loss per epoch =>  89 0.21457636626937504 train acc per epoch=>  0.9224544437340153\n",
      "val loss per epoch =>  0.5870706703089461 val acc per epoch => 0.8479034810126582 \n",
      "\n",
      "train loss per epoch =>  90 0.204122464198743 train acc per epoch=>  0.9259710677749361\n",
      "val loss per epoch =>  0.6019649283040928 val acc per epoch => 0.8479034810126582 \n",
      "\n",
      "train loss per epoch =>  91 0.2039383929556288 train acc per epoch=>  0.9274336637743293\n",
      "val loss per epoch =>  0.6202568826796133 val acc per epoch => 0.846123417721519 \n",
      "\n",
      "train loss per epoch =>  92 0.20158514957827375 train acc per epoch=>  0.926770300511509\n",
      "val loss per epoch =>  0.6078187168776235 val acc per epoch => 0.8493868670886076 \n",
      "\n",
      "train loss per epoch =>  93 0.19212477860014762 train acc per epoch=>  0.9297314578919764\n",
      "val loss per epoch =>  0.6223018131678617 val acc per epoch => 0.8410799050632911 \n",
      "\n",
      "train loss per epoch =>  94 0.19555020393313044 train acc per epoch=>  0.928644501339749\n",
      "val loss per epoch =>  0.5982277257533013 val acc per epoch => 0.8442444620253164 \n",
      "\n",
      "train loss per epoch =>  95 0.18748251419238118 train acc per epoch=>  0.9301990089209183\n",
      "val loss per epoch =>  0.6385373712717732 val acc per epoch => 0.8428599683544303 \n",
      "\n",
      "train loss per epoch =>  96 0.18706878535735333 train acc per epoch=>  0.9330282929303396\n",
      "val loss per epoch =>  0.6121026884905899 val acc per epoch => 0.844442246835443 \n",
      "\n",
      "train loss per epoch =>  97 0.1818345619741913 train acc per epoch=>  0.9348905051455778\n",
      "val loss per epoch =>  0.6164767017847375 val acc per epoch => 0.8462223101265823 \n",
      "\n",
      "train loss per epoch =>  98 0.18247785920377277 train acc per epoch=>  0.9354419757033248\n",
      "val loss per epoch =>  0.6080722812610336 val acc per epoch => 0.8482001582278481 \n",
      "\n",
      "train loss per epoch =>  99 0.17151368899113686 train acc per epoch=>  0.9392343350993398\n",
      "val loss per epoch =>  0.6455477266749249 val acc per epoch => 0.8463212025316456 \n",
      "\n",
      "train loss per epoch =>  100 0.17720667255656494 train acc per epoch=>  0.9373521419132457\n",
      "val loss per epoch =>  0.6480324898339525 val acc per epoch => 0.8457278481012658 \n",
      "\n",
      "train loss per epoch =>  101 0.17221472809648575 train acc per epoch=>  0.9389665921020995\n",
      "val loss per epoch =>  0.6280071046910708 val acc per epoch => 0.8429588607594937 \n",
      "\n",
      "train loss per epoch =>  102 0.16898502285599404 train acc per epoch=>  0.9390545077336109\n",
      "val loss per epoch =>  0.6373942339722114 val acc per epoch => 0.8474090189873418 \n",
      "\n",
      "train loss per epoch =>  103 0.1631114897234818 train acc per epoch=>  0.9416440217696187\n",
      "val loss per epoch =>  0.6521667770192593 val acc per epoch => 0.8492879746835443 \n",
      "\n",
      "train loss per epoch =>  104 0.165105557645602 train acc per epoch=>  0.9396579283887468\n",
      "val loss per epoch =>  0.6159042544380019 val acc per epoch => 0.8488924050632911 \n",
      "\n",
      "train loss per epoch =>  105 0.15833946225969384 train acc per epoch=>  0.9429867327060846\n",
      "val loss per epoch =>  0.6345947794144666 val acc per epoch => 0.8477056962025317 \n",
      "\n",
      "train loss per epoch =>  106 0.15509535397028984 train acc per epoch=>  0.9451166880710046\n",
      "val loss per epoch =>  0.7026783471243291 val acc per epoch => 0.8403876582278481 \n",
      "\n",
      "train loss per epoch =>  107 0.15614934438063055 train acc per epoch=>  0.9437739769820972\n",
      "val loss per epoch =>  0.6624136895318574 val acc per epoch => 0.8456289556962026 \n",
      "\n",
      "train loss per epoch =>  108 0.15558152071312262 train acc per epoch=>  0.9429587596822577\n",
      "val loss per epoch =>  0.6626230459424514 val acc per epoch => 0.8450356012658228 \n",
      "\n",
      "train loss per epoch =>  109 0.14705332518195557 train acc per epoch=>  0.9461756714469637\n",
      "val loss per epoch =>  0.6415121685477752 val acc per epoch => 0.8530458860759493 \n",
      "\n",
      "train loss per epoch =>  110 0.1465591782575373 train acc per epoch=>  0.9472826086651639\n",
      "val loss per epoch =>  0.6419407838507544 val acc per epoch => 0.8515625 \n",
      "\n",
      "train loss per epoch =>  111 0.1444572764818016 train acc per epoch=>  0.9481737532883959\n",
      "val loss per epoch =>  0.6539177302318283 val acc per epoch => 0.8497824367088608 \n",
      "\n",
      "train loss per epoch =>  112 0.13942243463700385 train acc per epoch=>  0.9502597506088979\n",
      "val loss per epoch =>  0.6772810658322105 val acc per epoch => 0.850870253164557 \n",
      "\n",
      "train loss per epoch =>  113 0.1388682028483552 train acc per epoch=>  0.9505874360613811\n",
      "val loss per epoch =>  0.6884082423735268 val acc per epoch => 0.8483979430379747 \n",
      "\n",
      "train loss per epoch =>  114 0.13581813146810398 train acc per epoch=>  0.9514825767873193\n",
      "val loss per epoch =>  0.6966414076240757 val acc per epoch => 0.8503757911392406 \n",
      "\n",
      "train loss per epoch =>  115 0.13760003017838043 train acc per epoch=>  0.9508591751613276\n",
      "val loss per epoch =>  0.6662958207764204 val acc per epoch => 0.8541337025316456 \n",
      "\n",
      "train loss per epoch =>  116 0.13095595918195632 train acc per epoch=>  0.9523217711607208\n",
      "val loss per epoch =>  0.6605236417130579 val acc per epoch => 0.8507713607594937 \n",
      "\n",
      "train loss per epoch =>  117 0.127066593826808 train acc per epoch=>  0.9545756073863915\n",
      "val loss per epoch =>  0.6840268267860895 val acc per epoch => 0.8516613924050633 \n",
      "\n",
      "train loss per epoch =>  118 0.12814145129355017 train acc per epoch=>  0.9541120524601558\n",
      "val loss per epoch =>  0.6760672040377991 val acc per epoch => 0.8514636075949367 \n",
      "\n",
      "train loss per epoch =>  119 0.12322605255505313 train acc per epoch=>  0.9555067136464521\n",
      "val loss per epoch =>  0.6951507567604885 val acc per epoch => 0.8512658227848101 \n",
      "\n",
      "train loss per epoch =>  120 0.1215385778537949 train acc per epoch=>  0.9561221227621484\n",
      "val loss per epoch =>  0.701258185920836 val acc per epoch => 0.8511669303797469 \n",
      "\n",
      "train loss per epoch =>  121 0.12269785688699358 train acc per epoch=>  0.9552789322860405\n",
      "val loss per epoch =>  0.6997724403686161 val acc per epoch => 0.8530458860759493 \n",
      "\n",
      "train loss per epoch =>  122 0.12015907111984994 train acc per epoch=>  0.9566336316830667\n",
      "val loss per epoch =>  0.7025536696744871 val acc per epoch => 0.8518591772151899 \n",
      "\n",
      "train loss per epoch =>  123 0.120613401698525 train acc per epoch=>  0.9568534206856242\n",
      "val loss per epoch =>  0.7131883524641206 val acc per epoch => 0.8499802215189873 \n",
      "\n",
      "train loss per epoch =>  124 0.11729821426046017 train acc per epoch=>  0.9571011828644501\n",
      "val loss per epoch =>  0.739796852763695 val acc per epoch => 0.846123417721519 \n",
      "\n",
      "train loss per epoch =>  125 0.11134728389170469 train acc per epoch=>  0.960334079344864\n",
      "val loss per epoch =>  0.7305361919010742 val acc per epoch => 0.8551226265822784 \n",
      "\n",
      "train loss per epoch =>  126 0.11138674439600361 train acc per epoch=>  0.9604579603580563\n",
      "val loss per epoch =>  0.718582720318927 val acc per epoch => 0.8520569620253164 \n",
      "\n",
      "train loss per epoch =>  127 0.10987156894429566 train acc per epoch=>  0.9609335038972937\n",
      "val loss per epoch =>  0.7254912789109387 val acc per epoch => 0.8556170886075949 \n",
      "\n",
      "train loss per epoch =>  128 0.10765015251000824 train acc per epoch=>  0.9616967710997443\n",
      "val loss per epoch =>  0.732661387965649 val acc per epoch => 0.8552215189873418 \n",
      "\n",
      "train loss per epoch =>  129 0.10731482215206642 train acc per epoch=>  0.9624040921020995\n",
      "val loss per epoch =>  0.7087839951243582 val acc per epoch => 0.8571993670886076 \n",
      "\n",
      "train loss per epoch =>  130 0.10067044741109661 train acc per epoch=>  0.9634590793753524\n",
      "val loss per epoch =>  0.7496291429181642 val acc per epoch => 0.8534414556962026 \n",
      "\n",
      "train loss per epoch =>  131 0.09849107948124713 train acc per epoch=>  0.96516943740113\n",
      "val loss per epoch =>  0.7422136211697059 val acc per epoch => 0.8537381329113924 \n",
      "\n",
      "train loss per epoch =>  132 0.09762194909898521 train acc per epoch=>  0.9657488811351455\n",
      "val loss per epoch =>  0.7634006542495534 val acc per epoch => 0.8562104430379747 \n",
      "\n",
      "train loss per epoch =>  133 0.09591377013460602 train acc per epoch=>  0.9659486893192887\n",
      "val loss per epoch =>  0.7375629110426842 val acc per epoch => 0.8564082278481012 \n",
      "\n",
      "train loss per epoch =>  134 0.09305531952215736 train acc per epoch=>  0.9665601023322786\n",
      "val loss per epoch =>  0.7615684463253504 val acc per epoch => 0.8553204113924051 \n",
      "\n",
      "train loss per epoch =>  135 0.09379803657989062 train acc per epoch=>  0.9674152814213882\n",
      "val loss per epoch =>  0.7595784758465199 val acc per epoch => 0.8527492088607594 \n",
      "\n",
      "train loss per epoch =>  136 0.09482739061650718 train acc per epoch=>  0.9665601023322786\n",
      "val loss per epoch =>  0.7409891742694227 val acc per epoch => 0.8550237341772152 \n",
      "\n",
      "train loss per epoch =>  137 0.09013880507263076 train acc per epoch=>  0.9674912084398977\n",
      "val loss per epoch =>  0.7770293400257449 val acc per epoch => 0.8549248417721519 \n",
      "\n",
      "train loss per epoch =>  138 0.09132318855608668 train acc per epoch=>  0.9681146100658895\n",
      "val loss per epoch =>  0.7443797343139407 val acc per epoch => 0.8553204113924051 \n",
      "\n",
      "train loss per epoch =>  139 0.08665943157185069 train acc per epoch=>  0.9691735934418486\n",
      "val loss per epoch =>  0.7805877541062198 val acc per epoch => 0.8525514240506329 \n",
      "\n",
      "train loss per epoch =>  140 0.0852176112163326 train acc per epoch=>  0.9698649297284958\n",
      "val loss per epoch =>  0.7551244353946251 val acc per epoch => 0.8549248417721519 \n",
      "\n",
      "train loss per epoch =>  141 0.08101607815545919 train acc per epoch=>  0.9711556905675727\n",
      "val loss per epoch =>  0.7654995386359058 val acc per epoch => 0.8566060126582279 \n",
      "\n",
      "train loss per epoch =>  142 0.08357666072237979 train acc per epoch=>  0.9706122123676798\n",
      "val loss per epoch =>  0.7841927748692187 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  143 0.08178193900314971 train acc per epoch=>  0.9705362851967287\n",
      "val loss per epoch =>  0.8041942813728429 val acc per epoch => 0.8529469936708861 \n",
      "\n",
      "train loss per epoch =>  144 0.07931249259073106 train acc per epoch=>  0.971423433564813\n",
      "val loss per epoch =>  0.7891549420884892 val acc per epoch => 0.8540348101265823 \n",
      "\n",
      "train loss per epoch =>  145 0.07873772017067046 train acc per epoch=>  0.9722386508646523\n",
      "val loss per epoch =>  0.7900245521641984 val acc per epoch => 0.8525514240506329 \n",
      "\n",
      "train loss per epoch =>  146 0.07639386934344955 train acc per epoch=>  0.9728580562354964\n",
      "val loss per epoch =>  0.808513577791709 val acc per epoch => 0.8515625 \n",
      "\n",
      "train loss per epoch =>  147 0.06965920929332524 train acc per epoch=>  0.9758511828949384\n",
      "val loss per epoch =>  0.8348833049022699 val acc per epoch => 0.8587816455696202 \n",
      "\n",
      "train loss per epoch =>  148 0.07262204837558976 train acc per epoch=>  0.9740049552429667\n",
      "val loss per epoch =>  0.7956060161696205 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  149 0.07240406467157709 train acc per epoch=>  0.9746683185057872\n",
      "val loss per epoch =>  0.823838571958904 val acc per epoch => 0.857001582278481 \n",
      "\n",
      "train loss per epoch =>  150 0.07143558282881518 train acc per epoch=>  0.9750519501583655\n",
      "val loss per epoch =>  0.8221995366525047 val acc per epoch => 0.8553204113924051 \n",
      "\n",
      "train loss per epoch =>  151 0.07058122046434742 train acc per epoch=>  0.9752477622398025\n",
      "val loss per epoch =>  0.8134296923121319 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  152 0.06803273988406525 train acc per epoch=>  0.9758791560712068\n",
      "val loss per epoch =>  0.8307730597031268 val acc per epoch => 0.8521558544303798 \n",
      "\n",
      "train loss per epoch =>  153 0.06753172890504684 train acc per epoch=>  0.9758631713554987\n",
      "val loss per epoch =>  0.8133613082427013 val acc per epoch => 0.8584849683544303 \n",
      "\n",
      "train loss per epoch =>  154 0.06273677400396684 train acc per epoch=>  0.978496643283483\n",
      "val loss per epoch =>  0.8327945629252663 val acc per epoch => 0.8553204113924051 \n",
      "\n",
      "train loss per epoch =>  155 0.06561669783638147 train acc per epoch=>  0.976886189349777\n",
      "val loss per epoch =>  0.8580024736591533 val acc per epoch => 0.8534414556962026 \n",
      "\n",
      "train loss per epoch =>  156 0.061014268344835094 train acc per epoch=>  0.9776174872732528\n",
      "val loss per epoch =>  0.8504064184955403 val acc per epoch => 0.8531447784810127 \n",
      "\n",
      "train loss per epoch =>  157 0.062409679569742256 train acc per epoch=>  0.9779291880100279\n",
      "val loss per epoch =>  0.854195557063139 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  158 0.06274442968990111 train acc per epoch=>  0.9769980819024089\n",
      "val loss per epoch =>  0.8628745090357864 val acc per epoch => 0.8577927215189873 \n",
      "\n",
      "train loss per epoch =>  159 0.05848176424127177 train acc per epoch=>  0.9795955882962707\n",
      "val loss per epoch =>  0.8588017587420307 val acc per epoch => 0.8578916139240507 \n",
      "\n",
      "train loss per epoch =>  160 0.06037324494051049 train acc per epoch=>  0.978928228930744\n",
      "val loss per epoch =>  0.8558508648148065 val acc per epoch => 0.8554193037974683 \n",
      "\n",
      "train loss per epoch =>  161 0.057932516721927604 train acc per epoch=>  0.9794757033857848\n",
      "val loss per epoch =>  0.8735707937916622 val acc per epoch => 0.8562104430379747 \n",
      "\n",
      "train loss per epoch =>  162 0.057030920344202415 train acc per epoch=>  0.9798433504750966\n",
      "val loss per epoch =>  0.8694348857749866 val acc per epoch => 0.8560126582278481 \n",
      "\n",
      "train loss per epoch =>  163 0.05560015982536175 train acc per epoch=>  0.981218030690537\n",
      "val loss per epoch =>  0.8533078186874148 val acc per epoch => 0.8569026898734177 \n",
      "\n",
      "train loss per epoch =>  164 0.05630817929225619 train acc per epoch=>  0.9803788363171355\n",
      "val loss per epoch =>  0.8936870697932907 val acc per epoch => 0.8521558544303798 \n",
      "\n",
      "train loss per epoch =>  165 0.05496700531076592 train acc per epoch=>  0.9808024297589841\n",
      "val loss per epoch =>  0.8867218528367296 val acc per epoch => 0.8552215189873418 \n",
      "\n",
      "train loss per epoch =>  166 0.053483983637560205 train acc per epoch=>  0.9807464834064474\n",
      "val loss per epoch =>  0.8813090069761759 val acc per epoch => 0.8542325949367089 \n",
      "\n",
      "train loss per epoch =>  167 0.05536196967515418 train acc per epoch=>  0.9810861573194909\n",
      "val loss per epoch =>  0.8715460089188588 val acc per epoch => 0.8555181962025317 \n",
      "\n",
      "train loss per epoch =>  168 0.05315765569967878 train acc per epoch=>  0.9806905370539106\n",
      "val loss per epoch =>  0.8855273908452143 val acc per epoch => 0.8564082278481012 \n",
      "\n",
      "train loss per epoch =>  169 0.054208255815974736 train acc per epoch=>  0.9806825448484982\n",
      "val loss per epoch =>  0.8862475277502325 val acc per epoch => 0.8560126582278481 \n",
      "\n",
      "train loss per epoch =>  170 0.053080814297470594 train acc per epoch=>  0.9813019502193422\n",
      "val loss per epoch =>  0.8851114209694199 val acc per epoch => 0.8557159810126582 \n",
      "\n",
      "train loss per epoch =>  171 0.05299968151208918 train acc per epoch=>  0.9813658887772914\n",
      "val loss per epoch =>  0.8877849431732033 val acc per epoch => 0.854628164556962 \n",
      "\n",
      "train loss per epoch =>  172 0.05175097662803081 train acc per epoch=>  0.9818374360613811\n",
      "val loss per epoch =>  0.8952616336224954 val acc per epoch => 0.8557159810126582 \n",
      "\n",
      "train loss per epoch =>  173 0.04811901736008885 train acc per epoch=>  0.9831321931556057\n",
      "val loss per epoch =>  0.9086940141418313 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  174 0.048715014809794975 train acc per epoch=>  0.9831122123371915\n",
      "val loss per epoch =>  0.9150720872456515 val acc per epoch => 0.8581882911392406 \n",
      "\n",
      "train loss per epoch =>  175 0.048945008604394276 train acc per epoch=>  0.9829243926136085\n",
      "val loss per epoch =>  0.897964817059191 val acc per epoch => 0.8565071202531646 \n",
      "\n",
      "train loss per epoch =>  176 0.049106981287307824 train acc per epoch=>  0.9827485613505859\n",
      "val loss per epoch =>  0.900524341230151 val acc per epoch => 0.8560126582278481 \n",
      "\n",
      "train loss per epoch =>  177 0.049414206021691644 train acc per epoch=>  0.9825447570637363\n",
      "val loss per epoch =>  0.9077467431750479 val acc per epoch => 0.857001582278481 \n",
      "\n",
      "train loss per epoch =>  178 0.047336514134083865 train acc per epoch=>  0.9836956521739131\n",
      "val loss per epoch =>  0.9106533776355695 val acc per epoch => 0.8566060126582279 \n",
      "\n",
      "train loss per epoch =>  179 0.04607429339364647 train acc per epoch=>  0.9842311381683935\n",
      "val loss per epoch =>  0.9130788701999036 val acc per epoch => 0.8562104430379747 \n",
      "\n",
      "train loss per epoch =>  180 0.04799832092941074 train acc per epoch=>  0.983324008981895\n",
      "val loss per epoch =>  0.8995730250696593 val acc per epoch => 0.8566060126582279 \n",
      "\n",
      "train loss per epoch =>  181 0.046618447448496164 train acc per epoch=>  0.984307065186903\n",
      "val loss per epoch =>  0.9102199137965336 val acc per epoch => 0.8559137658227848 \n",
      "\n",
      "train loss per epoch =>  182 0.048900315538048744 train acc per epoch=>  0.98227301796379\n",
      "val loss per epoch =>  0.9067509559136403 val acc per epoch => 0.8579905063291139 \n",
      "\n",
      "train loss per epoch =>  183 0.04578577571779566 train acc per epoch=>  0.9839833759895676\n",
      "val loss per epoch =>  0.9071416507793378 val acc per epoch => 0.8571993670886076 \n",
      "\n",
      "train loss per epoch =>  184 0.04483449754431425 train acc per epoch=>  0.9847426470893118\n",
      "val loss per epoch =>  0.9124647567543802 val acc per epoch => 0.8571993670886076 \n",
      "\n",
      "train loss per epoch =>  185 0.045362054235766384 train acc per epoch=>  0.9843230499026111\n",
      "val loss per epoch =>  0.9105715310271782 val acc per epoch => 0.8579905063291139 \n",
      "\n",
      "train loss per epoch =>  186 0.04407849847255727 train acc per epoch=>  0.9844029731762683\n",
      "val loss per epoch =>  0.9177246474767033 val acc per epoch => 0.8568037974683544 \n",
      "\n",
      "train loss per epoch =>  187 0.04512067960903925 train acc per epoch=>  0.9839554028132992\n",
      "val loss per epoch =>  0.9209187909772124 val acc per epoch => 0.8579905063291139 \n",
      "\n",
      "train loss per epoch =>  188 0.0441024387255311 train acc per epoch=>  0.9841911765315648\n",
      "val loss per epoch =>  0.918334121191049 val acc per epoch => 0.8558148734177216 \n",
      "\n",
      "train loss per epoch =>  189 0.044586006281635414 train acc per epoch=>  0.9846227621788259\n",
      "val loss per epoch =>  0.9172737387162221 val acc per epoch => 0.8566060126582279 \n",
      "\n",
      "train loss per epoch =>  190 0.045510214808828116 train acc per epoch=>  0.9843829923578541\n",
      "val loss per epoch =>  0.9159762025633945 val acc per epoch => 0.8582871835443038 \n",
      "\n",
      "train loss per epoch =>  191 0.04371166930895994 train acc per epoch=>  0.9851422634575983\n",
      "val loss per epoch =>  0.9185078770299501 val acc per epoch => 0.8571993670886076 \n",
      "\n",
      "train loss per epoch =>  192 0.0430847865301887 train acc per epoch=>  0.9845148657289002\n",
      "val loss per epoch =>  0.9213888630082335 val acc per epoch => 0.8571993670886076 \n",
      "\n",
      "train loss per epoch =>  193 0.04227162869480412 train acc per epoch=>  0.9847346547314578\n",
      "val loss per epoch =>  0.9212036423290832 val acc per epoch => 0.8558148734177216 \n",
      "\n",
      "train loss per epoch =>  194 0.044947982730005706 train acc per epoch=>  0.9843230499026111\n",
      "val loss per epoch =>  0.9172692842121366 val acc per epoch => 0.8565071202531646 \n",
      "\n",
      "train loss per epoch =>  195 0.04364464603974234 train acc per epoch=>  0.9849304668128948\n",
      "val loss per epoch =>  0.9199815792373464 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  196 0.041307934478897114 train acc per epoch=>  0.9858535805626598\n",
      "val loss per epoch =>  0.9261365498923049 val acc per epoch => 0.8562104430379747 \n",
      "\n",
      "train loss per epoch =>  197 0.04302431857975586 train acc per epoch=>  0.9844629156315113\n",
      "val loss per epoch =>  0.9278062121777595 val acc per epoch => 0.8564082278481012 \n",
      "\n",
      "train loss per epoch =>  198 0.047040367743138536 train acc per epoch=>  0.9830003197845596\n",
      "val loss per epoch =>  0.9205926524687417 val acc per epoch => 0.8572982594936709 \n",
      "\n",
      "train loss per epoch =>  199 0.04610304261350533 train acc per epoch=>  0.9832920397029203\n",
      "val loss per epoch =>  0.9162314138080501 val acc per epoch => 0.8580893987341772 \n",
      "\n",
      "CPU times: user 19min 43s, sys: 4min 23s, total: 24min 7s\n",
      "Wall time: 31min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../../data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../../data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=200\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./og_resnet.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8587816455696202"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3741368286292571,\n",
       " 0.5011349104706894,\n",
       " 0.5647178708439897,\n",
       " 0.6083200128791887,\n",
       " 0.6387148337900791,\n",
       " 0.6598185741383097,\n",
       " 0.6782888427109974,\n",
       " 0.6955362852577054,\n",
       " 0.7095867967056801,\n",
       " 0.7183144182500327,\n",
       " 0.7271379476313091,\n",
       " 0.7401374680611789,\n",
       " 0.7483735613505859,\n",
       " 0.7517703004810207,\n",
       " 0.7575127877237852,\n",
       " 0.7641464194373402,\n",
       " 0.7711956522348896,\n",
       " 0.7760429987517159,\n",
       " 0.7806585678054244,\n",
       " 0.7849544437645036,\n",
       " 0.7908367967361685,\n",
       " 0.7910845589149943,\n",
       " 0.7977781330837923,\n",
       " 0.8039162404396955,\n",
       " 0.8052829284497234,\n",
       " 0.8080083121119253,\n",
       " 0.8096267584034854,\n",
       " 0.8149536445622554,\n",
       " 0.8149616369201095,\n",
       " 0.8213075448180098,\n",
       " 0.8237052430277285,\n",
       " 0.8244485294117647,\n",
       " 0.8296715153757569,\n",
       " 0.8313499041225599,\n",
       " 0.8330762468640457,\n",
       " 0.8361333120814369,\n",
       " 0.8381513747412835,\n",
       " 0.8409686701377029,\n",
       " 0.8439138428024624,\n",
       " 0.8459638747412835,\n",
       " 0.8473265664961637,\n",
       " 0.8503996164292631,\n",
       " 0.8539202365728901,\n",
       " 0.8542718989464938,\n",
       " 0.8548833121119253,\n",
       " 0.8571171675191815,\n",
       " 0.8595148657289002,\n",
       " 0.8643342392218997,\n",
       " 0.8640465154062451,\n",
       " 0.868002717452281,\n",
       " 0.8693334399281866,\n",
       " 0.8699248721227621,\n",
       " 0.8731897378821507,\n",
       " 0.8704963234989235,\n",
       " 0.8724704284192352,\n",
       " 0.87561141301299,\n",
       " 0.8801470587930411,\n",
       " 0.879775415601023,\n",
       " 0.882304987181788,\n",
       " 0.8857376918463451,\n",
       " 0.8850303708439897,\n",
       " 0.8874999999695117,\n",
       " 0.8870564258616903,\n",
       " 0.888259271069256,\n",
       " 0.8929547633966217,\n",
       " 0.8925191816466543,\n",
       " 0.891795875928591,\n",
       " 0.8959838555901861,\n",
       " 0.8969309462610718,\n",
       " 0.9006353900255755,\n",
       " 0.8997882032943199,\n",
       " 0.9017063618620949,\n",
       " 0.9018222506698745,\n",
       " 0.9042119565217391,\n",
       " 0.9065936701681913,\n",
       " 0.9074168798258847,\n",
       " 0.9070412403787188,\n",
       " 0.9108855498721228,\n",
       " 0.911780690598061,\n",
       " 0.912280211058419,\n",
       " 0.9126638427109974,\n",
       " 0.9158328004810207,\n",
       " 0.9190617008587284,\n",
       " 0.9177789322250639,\n",
       " 0.9185262148642479,\n",
       " 0.9203444693399512,\n",
       " 0.9207360933503836,\n",
       " 0.9229060101996908,\n",
       " 0.9245804028437875,\n",
       " 0.9224544437340153,\n",
       " 0.9259710677749361,\n",
       " 0.9274336637743293,\n",
       " 0.926770300511509,\n",
       " 0.9297314578919764,\n",
       " 0.928644501339749,\n",
       " 0.9301990089209183,\n",
       " 0.9330282929303396,\n",
       " 0.9348905051455778,\n",
       " 0.9354419757033248,\n",
       " 0.9392343350993398,\n",
       " 0.9373521419132457,\n",
       " 0.9389665921020995,\n",
       " 0.9390545077336109,\n",
       " 0.9416440217696187,\n",
       " 0.9396579283887468,\n",
       " 0.9429867327060846,\n",
       " 0.9451166880710046,\n",
       " 0.9437739769820972,\n",
       " 0.9429587596822577,\n",
       " 0.9461756714469637,\n",
       " 0.9472826086651639,\n",
       " 0.9481737532883959,\n",
       " 0.9502597506088979,\n",
       " 0.9505874360613811,\n",
       " 0.9514825767873193,\n",
       " 0.9508591751613276,\n",
       " 0.9523217711607208,\n",
       " 0.9545756073863915,\n",
       " 0.9541120524601558,\n",
       " 0.9555067136464521,\n",
       " 0.9561221227621484,\n",
       " 0.9552789322860405,\n",
       " 0.9566336316830667,\n",
       " 0.9568534206856242,\n",
       " 0.9571011828644501,\n",
       " 0.960334079344864,\n",
       " 0.9604579603580563,\n",
       " 0.9609335038972937,\n",
       " 0.9616967710997443,\n",
       " 0.9624040921020995,\n",
       " 0.9634590793753524,\n",
       " 0.96516943740113,\n",
       " 0.9657488811351455,\n",
       " 0.9659486893192887,\n",
       " 0.9665601023322786,\n",
       " 0.9674152814213882,\n",
       " 0.9665601023322786,\n",
       " 0.9674912084398977,\n",
       " 0.9681146100658895,\n",
       " 0.9691735934418486,\n",
       " 0.9698649297284958,\n",
       " 0.9711556905675727,\n",
       " 0.9706122123676798,\n",
       " 0.9705362851967287,\n",
       " 0.971423433564813,\n",
       " 0.9722386508646523,\n",
       " 0.9728580562354964,\n",
       " 0.9758511828949384,\n",
       " 0.9740049552429667,\n",
       " 0.9746683185057872,\n",
       " 0.9750519501583655,\n",
       " 0.9752477622398025,\n",
       " 0.9758791560712068,\n",
       " 0.9758631713554987,\n",
       " 0.978496643283483,\n",
       " 0.976886189349777,\n",
       " 0.9776174872732528,\n",
       " 0.9779291880100279,\n",
       " 0.9769980819024089,\n",
       " 0.9795955882962707,\n",
       " 0.978928228930744,\n",
       " 0.9794757033857848,\n",
       " 0.9798433504750966,\n",
       " 0.981218030690537,\n",
       " 0.9803788363171355,\n",
       " 0.9808024297589841,\n",
       " 0.9807464834064474,\n",
       " 0.9810861573194909,\n",
       " 0.9806905370539106,\n",
       " 0.9806825448484982,\n",
       " 0.9813019502193422,\n",
       " 0.9813658887772914,\n",
       " 0.9818374360613811,\n",
       " 0.9831321931556057,\n",
       " 0.9831122123371915,\n",
       " 0.9829243926136085,\n",
       " 0.9827485613505859,\n",
       " 0.9825447570637363,\n",
       " 0.9836956521739131,\n",
       " 0.9842311381683935,\n",
       " 0.983324008981895,\n",
       " 0.984307065186903,\n",
       " 0.98227301796379,\n",
       " 0.9839833759895676,\n",
       " 0.9847426470893118,\n",
       " 0.9843230499026111,\n",
       " 0.9844029731762683,\n",
       " 0.9839554028132992,\n",
       " 0.9841911765315648,\n",
       " 0.9846227621788259,\n",
       " 0.9843829923578541,\n",
       " 0.9851422634575983,\n",
       " 0.9845148657289002,\n",
       " 0.9847346547314578,\n",
       " 0.9843230499026111,\n",
       " 0.9849304668128948,\n",
       " 0.9858535805626598,\n",
       " 0.9844629156315113,\n",
       " 0.9830003197845596,\n",
       " 0.9832920397029203]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
