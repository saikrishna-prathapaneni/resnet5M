{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "         LeakyReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "           Dropout-7           [-1, 64, 56, 56]               0\n",
      "         LeakyReLU-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "           ResNet-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "          Dropout-14           [-1, 64, 56, 56]               0\n",
      "        LeakyReLU-15           [-1, 64, 56, 56]               0\n",
      "           Conv2d-16           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-17           [-1, 64, 56, 56]             128\n",
      "           ResNet-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-20           [-1, 64, 56, 56]             128\n",
      "          Dropout-21           [-1, 64, 56, 56]               0\n",
      "        LeakyReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-24           [-1, 64, 56, 56]             128\n",
      "           ResNet-25           [-1, 64, 56, 56]               0\n",
      "           Conv2d-26          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-27          [-1, 128, 28, 28]             256\n",
      "          Dropout-28          [-1, 128, 28, 28]               0\n",
      "        LeakyReLU-29          [-1, 128, 28, 28]               0\n",
      "           Conv2d-30          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-31          [-1, 128, 28, 28]             256\n",
      "           Conv2d-32          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-33          [-1, 128, 28, 28]             256\n",
      "           ResNet-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-36          [-1, 128, 28, 28]             256\n",
      "          Dropout-37          [-1, 128, 28, 28]               0\n",
      "        LeakyReLU-38          [-1, 128, 28, 28]               0\n",
      "           Conv2d-39          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "           ResNet-41          [-1, 128, 28, 28]               0\n",
      "           Conv2d-42          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "          Dropout-44          [-1, 128, 28, 28]               0\n",
      "        LeakyReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "           ResNet-48          [-1, 128, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "          Dropout-51          [-1, 128, 28, 28]               0\n",
      "        LeakyReLU-52          [-1, 128, 28, 28]               0\n",
      "           Conv2d-53          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "           ResNet-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "          Dropout-58          [-1, 128, 28, 28]               0\n",
      "        LeakyReLU-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "           ResNet-62          [-1, 128, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "          Dropout-65          [-1, 128, 28, 28]               0\n",
      "        LeakyReLU-66          [-1, 128, 28, 28]               0\n",
      "           Conv2d-67          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "           ResNet-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70          [-1, 512, 14, 14]         589,824\n",
      "      BatchNorm2d-71          [-1, 512, 14, 14]           1,024\n",
      "          Dropout-72          [-1, 512, 14, 14]               0\n",
      "        LeakyReLU-73          [-1, 512, 14, 14]               0\n",
      "           Conv2d-74          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-75          [-1, 512, 14, 14]           1,024\n",
      "           Conv2d-76          [-1, 512, 14, 14]          65,536\n",
      "      BatchNorm2d-77          [-1, 512, 14, 14]           1,024\n",
      "           ResNet-78          [-1, 512, 14, 14]               0\n",
      "AdaptiveAvgPool2d-79            [-1, 512, 1, 1]               0\n",
      "          Flatten-80                  [-1, 512]               0\n",
      "           Linear-81                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 4,961,610\n",
      "Trainable params: 4,961,610\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 92.65\n",
      "Params size (MB): 18.93\n",
      "Estimated Total Size (MB): 112.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, inchannels, outchannels, kernel_size=3, stride=1, skip=True):\n",
    "        super().__init__()\n",
    "        self.skip = skip\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(inchannels, outchannels, kernel_size=kernel_size, stride=stride, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Conv2d(outchannels, outchannels, kernel_size=kernel_size, padding=1,bias=False),\n",
    "            nn.BatchNorm2d(outchannels),\n",
    "           \n",
    "        )\n",
    "        if stride == 2 or inchannels != outchannels:\n",
    "            self.skip = False\n",
    "            self.skip_conv = nn.Conv2d(inchannels, outchannels, kernel_size=1, stride=stride,bias=False)\n",
    "            self.skip_bn = nn.BatchNorm2d(outchannels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if not self.skip:\n",
    "            out += self.skip_bn(self.skip_conv(x))\n",
    "        else:\n",
    "            out += x\n",
    "        out = F.leaky_relu(out.clone())\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNetF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7,stride=2, padding=3,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.leakyrelu = nn.LeakyReLU(inplace=True)\n",
    "        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "        #self.resblock1 = ResNet(32, 32,stride=1)\n",
    "        #self.resblock2 = ResNet(64, 64,stride=1)\n",
    "        self.resblock3 = ResNet(64, 64,stride=1)\n",
    "        #self.resblock4=ResNet(64,64,stride=1)\n",
    "        #self.resblock5=ResNet(64,64,stride=1)\n",
    "        self.resblock6=ResNet(64,64,stride=1)\n",
    "        self.resblock7=ResNet(64,64,stride=1)\n",
    "        self.resblock8=ResNet(64,128,stride=2)\n",
    "        self.resblock9=ResNet(128,128,stride=1)\n",
    "        self.resblock10=ResNet(128,128,stride=1)\n",
    "        self.resblock11=ResNet(128,128,stride=1)\n",
    "        self.resblock12=ResNet(128,128,stride=1)\n",
    "        self.resblock13=ResNet(128,128,stride=1)\n",
    "        self.resblock14 =ResNet(128,512,stride=2)\n",
    "       #self.resblock15 =ResNet(512,512,stride=1)\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "        self.flat=nn.Flatten()\n",
    "        self.fc1= nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leakyrelu(x.clone())\n",
    "        x = self.maxpool(x)\n",
    "        #x = self.resblock1(x)\n",
    "        #x = self.resblock2(x)\n",
    "        x = self.resblock3(x)\n",
    "        #x = self.resblock4(x)\n",
    "        #x = self.resblock5(x)\n",
    "        x = self.resblock6(x)\n",
    "        x = self.resblock7(x)\n",
    "        x = self.resblock8(x)\n",
    "        x = self.resblock9(x)\n",
    "        x = self.resblock10(x)\n",
    "        x = self.resblock11(x)\n",
    "        x = self.resblock12(x)\n",
    "        x = self.resblock13(x)\n",
    "        x= self.resblock14(x)\n",
    "        #x= self.resblock15(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.fc1(x) \n",
    "     \n",
    "        return x\n",
    "\n",
    "\n",
    "model = ResNetF()\n",
    "model=model.cuda()\n",
    "random_matrix = torch.rand(1, 3, 224, 224).cuda()\n",
    "print(model.forward(random_matrix).shape)\n",
    "summary(model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    image_min = image.min()\n",
    "    image_max = image.max()\n",
    "    image.clamp_(min = image_min, max = image_max)\n",
    "    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n",
    "    return image\n",
    "\n",
    "def initialize_parameters(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight.data, nonlinearity = 'relu')\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_normal_(m.weight.data, gain = nn.init.calculate_gain('relu'))\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "def calculate_accuracy(y_pred, y): # calcualting the accuracy of the model\n",
    "    top_pred = y_pred.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, device): # traing the model on with the images in iterator\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "   \n",
    "    model.train()\n",
    "   \n",
    "    for (x, y) in iterator:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        y_pred = model(x)\n",
    "        \n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        acc = calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    lr_scheduler.step()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            y_pred= model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "\n",
    "            acc = calculate_accuracy(y_pred, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train loss per epoch =>  0 1.6578762186762621 train acc per epoch=>  0.3909327046340689\n",
      "val loss per epoch =>  1.4585759654829773 val acc per epoch => 0.48091376582278483 \n",
      "\n",
      "train loss per epoch =>  1 1.3464124775908488 train acc per epoch=>  0.512775735324606\n",
      "val loss per epoch =>  1.1972261961502364 val acc per epoch => 0.567246835443038 \n",
      "\n",
      "train loss per epoch =>  2 1.1993675129797758 train acc per epoch=>  0.5687380116004164\n",
      "val loss per epoch =>  1.2905958445766303 val acc per epoch => 0.5528085443037974 \n",
      "\n",
      "train loss per epoch =>  3 1.1019858217910123 train acc per epoch=>  0.6080562659846548\n",
      "val loss per epoch =>  1.070941504798358 val acc per epoch => 0.633504746835443 \n",
      "\n",
      "train loss per epoch =>  4 1.0262047707882074 train acc per epoch=>  0.6332201087261404\n",
      "val loss per epoch =>  1.0602887023853351 val acc per epoch => 0.6332080696202531 \n",
      "\n",
      "train loss per epoch =>  5 0.971597165707737 train acc per epoch=>  0.65589434152369\n",
      "val loss per epoch =>  0.9101601016672352 val acc per epoch => 0.678006329113924 \n",
      "\n",
      "train loss per epoch =>  6 0.922728569306376 train acc per epoch=>  0.6736213235599001\n",
      "val loss per epoch =>  0.9218841594985768 val acc per epoch => 0.6811708860759493 \n",
      "\n",
      "train loss per epoch =>  7 0.8859397980868055 train acc per epoch=>  0.6892103580257777\n",
      "val loss per epoch =>  0.9049904527543466 val acc per epoch => 0.6882911392405063 \n",
      "\n",
      "train loss per epoch =>  8 0.8544410108910192 train acc per epoch=>  0.6978021099439362\n",
      "val loss per epoch =>  0.7401254022423225 val acc per epoch => 0.7448575949367089 \n",
      "\n",
      "train loss per epoch =>  9 0.8204145420847646 train acc per epoch=>  0.7119205563574496\n",
      "val loss per epoch =>  0.7849854359143897 val acc per epoch => 0.7317049050632911 \n",
      "\n",
      "train loss per epoch =>  10 0.7972433353628954 train acc per epoch=>  0.7204763427414858\n",
      "val loss per epoch =>  0.7545061794262898 val acc per epoch => 0.7408030063291139 \n",
      "\n",
      "train loss per epoch =>  11 0.7739541402558232 train acc per epoch=>  0.7286325127572355\n",
      "val loss per epoch =>  0.7774286036249958 val acc per epoch => 0.7318037974683544 \n",
      "\n",
      "train loss per epoch =>  12 0.755954445323066 train acc per epoch=>  0.7341592072525902\n",
      "val loss per epoch =>  0.760738578778279 val acc per epoch => 0.7407041139240507 \n",
      "\n",
      "train loss per epoch =>  13 0.7355613938682829 train acc per epoch=>  0.7410326087871171\n",
      "val loss per epoch =>  0.6813306951824623 val acc per epoch => 0.7671083860759493 \n",
      "\n",
      "train loss per epoch =>  14 0.7226870674306475 train acc per epoch=>  0.745360454025171\n",
      "val loss per epoch =>  0.6470912793014623 val acc per epoch => 0.7845134493670886 \n",
      "\n",
      "train loss per epoch =>  15 0.7036459835441521 train acc per epoch=>  0.7520780051150895\n",
      "val loss per epoch =>  0.6792231190808212 val acc per epoch => 0.7672072784810127 \n",
      "\n",
      "train loss per epoch =>  16 0.6856367782406185 train acc per epoch=>  0.7594149617282936\n",
      "val loss per epoch =>  0.655290146040011 val acc per epoch => 0.7815466772151899 \n",
      "\n",
      "train loss per epoch =>  17 0.6756564291846722 train acc per epoch=>  0.7617527174827693\n",
      "val loss per epoch =>  0.6249847242349311 val acc per epoch => 0.784315664556962 \n",
      "\n",
      "train loss per epoch =>  18 0.6626263264652408 train acc per epoch=>  0.7674592391914113\n",
      "val loss per epoch =>  0.6400030100647407 val acc per epoch => 0.7820411392405063 \n",
      "\n",
      "train loss per epoch =>  19 0.6473724285659888 train acc per epoch=>  0.7727981138107417\n",
      "val loss per epoch =>  0.6742351409755175 val acc per epoch => 0.7748219936708861 \n",
      "\n",
      "train loss per epoch =>  20 0.6390416653412382 train acc per epoch=>  0.77766943740113\n",
      "val loss per epoch =>  0.5864233963097198 val acc per epoch => 0.7986550632911392 \n",
      "\n",
      "train loss per epoch =>  21 0.6275043697155955 train acc per epoch=>  0.7794916879795396\n",
      "val loss per epoch =>  0.6134700575206853 val acc per epoch => 0.7915348101265823 \n",
      "\n",
      "train loss per epoch =>  22 0.618737378236278 train acc per epoch=>  0.7837276214833759\n",
      "val loss per epoch =>  0.6697027566312235 val acc per epoch => 0.7769976265822784 \n",
      "\n",
      "train loss per epoch =>  23 0.6047819930574169 train acc per epoch=>  0.7897378517233807\n",
      "val loss per epoch =>  0.5701931224593634 val acc per epoch => 0.8024129746835443 \n",
      "\n",
      "train loss per epoch =>  24 0.5956334542587894 train acc per epoch=>  0.7903053069968358\n",
      "val loss per epoch =>  0.5711788237094879 val acc per epoch => 0.8084454113924051 \n",
      "\n",
      "train loss per epoch =>  25 0.5875085847609488 train acc per epoch=>  0.7934103260564682\n",
      "val loss per epoch =>  0.6092233804962303 val acc per epoch => 0.7976661392405063 \n",
      "\n",
      "train loss per epoch =>  26 0.5815649412934433 train acc per epoch=>  0.7962755754475703\n",
      "val loss per epoch =>  0.5721524350250824 val acc per epoch => 0.8106210443037974 \n",
      "\n",
      "train loss per epoch =>  27 0.568373675770162 train acc per epoch=>  0.8010789642248617\n",
      "val loss per epoch =>  0.585579869113391 val acc per epoch => 0.806368670886076 \n",
      "\n",
      "train loss per epoch =>  28 0.5648437111883822 train acc per epoch=>  0.8019021738825551\n",
      "val loss per epoch =>  0.5661171863350687 val acc per epoch => 0.8097310126582279 \n",
      "\n",
      "train loss per epoch =>  29 0.5550309051485622 train acc per epoch=>  0.8042319374316184\n",
      "val loss per epoch =>  0.5909624375119994 val acc per epoch => 0.8084454113924051 \n",
      "\n",
      "train loss per epoch =>  30 0.5475264074247511 train acc per epoch=>  0.8076926151200023\n",
      "val loss per epoch =>  0.5677483564690698 val acc per epoch => 0.8123022151898734 \n",
      "\n",
      "train loss per epoch =>  31 0.5373955851473162 train acc per epoch=>  0.8102022058823529\n",
      "val loss per epoch =>  0.5610857455036308 val acc per epoch => 0.8177412974683544 \n",
      "\n",
      "train loss per epoch =>  32 0.5373564268774389 train acc per epoch=>  0.8099424552429667\n",
      "val loss per epoch =>  0.5649128922178775 val acc per epoch => 0.8093354430379747 \n",
      "\n",
      "train loss per epoch =>  33 0.5228591225946041 train acc per epoch=>  0.8163043478565752\n",
      "val loss per epoch =>  0.561717521352104 val acc per epoch => 0.8169501582278481 \n",
      "\n",
      "train loss per epoch =>  34 0.5195780984123649 train acc per epoch=>  0.8164761828644501\n",
      "val loss per epoch =>  0.5178495973725862 val acc per epoch => 0.8221914556962026 \n",
      "\n",
      "train loss per epoch =>  35 0.5157322601589096 train acc per epoch=>  0.8189578006029739\n",
      "val loss per epoch =>  0.548656860107108 val acc per epoch => 0.8167523734177216 \n",
      "\n",
      "train loss per epoch =>  36 0.5034647126636846 train acc per epoch=>  0.8222786125624576\n",
      "val loss per epoch =>  0.5699775860279421 val acc per epoch => 0.8145767405063291 \n",
      "\n",
      "train loss per epoch =>  37 0.4998203150908965 train acc per epoch=>  0.8236492966751918\n",
      "val loss per epoch =>  0.5303688958475862 val acc per epoch => 0.825059335443038 \n",
      "\n",
      "train loss per epoch =>  38 0.4928664620727529 train acc per epoch=>  0.8245324489405698\n",
      "val loss per epoch =>  0.5691619089132622 val acc per epoch => 0.8126977848101266 \n",
      "\n",
      "train loss per epoch =>  39 0.4903604093262607 train acc per epoch=>  0.8267103581477309\n",
      "val loss per epoch =>  0.5044761352901217 val acc per epoch => 0.8324762658227848 \n",
      "\n",
      "train loss per epoch =>  40 0.48397714311204604 train acc per epoch=>  0.8298713235599001\n",
      "val loss per epoch =>  0.5566445109964926 val acc per epoch => 0.8192246835443038 \n",
      "\n",
      "train loss per epoch =>  41 0.47754656308142424 train acc per epoch=>  0.8298313619230714\n",
      "val loss per epoch =>  0.5252187587792361 val acc per epoch => 0.8277294303797469 \n",
      "\n",
      "train loss per epoch =>  42 0.4699826142214753 train acc per epoch=>  0.8332121163377981\n",
      "val loss per epoch =>  0.565150031560584 val acc per epoch => 0.8213014240506329 \n",
      "\n",
      "train loss per epoch =>  43 0.46674100479201586 train acc per epoch=>  0.834638746803069\n",
      "val loss per epoch =>  0.5896543939656849 val acc per epoch => 0.8130933544303798 \n",
      "\n",
      "train loss per epoch =>  44 0.4595843106889359 train acc per epoch=>  0.8375959079283888\n",
      "val loss per epoch =>  0.5055514113812507 val acc per epoch => 0.8381131329113924 \n",
      "\n",
      "train loss per epoch =>  45 0.4506729631625173 train acc per epoch=>  0.8398337595907929\n",
      "val loss per epoch =>  0.5534110819991631 val acc per epoch => 0.8253560126582279 \n",
      "\n",
      "train loss per epoch =>  46 0.4529691780619609 train acc per epoch=>  0.8390784847461964\n",
      "val loss per epoch =>  0.5254814760594428 val acc per epoch => 0.8289161392405063 \n",
      "\n",
      "train loss per epoch =>  47 0.4455946432355115 train acc per epoch=>  0.8440177429972402\n",
      "val loss per epoch =>  0.5255291254460057 val acc per epoch => 0.8285205696202531 \n",
      "\n",
      "train loss per epoch =>  48 0.4383032610425559 train acc per epoch=>  0.8427869246134063\n",
      "val loss per epoch =>  0.5167503096634829 val acc per epoch => 0.8341574367088608 \n",
      "\n",
      "train loss per epoch =>  49 0.43665743129485096 train acc per epoch=>  0.8454203963889491\n",
      "val loss per epoch =>  0.5016431167155881 val acc per epoch => 0.836629746835443 \n",
      "\n",
      "train loss per epoch =>  50 0.43211505922210186 train acc per epoch=>  0.8461037404701838\n",
      "val loss per epoch =>  0.5266689318644849 val acc per epoch => 0.829806170886076 \n",
      "\n",
      "train loss per epoch =>  51 0.42944731934905966 train acc per epoch=>  0.8477661445012787\n",
      "val loss per epoch =>  0.49202559888362885 val acc per epoch => 0.8401898734177216 \n",
      "\n",
      "train loss per epoch =>  52 0.4236142192502766 train acc per epoch=>  0.8508391944343782\n",
      "val loss per epoch =>  0.48984383895427364 val acc per epoch => 0.8448378164556962 \n",
      "\n",
      "train loss per epoch =>  53 0.41319935523030704 train acc per epoch=>  0.8533407928388747\n",
      "val loss per epoch =>  0.5269537332314479 val acc per epoch => 0.8345530063291139 \n",
      "\n",
      "train loss per epoch =>  54 0.4144983273332991 train acc per epoch=>  0.8535885550177006\n",
      "val loss per epoch =>  0.5152529259271259 val acc per epoch => 0.8367286392405063 \n",
      "\n",
      "train loss per epoch =>  55 0.41006600807237503 train acc per epoch=>  0.8533767583729971\n",
      "val loss per epoch =>  0.49954938322682924 val acc per epoch => 0.8445411392405063 \n",
      "\n",
      "train loss per epoch =>  56 0.40382900868382904 train acc per epoch=>  0.8557904411459822\n",
      "val loss per epoch =>  0.5170708857005155 val acc per epoch => 0.8346518987341772 \n",
      "\n",
      "train loss per epoch =>  57 0.4011434869235739 train acc per epoch=>  0.8579044117951942\n",
      "val loss per epoch =>  0.5100453620470022 val acc per epoch => 0.8379153481012658 \n",
      "\n",
      "train loss per epoch =>  58 0.3975115858990213 train acc per epoch=>  0.85889146425535\n",
      "val loss per epoch =>  0.5420528280584118 val acc per epoch => 0.8300039556962026 \n",
      "\n",
      "train loss per epoch =>  59 0.3937178044139272 train acc per epoch=>  0.8603580563574496\n",
      "val loss per epoch =>  0.49420567686799205 val acc per epoch => 0.8417721518987342 \n",
      "\n",
      "train loss per epoch =>  60 0.38731778716034904 train acc per epoch=>  0.8610294118256825\n",
      "val loss per epoch =>  0.5220129412563541 val acc per epoch => 0.8408821202531646 \n",
      "\n",
      "train loss per epoch =>  61 0.3817617149304246 train acc per epoch=>  0.8627797314882888\n",
      "val loss per epoch =>  0.5050537601301942 val acc per epoch => 0.840684335443038 \n",
      "\n",
      "train loss per epoch =>  62 0.3824339760538867 train acc per epoch=>  0.8642862852272171\n",
      "val loss per epoch =>  0.4899914555157287 val acc per epoch => 0.8470134493670886 \n",
      "\n",
      "train loss per epoch =>  63 0.36835531856092957 train acc per epoch=>  0.8694892903727949\n",
      "val loss per epoch =>  0.5298964592474925 val acc per epoch => 0.8373219936708861 \n",
      "\n",
      "train loss per epoch =>  64 0.37446310495019264 train acc per epoch=>  0.8673633312630227\n",
      "val loss per epoch =>  0.5381091391738457 val acc per epoch => 0.8324762658227848 \n",
      "\n",
      "train loss per epoch =>  65 0.3649163435944511 train acc per epoch=>  0.8693534207466008\n",
      "val loss per epoch =>  0.5281744697425939 val acc per epoch => 0.8389042721518988 \n",
      "\n",
      "train loss per epoch =>  66 0.3605387778690709 train acc per epoch=>  0.8707720588540178\n",
      "val loss per epoch =>  0.520695798759219 val acc per epoch => 0.8412776898734177 \n",
      "\n",
      "train loss per epoch =>  67 0.3578769340920631 train acc per epoch=>  0.8727741367981562\n",
      "val loss per epoch =>  0.5035658486281769 val acc per epoch => 0.8438488924050633 \n",
      "\n",
      "train loss per epoch =>  68 0.35297166173110534 train acc per epoch=>  0.8722866049508\n",
      "val loss per epoch =>  0.5227096160001392 val acc per epoch => 0.843057753164557 \n",
      "\n",
      "train loss per epoch =>  69 0.34938047700525854 train acc per epoch=>  0.8768342391914113\n",
      "val loss per epoch =>  0.49975939642024947 val acc per epoch => 0.8446400316455697 \n",
      "\n",
      "train loss per epoch =>  70 0.34724251693471925 train acc per epoch=>  0.8761748721837388\n",
      "val loss per epoch =>  0.49403705381894414 val acc per epoch => 0.8490901898734177 \n",
      "\n",
      "train loss per epoch =>  71 0.3443373842617435 train acc per epoch=>  0.8766544118256825\n",
      "val loss per epoch =>  0.5042770344622528 val acc per epoch => 0.846815664556962 \n",
      "\n",
      "train loss per epoch =>  72 0.34183243477283537 train acc per epoch=>  0.8778093030385654\n",
      "val loss per epoch =>  0.5030631800241108 val acc per epoch => 0.8443433544303798 \n",
      "\n",
      "train loss per epoch =>  73 0.3399164074141046 train acc per epoch=>  0.8775335678359126\n",
      "val loss per epoch =>  0.5246397211581846 val acc per epoch => 0.8439477848101266 \n",
      "\n",
      "train loss per epoch =>  74 0.3390680769139239 train acc per epoch=>  0.8796435422299768\n",
      "val loss per epoch =>  0.5169674784322328 val acc per epoch => 0.8480023734177216 \n",
      "\n",
      "train loss per epoch =>  75 0.32936928617527417 train acc per epoch=>  0.8826526534526854\n",
      "val loss per epoch =>  0.503288940538334 val acc per epoch => 0.8487935126582279 \n",
      "\n",
      "train loss per epoch =>  76 0.3268141439732383 train acc per epoch=>  0.8812060421690002\n",
      "val loss per epoch =>  0.5179841574988787 val acc per epoch => 0.8455300632911392 \n",
      "\n",
      "train loss per epoch =>  77 0.32131402407918136 train acc per epoch=>  0.8849944054623089\n",
      "val loss per epoch =>  0.4952879431126993 val acc per epoch => 0.8528481012658228 \n",
      "\n",
      "train loss per epoch =>  78 0.31664815964296344 train acc per epoch=>  0.8855298913043478\n",
      "val loss per epoch =>  0.5217312336722507 val acc per epoch => 0.8482990506329114 \n",
      "\n",
      "train loss per epoch =>  79 0.3153459449177203 train acc per epoch=>  0.8866927750275263\n",
      "val loss per epoch =>  0.5130690754968908 val acc per epoch => 0.8489912974683544 \n",
      "\n",
      "train loss per epoch =>  80 0.3129046131354159 train acc per epoch=>  0.8883032289612324\n",
      "val loss per epoch =>  0.5154401065805291 val acc per epoch => 0.8466178797468354 \n",
      "\n",
      "train loss per epoch =>  81 0.3117689230405461 train acc per epoch=>  0.887104379856373\n",
      "val loss per epoch =>  0.5109726313171508 val acc per epoch => 0.8490901898734177 \n",
      "\n",
      "train loss per epoch =>  82 0.31182637841195404 train acc per epoch=>  0.8876878196930946\n",
      "val loss per epoch =>  0.5102231036635894 val acc per epoch => 0.8511669303797469 \n",
      "\n",
      "train loss per epoch =>  83 0.30251510635666223 train acc per epoch=>  0.8916799872732528\n",
      "val loss per epoch =>  0.5124870738651179 val acc per epoch => 0.8482001582278481 \n",
      "\n",
      "train loss per epoch =>  84 0.2975890404351837 train acc per epoch=>  0.8937500000304883\n",
      "val loss per epoch =>  0.4983263321315186 val acc per epoch => 0.8495846518987342 \n",
      "\n",
      "train loss per epoch =>  85 0.29411502845604404 train acc per epoch=>  0.8924512468335574\n",
      "val loss per epoch =>  0.5142671609226661 val acc per epoch => 0.849189082278481 \n",
      "\n",
      "train loss per epoch =>  86 0.2978125181420685 train acc per epoch=>  0.8935102302095165\n",
      "val loss per epoch =>  0.5219289188898062 val acc per epoch => 0.8481012658227848 \n",
      "\n",
      "train loss per epoch =>  87 0.2915769073816821 train acc per epoch=>  0.8965353261479332\n",
      "val loss per epoch =>  0.5533137298837493 val acc per epoch => 0.8394976265822784 \n",
      "\n",
      "train loss per epoch =>  88 0.28797902429805083 train acc per epoch=>  0.8952125958774401\n",
      "val loss per epoch =>  0.518099795036678 val acc per epoch => 0.8520569620253164 \n",
      "\n",
      "train loss per epoch =>  89 0.2828474267364463 train acc per epoch=>  0.8980578644501279\n",
      "val loss per epoch =>  0.5225242912014828 val acc per epoch => 0.8544303797468354 \n",
      "\n",
      "train loss per epoch =>  90 0.28087910666795035 train acc per epoch=>  0.8988491048288467\n",
      "val loss per epoch =>  0.5415416241069383 val acc per epoch => 0.8482990506329114 \n",
      "\n",
      "train loss per epoch =>  91 0.27717462860409864 train acc per epoch=>  0.9006273976677214\n",
      "val loss per epoch =>  0.5258877743271333 val acc per epoch => 0.8517602848101266 \n",
      "\n",
      "train loss per epoch =>  92 0.27526316820355634 train acc per epoch=>  0.8997802110889074\n",
      "val loss per epoch =>  0.5306978708581079 val acc per epoch => 0.8486946202531646 \n",
      "\n",
      "train loss per epoch =>  93 0.27363614147276527 train acc per epoch=>  0.9010909527463986\n",
      "val loss per epoch =>  0.5420022340892237 val acc per epoch => 0.8510680379746836 \n",
      "\n",
      "train loss per epoch =>  94 0.2710982051385028 train acc per epoch=>  0.9035366049508\n",
      "val loss per epoch =>  0.5248997833909868 val acc per epoch => 0.856309335443038 \n",
      "\n",
      "train loss per epoch =>  95 0.266853414494973 train acc per epoch=>  0.9030210997747339\n",
      "val loss per epoch =>  0.5289585099944586 val acc per epoch => 0.8548259493670886 \n",
      "\n",
      "train loss per epoch =>  96 0.27168849163957876 train acc per epoch=>  0.9026094949458872\n",
      "val loss per epoch =>  0.5000981171674366 val acc per epoch => 0.857001582278481 \n",
      "\n",
      "train loss per epoch =>  97 0.26201619002062954 train acc per epoch=>  0.9045476343320764\n",
      "val loss per epoch =>  0.5302402454463742 val acc per epoch => 0.8507713607594937 \n",
      "\n",
      "train loss per epoch =>  98 0.2602362541095985 train acc per epoch=>  0.9062859655341224\n",
      "val loss per epoch =>  0.5303547295588481 val acc per epoch => 0.8525514240506329 \n",
      "\n",
      "train loss per epoch =>  99 0.25111015626917715 train acc per epoch=>  0.90873561384123\n",
      "val loss per epoch =>  0.5436014144480983 val acc per epoch => 0.8509691455696202 \n",
      "\n",
      "train loss per epoch =>  100 0.2543392259332225 train acc per epoch=>  0.9077605498416345\n",
      "val loss per epoch =>  0.5366891967722133 val acc per epoch => 0.8523536392405063 \n",
      "\n",
      "train loss per epoch =>  101 0.2536132309747779 train acc per epoch=>  0.9073689258312021\n",
      "val loss per epoch =>  0.529721407950679 val acc per epoch => 0.8545292721518988 \n",
      "\n",
      "train loss per epoch =>  102 0.24910449737782978 train acc per epoch=>  0.9095228581172426\n",
      "val loss per epoch =>  0.5619466140677657 val acc per epoch => 0.8500791139240507 \n",
      "\n",
      "train loss per epoch =>  103 0.24626113062776872 train acc per epoch=>  0.9116408248691608\n",
      "val loss per epoch =>  0.5360792856427687 val acc per epoch => 0.857693829113924 \n",
      "\n",
      "train loss per epoch =>  104 0.24374775148337455 train acc per epoch=>  0.9126798274267055\n",
      "val loss per epoch =>  0.5221344221241867 val acc per epoch => 0.8568037974683544 \n",
      "\n",
      "train loss per epoch =>  105 0.23639838435613286 train acc per epoch=>  0.9143582161735085\n",
      "val loss per epoch =>  0.5410060735442971 val acc per epoch => 0.8583860759493671 \n",
      "\n",
      "train loss per epoch =>  106 0.23334410030137548 train acc per epoch=>  0.9139905690841967\n",
      "val loss per epoch =>  0.5407856566996514 val acc per epoch => 0.8578916139240507 \n",
      "\n",
      "train loss per epoch =>  107 0.23129928868521205 train acc per epoch=>  0.9163043479175519\n",
      "val loss per epoch =>  0.5309625034090839 val acc per epoch => 0.857001582278481 \n",
      "\n",
      "train loss per epoch =>  108 0.2289747465258974 train acc per epoch=>  0.9185461956826623\n",
      "val loss per epoch =>  0.5635485630246657 val acc per epoch => 0.853243670886076 \n",
      "\n",
      "train loss per epoch =>  109 0.2289966181720919 train acc per epoch=>  0.9181785485933504\n",
      "val loss per epoch =>  0.5369905895447429 val acc per epoch => 0.8550237341772152 \n",
      "\n",
      "train loss per epoch =>  110 0.22427034355185527 train acc per epoch=>  0.9193973785166241\n",
      "val loss per epoch =>  0.5423598770476594 val acc per epoch => 0.8590783227848101 \n",
      "\n",
      "train loss per epoch =>  111 0.2256963668233903 train acc per epoch=>  0.918090632961839\n",
      "val loss per epoch =>  0.5333757977696914 val acc per epoch => 0.8608583860759493 \n",
      "\n",
      "train loss per epoch =>  112 0.2246980543255501 train acc per epoch=>  0.9187579923273658\n",
      "val loss per epoch =>  0.5330110471459883 val acc per epoch => 0.8607594936708861 \n",
      "\n",
      "train loss per epoch =>  113 0.2195834603417865 train acc per epoch=>  0.9197690217086422\n",
      "val loss per epoch =>  0.5474232963368862 val acc per epoch => 0.8589794303797469 \n",
      "\n",
      "train loss per epoch =>  114 0.21593496001437498 train acc per epoch=>  0.9221347506393862\n",
      "val loss per epoch =>  0.5582530226888536 val acc per epoch => 0.8562104430379747 \n",
      "\n",
      "train loss per epoch =>  115 0.21248987780126463 train acc per epoch=>  0.9233216112836853\n",
      "val loss per epoch =>  0.5396396906315526 val acc per epoch => 0.861748417721519 \n",
      "\n",
      "train loss per epoch =>  116 0.21199057586586384 train acc per epoch=>  0.9229539641943734\n",
      "val loss per epoch =>  0.5454876520965672 val acc per epoch => 0.8599683544303798 \n",
      "\n",
      "train loss per epoch =>  117 0.2111202677345032 train acc per epoch=>  0.9254076087566288\n",
      "val loss per epoch =>  0.5689117342610902 val acc per epoch => 0.8551226265822784 \n",
      "\n",
      "train loss per epoch =>  118 0.20565161709209231 train acc per epoch=>  0.9264106457800512\n",
      "val loss per epoch =>  0.5411762840385679 val acc per epoch => 0.8606606012658228 \n",
      "\n",
      "train loss per epoch =>  119 0.2010557790241583 train acc per epoch=>  0.9274776215138643\n",
      "val loss per epoch =>  0.5482506653930568 val acc per epoch => 0.8598694620253164 \n",
      "\n",
      "train loss per epoch =>  120 0.2024380370707768 train acc per epoch=>  0.9270540282244573\n",
      "val loss per epoch =>  0.5410303873351857 val acc per epoch => 0.8598694620253164 \n",
      "\n",
      "train loss per epoch =>  121 0.20567719907025853 train acc per epoch=>  0.9254835357751383\n",
      "val loss per epoch =>  0.5432380317132685 val acc per epoch => 0.860067246835443 \n",
      "\n",
      "train loss per epoch =>  122 0.19968062993663047 train acc per epoch=>  0.928696451437138\n",
      "val loss per epoch =>  0.5557790343897252 val acc per epoch => 0.8577927215189873 \n",
      "\n",
      "train loss per epoch =>  123 0.18982950441748894 train acc per epoch=>  0.9317295397334087\n",
      "val loss per epoch =>  0.5484154739334614 val acc per epoch => 0.8609572784810127 \n",
      "\n",
      "train loss per epoch =>  124 0.19209362802755497 train acc per epoch=>  0.9303348785471124\n",
      "val loss per epoch =>  0.5508335875936702 val acc per epoch => 0.8638251582278481 \n",
      "\n",
      "train loss per epoch =>  125 0.18751804894574767 train acc per epoch=>  0.9319213555596978\n",
      "val loss per epoch =>  0.5714801972425436 val acc per epoch => 0.8595727848101266 \n",
      "\n",
      "train loss per epoch =>  126 0.1899030839314546 train acc per epoch=>  0.9321651214833759\n",
      "val loss per epoch =>  0.5512390140491196 val acc per epoch => 0.8616495253164557 \n",
      "\n",
      "train loss per epoch =>  127 0.19037379292042358 train acc per epoch=>  0.9322610294727414\n",
      "val loss per epoch =>  0.5501885063286069 val acc per epoch => 0.8651107594936709 \n",
      "\n",
      "train loss per epoch =>  128 0.18836460306363947 train acc per epoch=>  0.931250000091465\n",
      "val loss per epoch =>  0.5512907827579523 val acc per epoch => 0.8592761075949367 \n",
      "\n",
      "train loss per epoch =>  129 0.1834870426322493 train acc per epoch=>  0.9331801471198001\n",
      "val loss per epoch =>  0.5522323246998123 val acc per epoch => 0.8629351265822784 \n",
      "\n",
      "train loss per epoch =>  130 0.1861493073956436 train acc per epoch=>  0.933835517872325\n",
      "val loss per epoch =>  0.5681187676478036 val acc per epoch => 0.8582871835443038 \n",
      "\n",
      "train loss per epoch =>  131 0.18175475170735814 train acc per epoch=>  0.9348625319693095\n",
      "val loss per epoch =>  0.5633936522504951 val acc per epoch => 0.8581882911392406 \n",
      "\n",
      "train loss per epoch =>  132 0.18305534131996468 train acc per epoch=>  0.9338994565827158\n",
      "val loss per epoch =>  0.5715945304571828 val acc per epoch => 0.8587816455696202 \n",
      "\n",
      "train loss per epoch =>  133 0.17412588948293414 train acc per epoch=>  0.9376918158262891\n",
      "val loss per epoch =>  0.5570538932386833 val acc per epoch => 0.8622428797468354 \n",
      "\n",
      "train loss per epoch =>  134 0.1734825658428547 train acc per epoch=>  0.9367287404396955\n",
      "val loss per epoch =>  0.5666136811428433 val acc per epoch => 0.8643196202531646 \n",
      "\n",
      "train loss per epoch =>  135 0.17307109307602545 train acc per epoch=>  0.9375479539946827\n",
      "val loss per epoch =>  0.5622585427157486 val acc per epoch => 0.8630340189873418 \n",
      "\n",
      "train loss per epoch =>  136 0.17393384779543827 train acc per epoch=>  0.9368765985264498\n",
      "val loss per epoch =>  0.5640460414977013 val acc per epoch => 0.8629351265822784 \n",
      "\n",
      "train loss per epoch =>  137 0.16668004071925913 train acc per epoch=>  0.9400775255754475\n",
      "val loss per epoch =>  0.5690568862836572 val acc per epoch => 0.861056170886076 \n",
      "\n",
      "train loss per epoch =>  138 0.1655381661089485 train acc per epoch=>  0.939865728930744\n",
      "val loss per epoch =>  0.5775783563716502 val acc per epoch => 0.8599683544303798 \n",
      "\n",
      "train loss per epoch =>  139 0.1626572096077225 train acc per epoch=>  0.9419437340458335\n",
      "val loss per epoch =>  0.5733694164436075 val acc per epoch => 0.8606606012658228 \n",
      "\n",
      "train loss per epoch =>  140 0.16006447017535835 train acc per epoch=>  0.9416280370539106\n",
      "val loss per epoch =>  0.587384937496125 val acc per epoch => 0.861056170886076 \n",
      "\n",
      "train loss per epoch =>  141 0.16332926854605564 train acc per epoch=>  0.9412963554987213\n",
      "val loss per epoch =>  0.5865745069105414 val acc per epoch => 0.8606606012658228 \n",
      "\n",
      "train loss per epoch =>  142 0.1605902117060121 train acc per epoch=>  0.9423233695957057\n",
      "val loss per epoch =>  0.5750601065309742 val acc per epoch => 0.8633306962025317 \n",
      "\n",
      "train loss per epoch =>  143 0.1578090838573473 train acc per epoch=>  0.9432624680611789\n",
      "val loss per epoch =>  0.5813904412185089 val acc per epoch => 0.8612539556962026 \n",
      "\n",
      "train loss per epoch =>  144 0.1542025873785281 train acc per epoch=>  0.9447170717027181\n",
      "val loss per epoch =>  0.5862577289720124 val acc per epoch => 0.861748417721519 \n",
      "\n",
      "train loss per epoch =>  145 0.15084105801513736 train acc per epoch=>  0.9461317135549873\n",
      "val loss per epoch =>  0.5806978816473032 val acc per epoch => 0.8602650316455697 \n",
      "\n",
      "train loss per epoch =>  146 0.15142382834764087 train acc per epoch=>  0.9462196291864985\n",
      "val loss per epoch =>  0.5906057761439795 val acc per epoch => 0.8616495253164557 \n",
      "\n",
      "train loss per epoch =>  147 0.15426332028130132 train acc per epoch=>  0.9440217391609231\n",
      "val loss per epoch =>  0.5749808776227734 val acc per epoch => 0.862440664556962 \n",
      "\n",
      "train loss per epoch =>  148 0.15015939803189024 train acc per epoch=>  0.9462835677444478\n",
      "val loss per epoch =>  0.5780547630937793 val acc per epoch => 0.8649129746835443 \n",
      "\n",
      "train loss per epoch =>  149 0.1508558492945588 train acc per epoch=>  0.9457600703629692\n",
      "val loss per epoch =>  0.5755005697660809 val acc per epoch => 0.8650118670886076 \n",
      "\n",
      "train loss per epoch =>  150 0.14772742395015323 train acc per epoch=>  0.9475743287359663\n",
      "val loss per epoch =>  0.584923295657846 val acc per epoch => 0.8603639240506329 \n",
      "\n",
      "train loss per epoch =>  151 0.14507707083583488 train acc per epoch=>  0.9475143862807233\n",
      "val loss per epoch =>  0.5786409785475912 val acc per epoch => 0.8612539556962026 \n",
      "\n",
      "train loss per epoch =>  152 0.14285455998557303 train acc per epoch=>  0.9482576726647594\n",
      "val loss per epoch =>  0.5820100503631785 val acc per epoch => 0.8631329113924051 \n",
      "\n",
      "train loss per epoch =>  153 0.13789894916784123 train acc per epoch=>  0.9505554667824064\n",
      "val loss per epoch =>  0.5759885643101945 val acc per epoch => 0.8635284810126582 \n",
      "\n",
      "train loss per epoch =>  154 0.14241508497377794 train acc per epoch=>  0.9485933504750966\n",
      "val loss per epoch =>  0.5924874976088729 val acc per epoch => 0.8629351265822784 \n",
      "\n",
      "train loss per epoch =>  155 0.14118184327431346 train acc per epoch=>  0.9489010549567239\n",
      "val loss per epoch =>  0.5814831879320024 val acc per epoch => 0.8640229430379747 \n",
      "\n",
      "train loss per epoch =>  156 0.14012302092426573 train acc per epoch=>  0.9493206521434248\n",
      "val loss per epoch =>  0.5877676342107072 val acc per epoch => 0.8625395569620253 \n",
      "\n",
      "train loss per epoch =>  157 0.13761794941542704 train acc per epoch=>  0.9507512788638435\n",
      "val loss per epoch =>  0.5883988375905194 val acc per epoch => 0.8637262658227848 \n",
      "\n",
      "train loss per epoch =>  158 0.1428586791181351 train acc per epoch=>  0.9476422633966217\n",
      "val loss per epoch =>  0.5907113089968886 val acc per epoch => 0.8623417721518988 \n",
      "\n",
      "train loss per epoch =>  159 0.135821942706852 train acc per epoch=>  0.9524696292474751\n",
      "val loss per epoch =>  0.5932008484496346 val acc per epoch => 0.8622428797468354 \n",
      "\n",
      "train loss per epoch =>  160 0.13276902820600572 train acc per epoch=>  0.952505594629156\n",
      "val loss per epoch =>  0.588210139848009 val acc per epoch => 0.8654074367088608 \n",
      "\n",
      "train loss per epoch =>  161 0.1321533804716509 train acc per epoch=>  0.9529211957131505\n",
      "val loss per epoch =>  0.5964844777614255 val acc per epoch => 0.8650118670886076 \n",
      "\n",
      "train loss per epoch =>  162 0.13127000236412142 train acc per epoch=>  0.9526015026185214\n",
      "val loss per epoch =>  0.5887656381613091 val acc per epoch => 0.8637262658227848 \n",
      "\n",
      "train loss per epoch =>  163 0.12701072182763568 train acc per epoch=>  0.9537324169102837\n",
      "val loss per epoch =>  0.5946720038788228 val acc per epoch => 0.8646162974683544 \n",
      "\n",
      "train loss per epoch =>  164 0.13177914211474112 train acc per epoch=>  0.9524816177080354\n",
      "val loss per epoch =>  0.5928970262219634 val acc per epoch => 0.8656052215189873 \n",
      "\n",
      "train loss per epoch =>  165 0.13146577172381493 train acc per epoch=>  0.953820332541795\n",
      "val loss per epoch =>  0.5912277924863598 val acc per epoch => 0.8643196202531646 \n",
      "\n",
      "train loss per epoch =>  166 0.1299194529785982 train acc per epoch=>  0.9535725703629692\n",
      "val loss per epoch =>  0.5945394903044158 val acc per epoch => 0.8657041139240507 \n",
      "\n",
      "train loss per epoch =>  167 0.1277644557454397 train acc per epoch=>  0.9540680947206209\n",
      "val loss per epoch =>  0.5922164211544809 val acc per epoch => 0.8639240506329114 \n",
      "\n",
      "train loss per epoch =>  168 0.1256994888510393 train acc per epoch=>  0.9548473466387795\n",
      "val loss per epoch =>  0.590894558007204 val acc per epoch => 0.8663963607594937 \n",
      "\n",
      "train loss per epoch =>  169 0.12714599696990778 train acc per epoch=>  0.9552909207466008\n",
      "val loss per epoch =>  0.5922092561480365 val acc per epoch => 0.8662974683544303 \n",
      "\n",
      "train loss per epoch =>  170 0.12692907907049675 train acc per epoch=>  0.9552189898307976\n",
      "val loss per epoch =>  0.5918007775952544 val acc per epoch => 0.8666930379746836 \n",
      "\n",
      "train loss per epoch =>  171 0.128147263260906 train acc per epoch=>  0.9546835038363172\n",
      "val loss per epoch =>  0.5950879903533791 val acc per epoch => 0.8647151898734177 \n",
      "\n",
      "train loss per epoch =>  172 0.12660899048532975 train acc per epoch=>  0.9553228900255755\n",
      "val loss per epoch =>  0.5958681057525587 val acc per epoch => 0.8650118670886076 \n",
      "\n",
      "train loss per epoch =>  173 0.12251986712788987 train acc per epoch=>  0.9560741687674656\n",
      "val loss per epoch =>  0.6005631009989147 val acc per epoch => 0.8649129746835443 \n",
      "\n",
      "train loss per epoch =>  174 0.1201671695960757 train acc per epoch=>  0.9567135549567239\n",
      "val loss per epoch =>  0.6032415639750565 val acc per epoch => 0.865506329113924 \n",
      "\n",
      "train loss per epoch =>  175 0.12264379310181074 train acc per epoch=>  0.9570692135854755\n",
      "val loss per epoch =>  0.5995140154904957 val acc per epoch => 0.8657041139240507 \n",
      "\n",
      "train loss per epoch =>  176 0.11957285277869391 train acc per epoch=>  0.9573649297589841\n",
      "val loss per epoch =>  0.5988622794422922 val acc per epoch => 0.8656052215189873 \n",
      "\n",
      "train loss per epoch =>  177 0.12349226380057653 train acc per epoch=>  0.9559382992937132\n",
      "val loss per epoch =>  0.5976887137829503 val acc per epoch => 0.8645174050632911 \n",
      "\n",
      "train loss per epoch =>  178 0.12079224560190649 train acc per epoch=>  0.9564378197540713\n",
      "val loss per epoch =>  0.597453465944604 val acc per epoch => 0.864814082278481 \n",
      "\n",
      "train loss per epoch =>  179 0.1203955880573491 train acc per epoch=>  0.9568134590487956\n",
      "val loss per epoch =>  0.5931229772447031 val acc per epoch => 0.8654074367088608 \n",
      "\n",
      "train loss per epoch =>  180 0.12124153916411998 train acc per epoch=>  0.9566855819328971\n",
      "val loss per epoch =>  0.5934571604939956 val acc per epoch => 0.8650118670886076 \n",
      "\n",
      "train loss per epoch =>  181 0.1202835532672265 train acc per epoch=>  0.9579643543114138\n",
      "val loss per epoch =>  0.5956708558752567 val acc per epoch => 0.864121835443038 \n",
      "\n",
      "train loss per epoch =>  182 0.1145392908140674 train acc per epoch=>  0.9586117327060846\n",
      "val loss per epoch =>  0.5993567033659054 val acc per epoch => 0.8660007911392406 \n",
      "\n",
      "train loss per epoch =>  183 0.12170409007221841 train acc per epoch=>  0.9575767264036876\n",
      "val loss per epoch =>  0.5975415499149999 val acc per epoch => 0.8656052215189873 \n",
      "\n",
      "train loss per epoch =>  184 0.12199079272006173 train acc per epoch=>  0.9566855819328971\n",
      "val loss per epoch =>  0.595297989211505 val acc per epoch => 0.865506329113924 \n",
      "\n",
      "train loss per epoch =>  185 0.12035926833482044 train acc per epoch=>  0.9570332480513531\n",
      "val loss per epoch =>  0.5973390740684316 val acc per epoch => 0.8651107594936709 \n",
      "\n",
      "train loss per epoch =>  186 0.11805189954464698 train acc per epoch=>  0.9580362852272171\n",
      "val loss per epoch =>  0.5944566768181475 val acc per epoch => 0.8658030063291139 \n",
      "\n",
      "train loss per epoch =>  187 0.11839617454373014 train acc per epoch=>  0.9573609335038363\n",
      "val loss per epoch =>  0.5981708934789971 val acc per epoch => 0.8653085443037974 \n",
      "\n",
      "train loss per epoch =>  188 0.12066696224081547 train acc per epoch=>  0.9571051791195979\n",
      "val loss per epoch =>  0.5965171263942236 val acc per epoch => 0.8644185126582279 \n",
      "\n",
      "train loss per epoch =>  189 0.11765544132693954 train acc per epoch=>  0.9584798593350383\n",
      "val loss per epoch =>  0.5989986873125728 val acc per epoch => 0.8647151898734177 \n",
      "\n",
      "train loss per epoch =>  190 0.11755393708453458 train acc per epoch=>  0.9589194373401535\n",
      "val loss per epoch =>  0.5989685975298097 val acc per epoch => 0.8646162974683544 \n",
      "\n",
      "train loss per epoch =>  191 0.11731696323208186 train acc per epoch=>  0.9579243926745852\n",
      "val loss per epoch =>  0.5956478703625595 val acc per epoch => 0.8660996835443038 \n",
      "\n",
      "train loss per epoch =>  192 0.11661931946206733 train acc per epoch=>  0.9589993606138107\n",
      "val loss per epoch =>  0.595928088773655 val acc per epoch => 0.8651107594936709 \n",
      "\n",
      "train loss per epoch =>  193 0.1187167177095895 train acc per epoch=>  0.9570132672329388\n",
      "val loss per epoch =>  0.5964357909522479 val acc per epoch => 0.8650118670886076 \n",
      "\n",
      "train loss per epoch =>  194 0.11893306893613333 train acc per epoch=>  0.9574888107721763\n",
      "val loss per epoch =>  0.5983476525620569 val acc per epoch => 0.8662974683544303 \n",
      "\n",
      "train loss per epoch =>  195 0.11672317939798546 train acc per epoch=>  0.9577925191510974\n",
      "val loss per epoch =>  0.598598911037928 val acc per epoch => 0.8660007911392406 \n",
      "\n",
      "train loss per epoch =>  196 0.11708285813898686 train acc per epoch=>  0.957960358056266\n",
      "val loss per epoch =>  0.6004856217511093 val acc per epoch => 0.865506329113924 \n",
      "\n",
      "train loss per epoch =>  197 0.11752596964387943 train acc per epoch=>  0.9567575128487004\n",
      "val loss per epoch =>  0.5983465219600291 val acc per epoch => 0.8659018987341772 \n",
      "\n",
      "train loss per epoch =>  198 0.11990457353994365 train acc per epoch=>  0.9566096547619461\n",
      "val loss per epoch =>  0.5990432629102393 val acc per epoch => 0.8657041139240507 \n",
      "\n",
      "train loss per epoch =>  199 0.11792753928381464 train acc per epoch=>  0.9577046036720276\n",
      "val loss per epoch =>  0.5990638853628424 val acc per epoch => 0.865506329113924 \n",
      "\n",
      "CPU times: user 19min 52s, sys: 4min 8s, total: 24min\n",
      "Wall time: 31min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.RandomCrop(size=32, padding=4),\n",
    "      transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomRotation(20),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "     ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True,transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "                            ]))\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "model =ResNetF()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "  \n",
    "\n",
    "loss =10\n",
    "train_loss_history=[]\n",
    "train_acc_history=[]\n",
    "test_loss_history=[]\n",
    "test_acc_history=[]\n",
    "EPOCHS=200\n",
    "for i in range(EPOCHS):\n",
    "    # using train iterator\n",
    "    train_loss , epoch_acc = train(model,trainloader, optimizer, criterion, device=device)\n",
    "    print(\"train loss per epoch => \", i, train_loss, \"train acc per epoch=> \" , epoch_acc)\n",
    "    \n",
    "    epoch_loss , epoch_valid_acc = evaluate(model, testloader, criterion, device)\n",
    "    print(\"val loss per epoch => \", epoch_loss , \"val acc per epoch =>\",epoch_valid_acc,\"\\n\")\n",
    "        \n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(epoch_acc)\n",
    "    test_loss_history.append(epoch_loss)\n",
    "    test_acc_history.append(epoch_valid_acc)\n",
    "    \n",
    "    if epoch_loss<loss:\n",
    "        torch.save(model,\"./res_512_d.pt\")\n",
    "        loss= epoch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666930379746836"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(test_acc_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"./train_acc_history.txt\",train_acc_history)\n",
    "np.savetxt(\"./train_loss_history.txt\",train_loss_history)\n",
    "np.savetxt(\"./test_acc_history.txt\",test_acc_history)\n",
    "np.savetxt(\"./test_loss_history.txt\",test_loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
